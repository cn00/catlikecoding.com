<!DOCTYPE html>
<html lang="en">
	<head prefix="og: http://ogp.me/ns#">
		<meta charset="utf-8">
		<meta property="og:url" content="https://catlikecoding.com/unity/tutorials/custom-srp/particles/">
		<meta property="og:type" content="article">
		<meta property="og:image:width" content="1024">
		<meta property="og:image:height" content="512">
		<meta property="og:image" content="https://catlikecoding.com/unity/tutorials/custom-srp/particles/tutorial-image.jpg">
		<meta property="og:title" content="Particles">
		<meta property="og:description" content="A Unity Custom SRP tutorial about creating particles that sample color and depth.">
		<meta property="twitter:card" content="summary_large_image">
		<meta property="twitter:creator" content="@catlikecoding">
		<meta name="viewport" content="width=768">
		<title>Particles</title>
		<link href="../../tutorials.css" rel="stylesheet">
		<link rel="manifest" href="../../../../site.webmanifest">
		<link rel="mask-icon" href="../../../../safari-pinned-tab.svg" color="#aa0000">
		
		<script type="application/ld+json">{
			"@context": "http://schema.org",
			"@type": "WebPage",
			"mainEntity": {
				"@type": "TechArticle",
				"@id": "https://catlikecoding.com/unity/tutorials/custom-srp/particles/#article",
				"headline": "Particles",
				"alternativeHeadline": "Color and Depth Textures",
				"datePublished": "2020-11-27",
				"dateModified": "2021-01-27",
				"author": { "@type": "Person", "name": "Jasper Flick", "@id": "https://catlikecoding.com/jasper-flick/#person" },
				"publisher": { "@type": "Organization", "name": "Catlike Coding", "@id": "https://catlikecoding.com/#organization" },
				"description": "A Unity Custom SRP tutorial about creating particles that sample color and depth.",
				"image": "https://catlikecoding.com/unity/tutorials/custom-srp/particles/tutorial-image.jpg",
				"dependencies": "Unity 2019.4.14f1",
				"proficiencyLevel": "Expert"
			},
			"breadcrumb": {
				"@type": "BreadcrumbList",
				"itemListElement": [
					{ "@type": "ListItem", "position": 1, "item": { "@id": "https://catlikecoding.com/unity/", "name": "Unity" }},
					{ "@type": "ListItem", "position": 2, "item": { "@id": "https://catlikecoding.com/unity/tutorials/", "name": "Tutorials" }},
					{ "@type": "ListItem", "position": 3, "item": { "@id": "https://catlikecoding.com/unity/tutorials/custom-srp/", "name": "Custom SRP" }}
				]
			}
		}</script>
		<script>
			var customTypes = {
				BloomSettings: 1,
				CameraBufferSettings: 1,
				CameraRenderer: 1,
				CameraSettings: 1,
				CascadeBlendMode: 1,
				ChannelMixerSettings: 1,
				ColorAdjustmentsSettings: 1,
				ColorLUTResolution: 1,
				CustomLightEditor: 1,
				CustomRenderPipeline: 1,
				CustomRenderPipelineAsset: 1,
				CustomRenderPipelineCamera: 1,
				CustomShaderGUI: 1,
				Directional: 1,
				DirectionalShadowData: 1,
				FilterMode: 1,
				FinalBlendMode: 1,
				InputConfig: 1,
				IntFloat: 1,
				Lighting: 1,
				MeshBall: 1,
				Mode: 1,
				Other: 1,
				OtherShadowData: 1,
				Pass: 1,
				ReinterpretExtensions: 1,
				RenderingLayerMaskDrawer: 1,
				RenderingLayerMaskField : 1,
				RenderingLayerMaskFieldAttribute: 1,
				PerObjectMaterialProperties: 1,
				PostFXSettings: 1,
				PostFXStack: 1,
				ShadowedDirLight: 1,
				ShadowedOtherLight: 1,
				ShadowData: 1,
				Shadows: 1,
				ShadowsMidtonesHighlightsSettings: 1,
				ShadowMask: 1,
				ShadowMode: 1,
				ShadowSettings: 1,
				SplitToningSettings: 1,
				TextureSize: 1,
				ToneMappingSettings: 1,
				WhiteBalanceSettings: 1
			};
			
			var defaultCodeClass = 'shader';
			var hasMath = false;
		</script>
	</head>
	<body>
		<header>
			<a href="../../../../index.html"><img src="../../../../catlike-coding-logo.svg" alt="Catlike Coding" width="45" height="45"></a>
			<nav>
				<ol>
					<li><a href="../../../../index.html">Catlike Coding</a></li>
					<li><a href="../../../index.html">Unity</a></li>
					<li><a href="../../../tutorials">Tutorials</a></li>
					<li><a href="../index.html">Custom SRP</a></li>
				</ol>
			</nav>
		</header>
		
		<main>
			<article>
				<header>
					<h1>Particles</h1>
					<p>Color and Depth Textures</p>
					<ul>
						<li>Support flipbook, near fade, soft, and distortion particles.</li>
						<li>Determine fragment depth, for orthographic and perspective projections.</li>
						<li>Copy and sample the color and depth buffers.</li>					</ul>
				</header>
				
				<p>This is the 15th part of a tutorial series about creating a <a href="../index.html">custom scriptable render pipeline</a>. We'll create depth-based fading and distorting particles, relying on a color and depth texture.</p>
				
				<p>This tutorial is made with Unity 2019.4.14f1.</p>
				
				<figure>
					<img src="tutorial-image.jpg" width="512" height="256">
					<figcaption>Using particles to create a messy atmosphere.</figcaption>
				</figure>
				
				<section>
					<h2>Unlit Particles</h2>
					
					<p>A particle system can use any material, so our RP can already render them, with limitations. In this tutorial we'll only consider unlit particles. Lit particles work the same way, just with more shader properties and lighting calculations.</p>
					
					<p>I set up a new scene for the particles that is a variant of the already existing test scene. It has a few long vertical cubes and a bright yellow lightbulb to serve as a background for the particles systems.</p>
					
					<figure>
						<img src="unlit-particles/without-particles.png" width="440" height="280">
						<figcaption>Scene without particles and without post FX.</figcaption>
					</figure>
					
					<section>
						<h3>Particle System</h3>
						
						<p>Create a particle system via <em translate="no">GameObject / Effects / Particle System</em> and position it a bit below the ground plane. I assume that you already know how to configure particle systems and won't go into details about that. If not check <a href="https://docs.unity3d.com/Manual/class-ParticleSystem.html">Unity's documentation</a> to learn about the specific modules and their settings.</p>
						
						<aside>
							<h3>What about the Visual Effects Graph?</h3>
							<div>
								<p>The VFX graph is compute-shader based and currently tightly coupled with URP and HDRP. It cannot be easily used with a custom SRP.</p>
								
								<p>Note that the regular particle system is not superseded by the VFX graph. It's better for many small systems&mdash;up to about a thousand particles each&mdash;while VFX graph is better for massive systems.</p>
							</div>
						</aside>
						
						<p>The default system makes particles move upward and fill a cone-shaped region. If we assign our unlit material to it the particles will show up as solid white squares aligned with the camera plane. They pop in and out of existence but because they start below plane they appear to rise out of the ground.</p>
						
						<figure>
							<img src="unlit-particles/default-particle-system.png" width="440" height="280">
							<figcaption>Default particle system with unlit material, positioned below ground.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Unlit Particles Shader</h3>
						
						<p>We could use our unlit shader for particles, but let's make a dedicated one for them. It starts as a copy of the unlit shader with its menu item changed to <em translate="no">Custom RP/Particles/Unlit</em>. Also, as particles are always dynamic it doesn't need a meta pass.</p>
						
						<pre translate="no">Shader "Custom RP/<ins>Particles/</ins>Unlit" {
	
	&hellip;
	
	SubShader {
		&hellip;

		<del>//Pass {</del>
			<del>//Tags {</del>
				<del>//"LightMode" = "Meta"</del>
			<del>//}</del>

			<del>//&hellip;</del>
		<del>//}</del>
	}

	CustomEditor "CustomShaderGUI"
}</pre>
						
						<p>Create a dedicated material for unlit particles with this shader, then make the particle system use it. Currently it is equivalent to the earlier unlit material. It's also possible to set the particle system to render meshes, even with shadows if that's enabled for both the material and the particle system. However, GPU instancing doesn't work because particles systems use procedural drawing for that, which we won't cover in this tutorial. Instead all particle meshes get merged into a single mesh, just like billboard particles.</p>
						
						<figure>
							<img src="unlit-particles/particles-sphere.png" width="440" height="280">
							<figcaption>Sphere mesh particles, with shadows.</figcaption>
						</figure>
						
						<p>From now on we'll only concern ourselves with billboard particles, without shadows. <a href="unlit-particles/particles-single.png">Here</a> is a base map for a single particle, containing a simple smoothly fading white disc.</p>
						
						<figure>
							<img src="unlit-particles/particles-single.png" width="128" height="128" style="background-color: black">
							<figcaption>Base map for single particle, on black background.</figcaption>
						</figure>
						
						<p>When using that texture for our fade particles we get a simplistic effect that looks somewhat like white smoke is coming out of the ground. To make it more convincing increase the emission rate to something like 100.</p>
						
						<figure>
							<img src="unlit-particles/textured-particles.png" width="440" height="280">
							<figcaption>Textured billboard particles, emission rate set to 100.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Vertex Colors</h3>
						
						<p>It's possible to use a different color per particle. The simplest way to demonstrate this is to set the starting color to randomly pick between black and white. However, doing so doesn't currently change the appearance of the particles. To make this work we have to add support for vertex colors to our shader. Rather than create new HLSL files for particles we'll add support for it to <em translate="no">UnlitPass</em>.</p>
						
						<p>The first step is to add a <code>float4</code> vertex attribute with the <code>COLOR</code> semantic.</p>
						
						<pre translate="no">struct Attributes {
	float3 positionOS : POSITION;
	<ins>float4 color : COLOR;</ins>
	float2 baseUV : TEXCOORD0;
	UNITY_VERTEX_INPUT_INSTANCE_ID
};</pre>
						
						<p>Add it to <code>Varyings</code> as well and pass it through <code>UnlitPassVertex</code>, but only if <em translate="no">_VERTEX_COLORS</em> is defined. That way we can enable and disable vertex color support as desired.</p>
						
						<pre translate="no">struct Varyings {
	float4 positionCS : SV_POSITION;
	<ins>#if defined(_VERTEX_COLORS)</ins>
		<ins>float4 color : VAR_COLOR;</ins>
	<ins>#endif</ins>
	float2 baseUV : VAR_BASE_UV;
	UNITY_VERTEX_INPUT_INSTANCE_ID
};

Varyings UnlitPassVertex (Attributes input) {
	&hellip;
	<ins>#if defined(_VERTEX_COLORS)</ins>
		<ins>output.color = input.color;</ins>
	<ins>#endif</ins>
	output.baseUV = TransformBaseUV(input.baseUV);
	return output;
}</pre>
						
						<p>Next, add a color to <code>InputConfig</code> in <em translate="no">UnlitInput</em>, set it to opaque white by default, and factor it into the result of <code>GetBase</code>.</p>
						
						<pre translate="no">struct InputConfig {
	<ins>float4 color;</ins>
	float2 baseUV;
};

InputConfig GetInputConfig (float2 baseUV) {
	InputConfig c;
	<ins>c.color = 1.0;</ins>
	c.baseUV = baseUV;
	return c;
}

&hellip;

float4 GetBase (InputConfig c) {
	float4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, c.baseUV);
	float4 baseColor = INPUT_PROP(_BaseColor);
	return baseMap * baseColor <ins>* c.color</ins>;
}</pre>
						
						<p>Back to <em translate="no">UnlitPass</em>, copy the interpolated vertex color to <code>config</code> if it exists in <code>UnlitPassFragment</code>.</p>
						
						<pre translate="no">	InputConfig config = GetInputConfig(input.baseUV);
	<ins>#if defined(_VERTEX_COLORS)</ins>
		<ins>config.color = input.color;</ins>
	<ins>#endif</ins></pre>
						
						<p>To finally add support for vertex colors to <em translate="no">UnlitParticles</em> add a toggle shader property to it.</p>
						
						<pre translate="no">		[HDR] _BaseColor("Color", Color) = (1.0, 1.0, 1.0, 1.0)
		<ins>[Toggle(_VERTEX_COLORS)] _VertexColors ("Vertex Colors", Float) = 0</ins></pre>
						
						<p>Along with the corresponding shader feature that defined the keyword. You can do this for the regular <em translate="no">Unlit</em> shader too if you want it to support vertex colors as well.</p>
						
						<pre translate="no">			<ins>#pragma shader_feature _VERTEX_COLORS</ins></pre>
						
						<figure>
							<img src="unlit-particles/vertex-colors-without-sorting.png" width="440" height="280" alt="without">
							<img src="unlit-particles/vertex-colors-with-sorting.png" width="440" height="280" alt="with">
							<figcaption>Using vertex colors, without and with sorting by distance.</figcaption>
						</figure>
						
						<p>We now get colored particles. At this point particle sorting becomes an issue. If all particles have the same color their draw order doesn't matter, but if they're different we need to sort them by distance to get the correct result. Note that when sorting based on distance particles might suddenly swap draw order due to position of view changes, like any transparent object.</p>
					</section>
					
					<section>
						<h3>Flipbooks</h3>
						
						<p>Billboard particles can be animated, by cycling through different base maps. Unity refers to these as flipbook particles. This is done by using a texture atlas laid out in a regular grid, like <a href="unlit-particles/particles-flipbook.png">this texture</a> containing a 4&times;4 grid of a looping noise pattern.</p>
						
						<figure>
							<img src="unlit-particles/particles-flipbook.png" width="512" height="512" style="background-color: black">
							<figcaption>Base map for particle flipbook, on black background.</figcaption>
						</figure>
						
						<p>Create a new unlit particle material that uses the flipbook map, then duplicate our particle system and have it use that flipbook material. Deactivate the singular-particle version so we only see the flipbook system. As each particle now represents a little cloud increase their size to something like 2. Enable the <em translate="no">Texture Sheet Animation</em> module of the particle system, configure it for a 4&times;4 flipbook, make it start at a random frame, and go through one cycle during a particle's lifetime.</p>
						
						<p>Extra variety can be added by randomly flipping particles along X and Y 50% of the time, starting with an arbitrary rotation, and making particles rotate with a random velocity.</p>
						
						<figure>
							<img src="unlit-particles/particles-with-flipbook.png" width="440" height="280">
							<figcaption>Flipbook particle system.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Flipbook Blending</h3>
					
						<p>When the system is in motion it is obvious that the particles cycle through a few frames, because the flipbook frame rate is very low. For particles with a lifetime of five seconds it's 3.2 frames per second. This can be smoothed out by blending between successive frames. This requires us to send a second pair of UV coordinates and an animation blend factor to the shader. We do that by enabling custom vertex stream in the <em translate="no">Renderer</em> module. Add <em translate="no">UV2</em> and <em translate="no">AnimBlend</em>. You could also remove the normal stream, as we don't need it.</p>
						
						<figure>
							<img src="unlit-particles/custom-vertex-streams.png" width="296" height="198">
							<figcaption>Custom vertex streams.</figcaption>
						</figure>
						
						<p>After adding the streams an error will be displayed indicating a mismatch between the particle system and the shader that it currently uses. This error will go away after we consume these streams in our shader. Add a shader keyword toggle property to <em translate="no">UnlitParticle</em> to control whether we support flipbook blending or not.</p>
						
						<pre translate="no">		[Toggle(_VERTEX_COLORS)] _VertexColors ("Vertex Colors", Float) = 0
		<ins>[Toggle(_FLIPBOOK_BLENDING)] _FlipbookBlending ("Flipbook Blending", Float) = 0</ins></pre>
						
						<p>Along with the accompanying shader feature.</p>
						
						<pre translate="no">			<ins>#pragma shader_feature _FLIPBOOK_BLENDING</ins></pre>
						
						<p>If flipbook blending is active both UV pairs are provided via <code>TEXCOORD0</code>, so it has to be a <code>float4</code> instead of a <code>float2</code>. The blend factor is provided as a single <code>float</code> via <code>TEXCOORD1</code>.</p>
						
						<pre translate="no">struct Attributes {
	float3 positionOS : POSITION;
	float4 color : COLOR;
	<ins>#if defined(_FLIPBOOK_BLENDING)</ins>
		<ins>float4 baseUV : TEXCOORD0;</ins>
		<ins>float flipbookBlend : TEXCOORD1;</ins>
	<ins>#else</ins>
		float2 baseUV : TEXCOORD0;
	<ins>#endif</ins>
	UNITY_VERTEX_INPUT_INSTANCE_ID
};</pre>
						
						<p>We'll add the new data as a single <code>float3 flipbookUVB</code> field to <code>Varyings</code>, if needed.</p>
						
						<pre translate="no">struct Varyings {
	&hellip;
	float2 baseUV : VAR_BASE_UV;
	<ins>#if defined(_FLIPBOOK_BLENDING)</ins>
		<ins>float3 flipbookUVB : VAR_FLIPBOOK;</ins>
	<ins>#endif</ins>
	UNITY_VERTEX_INPUT_INSTANCE_ID
};</pre>
						
						<p>Adjust <code>UnlitPassVertex</code> so it copies all relevant data to it, when appropriate.</p>
						
						<pre translate="no">Varyings UnlitPassVertex (Attributes input) {
	&hellip;
	output.baseUV<ins>.xy</ins> = TransformBaseUV(input.baseUV<ins>.xy</ins>);
	<ins>#if defined(_FLIPBOOK_BLENDING)</ins>
		<ins>output.flipbookUVB.xy = TransformBaseUV(input.baseUV.zw);</ins>
		<ins>output.flipbookUVB.z = input.flipbookBlend;</ins>
	<ins>#endif</ins>
	return output;
}</pre>
						
						<p>Add <code>flipbookUVB</code> to <code>InputConfig</code> as well, along with a boolean to indicate whether flipbook blending is enabled, which isn't the case by default.</p>
						
						<pre translate="no">struct InputConfig {
	float4 color;
	float2 baseUV;
	<ins>float3 flipbookUVB;</ins>
	<ins>bool flipbookBlending;</ins>
};

InputConfig GetInputConfig (float2 baseUV) {
	&hellip;
	<ins>c.flipbookUVB = 0.0;</ins>
	<ins>c.flipbookBlending = false;</ins>
	return c;
}</pre>
						
						<p>If flipbook blending is enabled we have to sample the base map a second time in <code>GetBase</code>, with the flipbook UV, then interpolate from the first to second sample based on the blend factor.</p>
						
						<pre translate="no">float4 GetBase (InputConfig c) {
	float4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, c.baseUV);
	<ins>if (c.flipbookBlending) {</ins>
		<ins>baseMap = lerp(</ins>
			<ins>baseMap, SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, c.flipbookUVB.xy),</ins>
			<ins>c.flipbookUVB.z</ins>
		<ins>);</ins>
	<ins>}</ins>
	float4 baseColor = INPUT_PROP(_BaseColor);
	return baseMap * baseColor * c.color;
}</pre>
						
						<p>To finally activate flipbook blending override the default configuration in <code>UnlitPassFragment</code> when appropriate.</p>
						
						<pre translate="no">	#if defined(_VERTEX_COLORS)
		config.color = input.color;
	#endif
	<ins>#if defined(_FLIPBOOK_BLENDING)</ins>
		<ins>config.flipbookUVB = input.flipbookUVB;</ins>
		<ins>config.flipbookBlending = true;</ins>
	<ins>#endif</ins></pre>
						
						<figure>
							<div class="vid" style="width: 250px; height:216px;"><iframe src='https://gfycat.com/ifr/limpwelltododutchsmoushond?controls=0'></iframe></div>
							<figcaption>Flipbook blending.</figcaption>
						</figure>
						
					</section>
				</section>
				
				<section>
					<h2>Fading Near Camera</h2>
					
					<p>When the camera is inside a particle system particles will end up very close to the camera's near place and also pass from one side to the other. The particle system has a <em translate="no">Renderer / Max Particle Size</em> property that prevents individual billboard particles from covering too much of the window. Once they reach their maximum visible size they'll appear to slide out of the way instead of growing larger as they approach the near plane.</p>
					
					<p>Another way to deal with particles close to the near plane is to fade them out based on their fragment depth. This can look better when moving through a particle system that represents atmospherical effects.</p>
					
					<section>
						<h3>Fragment Data</h3>
						
						<p>We already have the fragment depth available in out fragment functions. It's provided via the <code>float4</code> with the <code>SV_POSITION</code> semantic. We've already used the XY components of it for dithering, but let's now make it formal that we're using fragment data.</p>
						
						<p>In the vertex function <code>SV_POSITION</code> represents the clip-space position of the vertex, as 4D homogeneous coordinates. But in the fragment function <code>SV_POSITION</code> represents the screen-space&mdash;also known as window-space&mdash;position of the fragment. The space conversion is performed by the GPU. To make this explicit let's rename <code>postionCS</code> to <code>positionCS_SS</code> in all our <code>Varyings</code> structs.</p>
						
						<pre translate="no">	float4 <ins>positionCS_SS</ins> : SV_POSITION;</pre>
						
						<p>Make the adjustment in the accompanying vertex functions as well.</p>
						
						<pre translate="no">	output.<ins>positionCS_SS</ins> = TransformWorldToHClip(positionWS);</pre>
						
						<p>Next, we'll introduce a new <em translate="no">Fragment</em> HLSL include file containing a <code>Fragment</code> struct and a <code>GetFragment</code> function that returns the fragment, given a <code>float4</code> screen-space position vector. Initially the fragment only has a 2D position, which comes from the screen-space position's XY components. These are texel coordinates with a 0.5 offset. It's (0.5, 0.5) for the texel in the bottom left corner of the screen, (1.5, 0.5) for the texel to the right of it, and so on.</p>
						
						<pre translate="no"><ins>#ifndef FRAGMENT_INCLUDED</ins>
<ins>#define FRAGMENT_INCLUDED</ins>

<ins>struct Fragment {</ins>
	<ins>float2 positionSS;</ins>
<ins>};</ins>

<ins>Fragment GetFragment (float4 positionSS) {</ins>
	<ins>Fragment f;</ins>
	<ins>f.positionSS = positionSS.xy;</ins>
	<ins>return f;</ins>
<ins>}</ins>

<ins>#endif</ins></pre>
						
						<p>Include this file in <em translate="no">Common</em> after all other include statements, then adjust <code>ClipLOD</code> so its first argument is a <code>Fragment</code> instead of a <code>float4</code>.</p>
						
						<pre translate="no">#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/UnityInstancing.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Packing.hlsl"

<ins>#include "Fragment.hlsl"</ins>

&hellip;

void ClipLOD (<ins>Fragment fragment</ins>, float fade) {
	#if defined(LOD_FADE_CROSSFADE)
		float dither = InterleavedGradientNoise(<ins>fragment.positionSS</ins>, 0);
		clip(fade + (fade < 0.0 ? dither : -dither));
	#endif
}</pre>
						
						<p>Let's also define common linear and point clamp sampler states in <em translate="no">Common</em> as well at this point, because we'll be using those in multiple places later. Do so before including <em translate="no">Fragment</em>.</p>
						
						<pre translate="no">#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Packing.hlsl"

<ins>SAMPLER(sampler_linear_clamp);</ins>
<ins>SAMPLER(sampler_point_clamp);</ins>

#include "Fragment.hlsl"</pre>
						
						<p>Then remove the generic sampler definition from <em translate="no">PostFXStackPasses</em>, as that's now a duplicate that would cause a compiler error.</p>
						
						<pre translate="no">TEXTURE2D(_PostFXSource2);
<del>//SAMPLER(sampler_linear_clamp);</del></pre>
						
						<p>Next, add a fragment to the <code>InputConfig</code> structs of both <em translate="no">LitInput</em> and <em translate="no">UnlitInput</em>. Then add the screen-space position vector as a first parameter to the <code>GetInputConfig</code> functions, so they can invoke <code>GetFragment</code> with it.</p>
						
						<pre translate="no">struct InputConfig {
	<ins>Fragment fragment;</ins>
	&hellip;
};

InputConfig GetInputConfig (<ins>float4 positionSS,</ins> &hellip;) {
	InputConfig c;
	<ins>c.fragment = GetFragment(positionSS);</ins>
	&hellip;
}</pre>
						
						<p>Add the argument in all places that we invoke <code>GetInputConfig</code>.</p>
						
						<pre translate="no">	InputConfig config = GetInputConfig(<ins>input.positionCS_SS,</ins> &hellip;);</pre>
						
						<p>Then adjust <code>LitPassFragment</code> so it invokes <code>ClipLOD</code> after getting the config, so it can pass a fragment to it. Also pass the fragment's position to <code>InterleavedGradientNoise</code> instead of using <code>input.positionCS_SS</code> directly.</p>
						
						<pre translate="no">float4 LitPassFragment (Varyings input) : SV_TARGET {
	UNITY_SETUP_INSTANCE_ID(input);
	<del>//ClipLOD(input.positionSS.xy, unity_LODFade.x);</del>
	InputConfig config = GetInputConfig(input.positionCS_SS, input.baseUV);
	<ins>ClipLOD(config.fragment, unity_LODFade.x);</ins>
	
	&hellip;
	surface.dither = InterleavedGradientNoise(<ins>config.fragment.positionSS</ins>, 0);
	&hellip;
}</pre>
						
						<p><code>ShadowCasterPassFragment</code> must also be changed so it clips after getting the config.</p>
						
						<pre translate="no">void ShadowCasterPassFragment (Varyings input) {
	UNITY_SETUP_INSTANCE_ID(input);
	<del>//ClipLOD(input.positionCS.xy, unity_LODFade.x);</del>
	InputConfig config = GetInputConfig(input.positionCS_SS, input.baseUV);
	<ins>ClipLOD(config.fragment, unity_LODFade.x);</ins>
	
	float4 base = GetBase(config);
	#if defined(_SHADOWS_CLIP)
		clip(base.a - GetCutoff(config));
	#elif defined(_SHADOWS_DITHER)
		float dither = InterleavedGradientNoise(input.positionSS.xy, 0);
		clip(base.a - dither);
	#endif
}</pre>
					</section>
					
					<section>
						<h3>Fragment Depth</h3>
						
						<p>To fade particles near the camera we need to know the fragment's depth. So add a depth field to <code>Fragment</code>.
						
						<pre translate="no">struct Fragment {
	float2 positionSS;
	<ins>float depth;</ins>
};</pre>
						
						<p>The fragment depth is stored in the last component of the screen-space position vector. It's the value that was used to perform the perspective division to project 3D positions onto the screen. This is the view-space depth, so it's the distance from the camera XY plane, not its near plane.</p>
						
						<pre translate="no">Fragment GetFragment (float4 positionCS_SS) {
	Fragment f;
	f.positionSS = positionSS.xy;
	<ins>f.depth = positionSS.w;</ins>
	return f;
}</pre>
						
						<aside>
							<h3>What is view space?</h3>
							<div>
								<p>It is world space rotated and translated so the camera ends up without rotation at the origin.</p>
							</div>
						</aside>
						
						<p>We can verify that this is correct by directly returning the fragment depth in <code>LitPassFragment</code> and <code>UnlitPassFragment</code>, scaled down so we can see it as a grayscale gradient.</p>
						
						<pre translate="no">	InputConfig config = GetInputConfig(input.positionCS_SS, input.baseUV);
	<ins>return float4(config.fragment.depth.xxx / 20.0, 1.0);</ins></pre>
						
						<figure>
							<img src="fading-near-camera/fragment-depth.png" width="440" height="280">
							<figcaption>Fragment depth, divided by 20.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Orthographic Depth</h3>
						
						<p>The above approach only works when using a perspective camera. When using an orthographic camera there is no perspective division, thus the last component of the screen-space position vector is always 1.</p>
						
						<p>We can determine whether we're dealing with an orthographic camera by adding a <code>float4 unity_OrthoParams</code> field to <em translate="no">UnityInput</em>, via which Unity communicated information about the orthographic camera to the GPU.</p>
						
						<pre translate="no"><ins>float4 unity_OrthoParams;</ins>
float4 _ProjectionParams;</pre>
						
						<p>In case of an orthographic camera its last component will be 1, otherwise it will be zero. Add an <code>IsOrthographicCamera</code> function to <em translate="no">Common</em> that uses this fact, defined before including <em translate="no">Fragment</em> so we can use it there. If you'll never use orthographic cameras you could hard-code it to return <code>false</code>, or control this via a shader keyword.</p>
						
						<pre translate="no"><ins>bool IsOrthographicCamera () {</ins>
	<ins>return unity_OrthoParams.w;</ins>
<ins>}</ins>

#include "Fragment.hlsl"</pre>
						
						<p>For an orthographic camera the best we can do is rely on the Z component of the screen-space position vector, which contains the converted clip-space depth of the fragment. This is the raw value that is used for depth comparisons and is written to the depth buffer if depth writing is enabled. It's a value in the 0&ndash;1 range and is linear for orthographic projections. To convert it to view-space depth we have to scale it by the camera's near&ndash;far range and then add the near plane distance. The near and far distances are stored in the Y and Z components of <code>_ProjectionParams</code>. We also need to reverse the raw depth if a reversed depth buffer is used. Do this in a new <code>OrthographicDepthBufferToLinear</code> function, also defined in <em translate="no">Common</em> before including <em translate="no">Fragment</em>.</p>
						
						<pre translate="no"><ins>float OrthographicDepthBufferToLinear (float rawDepth) {</ins>
	<ins>#if UNITY_REVERSED_Z</ins>
		<ins>rawDepth = 1.0 - rawDepth;</ins>
	<ins>#endif</ins>
	<ins>return (_ProjectionParams.z - _ProjectionParams.y) * rawDepth + _ProjectionParams.y;</ins>
<ins>}</ins>

#include "Fragment.hlsl"</pre>
						
						<p>Now <code>GetFragment</code> can check whether an orthographic camera is used and if so rely on <code>OrthographicDepthBufferToLinear</code> to determine the fragment depth.</p>
						
						<pre translate="no">	f.depth = <ins>IsOrthographicCamera() ?</ins>
		<ins>OrthographicDepthBufferToLinear(positionSS.z) :</ins> positionSS.w;</pre>
						
						<figure>
							<img src="fading-near-camera/orthographic-depth.png" width="440" height="280">
							<figcaption>Fragment depth for orthographic camera.</figcaption>
						</figure>
						
						<p>After verifying that the fragment depth is correct for both camera types remove the debug visualization from <code>LitPassFragment</code> and <code>UnlitPassFragment</code>.</p>
						
						<pre translate="no">	<del>//return float4(config.fragment.depth.xxx / 20.0, 1.0);</del></pre>						
					</section>
					
					<section>
						<h3>Distance-Based Fading</h3>
						
						<p>Back to the <em translate="no">UnlitParticles</em> shader, add a <em translate="no">Near Fade</em> keyword toggle property, along with properties to make its distance and range configurable. The distance determines how close to the camera plane the particles should disappear completely. This is the camera plane, not its near place. So a value of at least the near plane should be used. 1 is a reasonable default. The range controls the length of the transition region, inside which the particles will linearly fade out. Again 1 is a reasonable default, and at minimum it must be a small positive value.</p>
						
						<pre translate="no">		<ins>[Toggle(_NEAR_FADE)] _NearFade ("Near Fade", Float) = 0</ins>
		<ins>_NearFadeDistance ("Near Fade Distance", Range(0.0, 10.0)) = 1</ins>
		<ins>_NearFadeRange ("Near Fade Range", Range(0.01, 10.0)) = 1</ins></pre>
						
						<p>Add a shader feature to enable the near fading.</p>
						
						<pre translate="no">			<ins>#pragma shader_feature _NEAR_FADE</ins></pre>
						
						<p>Then include the distance and range in the <code>UnityPerMaterial</code> buffer in <em translate="no">UnlitInput</em>.</p>
						
						<pre translate="no">UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	UNITY_DEFINE_INSTANCED_PROP(float4, _BaseMap_ST)
	UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor)
	<ins>UNITY_DEFINE_INSTANCED_PROP(float, _NearFadeDistance)</ins>
	<ins>UNITY_DEFINE_INSTANCED_PROP(float, _NearFadeRange)</ins>
	UNITY_DEFINE_INSTANCED_PROP(float, _Cutoff)
	UNITY_DEFINE_INSTANCED_PROP(float, _ZWrite)
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)</pre>
						
						<p>Next, add a boolean <code>nearFade</code> field to <code>InputConfig</code> to control whether near fading is active, which it isn't by default.</p>
						
						<pre translate="no">struct InputConfig {
	&hellip;
	<ins>bool nearFade;</ins>
};

InputConfig GetInputConfig (float4 positionCC_SS, float2 baseUV) {
	&hellip;
	<ins>c.nearFade = false;</ins>
	return c;
}</pre>
						
						<p>Fading near the camera is done by simply decreasing the fragment's base alpha. The attenuation factor is equal to the fragment depth minus the fade distance, then divided by the fade range. As the result can be negative saturate it before factoring it into the alpha of the base map. Do this in <code>GetBase</code>, when appropriate.</p>
						
						<pre translate="no">	if (c.flipbookBlending) { &hellip; }
	<ins>if (c.nearFade) {</ins>
		<ins>float nearAttenuation = (c.fragment.depth - INPUT_PROP(_NearFadeDistance)) /</ins>
			<ins>INPUT_PROP(_NearFadeRange);</ins>
		<ins>baseMap.a *= saturate(nearAttenuation);</ins>
	<ins>}</ins></pre>
						
						<p>Finally, to activate the feature set the fragment's <code>nearFade</code> field to <code>true</code> in <code>UnlitPassFragment</code> is the <em translate="no">_NEAR_FADE</em> keyword if defined.</p>
						
						<pre translate="no">	#if defined(_FLIPBOOK_BLENDING)
		config.flipbookUVB = input.flipbookUVB;
		config.flipbookBlending = true;
	#endif
	<ins>#if defined(_NEAR_FADE)</ins>
		<ins>config.nearFade = true;</ins>
	<ins>#endif</ins></pre>
						
						<figure>
							<div class="vid" style="width: 250px; height:216px;"><iframe src='https://gfycat.com/ifr/inbornhollowantarcticgiantpetrel?controls=0'></iframe></div>
							<figcaption>Adjusting near fade distance.</figcaption>
						</figure>
					</section>
				</section>
				
				<section>
					<h2>Soft Particles</h2>
					
					<p>When billboard particles intersect geometry the sharp transition is both visually jarring and makes their flat nature obvious. The solution for this is to use soft particles, which fade out when there's opaque geometry close behind them. To make this work the particle's fragment depth has to be compared to the depth of whatever has been drawn earlier to the same position in the camera's buffer. This means that we'll have to sample the depth buffer.</p>
					
					<section>
						<h3>Separate Depth Buffer</h3>
						
						<p>Up to this point we've always used a single frame buffer for the camera, which contained both color and depth information. This is the typical frame buffer configuration, but the color and depth data are always stored in separate buffers, known as frame buffer attachments. To access the depth buffer we'll need to define these attachments separately.</p>
						
						<p>The first step is to replace the <em translate="no">_CameraFrameBuffer</em> identifier in <code class="csharp">CameraRenderer</code> with two identifiers, which we'll name <em translate="no">_CameraColorAttachment</em> and <em translate="no">_CameraDepthAttachment</em>.</p>
						
						<pre class="shader" translate="no">	<del>//static int frameBufferId = Shader.PropertyToID("_CameraFrameBuffer");</del>
	<ins>static int</ins>
		<ins>colorAttachmentId = Shader.PropertyToID("_CameraColorAttachment"),</ins>
		<ins>depthAttachmentId = Shader.PropertyToID("_CameraDepthAttachment");</ins></pre>
						
						<p>In <code class="csharp">Render</code> we now have to pass the color attachment to <code class="csharp">PostFXStack.Render</code>, which is functionally equivalent to what we did before.</p>
						
						<pre class="csharp">		if (postFXStack.IsActive) {
			postFXStack.Render(<ins>colorAttachmentId</ins>);
		}</pre>
						
						<p>In <code class="csharp">Setup</code> we now have to get two buffers instead of one composite buffer. The color buffer has no depth, while the depth buffer's format is <code class="csharp">RenderTextureFormat.Depth</code> and its filter mode is <code class="csharp">FilterMode.Point</code>, because blending depth data makes no sense. Both attachments can be set with a single invocation of <code class="csharp">SetRenderTarget</code>, using the same load and store actions for each.</p>
						
						<pre class="csharp">		if (postFXStack.IsActive) {
			if (flags > CameraClearFlags.Color) {
				flags = CameraClearFlags.Color;
			}
			buffer.GetTemporaryRT(
				<ins>colorAttachmentId</ins>, camera.pixelWidth, camera.pixelHeight,
				<ins>0</ins>, FilterMode.Bilinear, useHDR ?
					RenderTextureFormat.DefaultHDR : RenderTextureFormat.Default
			);
			<ins>buffer.GetTemporaryRT(</ins>
				<ins>depthAttachmentId, camera.pixelWidth, camera.pixelHeight,</ins>
				<ins>32, FilterMode.Point, RenderTextureFormat.Depth</ins>
			<ins>);</ins>
			buffer.SetRenderTarget(
				<ins>colorAttachmentId</ins>,
				RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store<ins>,</ins>
				<ins>depthAttachmentId,</ins>
				<ins>RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store</ins>
			);
		}</pre>
						
						<p>Both buffers have to be released as well. Once that's done our RP still works the same way as before, but now with frame buffer attachments that we can accessed separately.</p>
						
						<pre class="csharp">	void Cleanup () {
		lighting.Cleanup();
		if (postFXStack.IsActive) {
			buffer.ReleaseTemporaryRT(<ins>colorAttachmentId</ins>);
			<ins>buffer.ReleaseTemporaryRT(depthAttachmentId);</ins>
		}
	}</pre>
					</section>
					
					<section>
						<h3>Copying Depth</h3>
						
						<p>We cannot sample the depth buffer at the same time that it's used for rendering. We have to make a copy of it. So introduce a <em translate="no">_CameraDepthTexture</em> identifier and add a boolean field to indicate whether we're using a depth texture. We should only bother with copying depth when needed, which we'll determine in <code class="csharp">Render</code> after getting the camera settings. But we'll initially simply always enable it.</p>
						
						<pre class="csharp">	static int
		colorAttachmentId = Shader.PropertyToID("_CameraColorAttachment"),
		depthAttachmentId = Shader.PropertyToID("_CameraDepthAttachment")<ins>,</ins>
		<ins>depthTextureId = Shader.PropertyToID("_CameraDepthTexture")</ins>;

	&hellip;

	<ins>bool useDepthTexture;</ins>

	public void Render (&hellip;) {
		&hellip;
		CameraSettings cameraSettings =
			crpCamera ? crpCamera.Settings : defaultCameraSettings;

		<ins>useDepthTexture = true;</ins>

		&hellip;
	}</pre>
						
						<p>Create a new <code class="csharp">CopyAttachments</code> method that gets a temporary duplicate depth texture if needed and copies the depth attachment data to it. This can be done by invoking <code class="csharp">CopyTexture</code> on the command buffer with a source and destination texture. This is much more efficient than doing it via a full-screen draw call. Also make sure to release the extra depth texture in <code class="csharp">Cleanup</code>.</p>
						
						<pre class="csharp">	void Cleanup () {
		&hellip;
		<ins>if (useDepthTexture) {</ins>
			<ins>buffer.ReleaseTemporaryRT(depthTextureId);</ins>
		<ins>}</ins>
	}

	&hellip;

	<ins>void CopyAttachments () {</ins>
		<ins>if (useDepthTexture) {</ins>
			<ins>buffer.GetTemporaryRT(</ins>
				<ins>depthTextureId, camera.pixelWidth, camera.pixelHeight,</ins>
				<ins>32, FilterMode.Point, RenderTextureFormat.Depth</ins>
			<ins>);</ins>
			<ins>buffer.CopyTexture(depthAttachmentId, depthTextureId);</ins>
			<ins>ExecuteBuffer();</ins>
		<ins>}</ins>
	<ins>}</ins></pre>
						
						<p>We'll copy the attachments only once, after all opaque geometry has been drawn, so after the skybox in <code class="csharp">Render</code>. This means that the depth texture is only available when rendering transparent objects.</p>
						
						<pre class="csharp">		context.DrawSkybox(camera);
		<ins>CopyAttachments();</ins></pre>
						
						
					</section>
					
					<section>
						<h3>Copying Depth Without Post FX</h3>
						
						<p>Copying depth only works if we have a depth attachment to copy from, which is currently only the case when post FX are enabled. To make it possible without post FX we'll also need to use an intermediate frame buffer when a depth texture is used. Introduce a <code class="csharp">useIntermediateBuffer</code> boolean field to keep track of this, initialized in <code class="csharp">Setup</code> before potentially getting the attachments. This should now be done when either a depth texture is used or post FX are active. <code class="csharp">Cleanup</code> is affected in the same way.</p>
						
						<pre class="csharp">	bool useDepthTexture<ins>, useIntermediateBuffer</ins>;
	
	&hellip;
	
	void Setup () {
		context.SetupCameraProperties(camera);
		CameraClearFlags flags = camera.clearFlags;
		
		<ins>useIntermediateBuffer = useDepthTexture || postFXStack.IsActive;</ins>
		if (<ins>useIntermediateBuffer</ins>) {
			if (flags > CameraClearFlags.Color) {
				flags = CameraClearFlags.Color;
			}
			&hellip;
		}

		&hellip;
	}

	void Cleanup () {
		lighting.Cleanup();
		if (<ins>useIntermediateBuffer</ins>) {
			buffer.ReleaseTemporaryRT(colorAttachmentId);
			buffer.ReleaseTemporaryRT(depthAttachmentId);
		<del>//}</del>
			if (useDepthTexture) {
				buffer.ReleaseTemporaryRT(depthTextureId);
			}
		<ins>}</ins>
	}</pre>
						
						<p>But now rendering fails when no post FX are active, because we're only rendering to the intermediate buffer. We have to perform a final copy to the camera's target. Unfortunately we can only use <code class="csharp">CopyTexture</code> to copy to a render texture, not to the final frame buffer. We could use the post FX copy pass to do it, but this step is specific to the camera renderer so we'll create a dedicated <em translate="no">CameraRenderer</em> shader for it. It starts the same as the <em translate="no">PostFX</em> shader but with only a copy pass and it includes its own HLSL file.</p>
						
						<pre translate="no"><ins>Shader "Hidden/Custom RP/Camera Renderer" {</ins>
	
	<ins>SubShader {</ins>
		<ins>Cull Off</ins>
		<ins>ZTest Always</ins>
		<ins>ZWrite Off</ins>
		
		<ins>HLSLINCLUDE</ins>
		<ins>#include "../ShaderLibrary/Common.hlsl"</ins>
		<ins>#include "CameraRendererPasses.hlsl"</ins>
		<ins>ENDHLSL</ins>

		<ins>Pass {</ins>
			<ins>Name "Copy"</ins>

			<ins>HLSLPROGRAM</ins>
				<ins>#pragma target 3.5</ins>
				<ins>#pragma vertex DefaultPassVertex</ins>
				<ins>#pragma fragment CopyPassFragment</ins>
			<ins>ENDHLSL</ins>
		<ins>}</ins>
	<ins>}</ins>
<ins>}</ins></pre>
						
						<p>The new <em translate="no">CameraRendererPasses</em> HLSL file has the same <code>Varyings</code> struct and <code>DefaultPassVertex</code> function as <em translate="no">PostFXStackPasses</em>. It also has a <em translate="no">_SourceTexture</em> texture and a <code>CopyPassFragment</code> function that simply returns the sampled source texture.</p>
						
						<pre translate="no"><ins>#ifndef CUSTOM_CAMERA_RENDERER_PASSES_INCLUDED</ins>
<ins>#define CUSTOM_CAMERA_RENDERER_PASSES_INCLUDED</ins>

<ins>TEXTURE2D(_SourceTexture);</ins>

<ins>struct Varyings { &hellip; };</ins>

<ins>Varyings DefaultPassVertex (uint vertexID : SV_VertexID) { &hellip; }</ins>

<ins>float4 CopyPassFragment (Varyings input) : SV_TARGET {</ins>
	<ins>return SAMPLE_TEXTURE2D_LOD(_SourceTexture, sampler_linear_clamp, input.screenUV, 0);</ins>
<ins>}</ins>

<ins>#endif</ins></pre>
						
						<p>Next, add a material field to <code class="csharp">CameraRenderer</code>. To initialize it create a public constructor method with a shader parameter and have it invoke <code class="csharp">CoreUtils.CreateEngineMaterial</code> with the shader as an argument. That method creates a new material and sets it to be hidden in the editor makes sure that it doesn't get saved as an asset, so we don't have to do this explictly ourselves. It also logs an error if the shader is missing.</p>
						
						<pre class="csharp">	<ins>Material material;</ins>

	<ins>public CameraRenderer (Shader shader) {</ins>
		<ins>material = CoreUtils.CreateEngineMaterial(shader);</ins>
	<ins>}</ins></pre>
						
						<p>Also add a public <code class="csharp">Dispose</code> method that gets rid of the material by passing it to <code class="csharp">CoreUtils.Destroy</code>. That method either regularly or immediately destroys the material, depending on whether Unity is in play mode or not. We need to do this because new RP instances and thus renderers get created whenever the RP asset is modified, which could result in many materials getting created in the editor.</p>
						
						<pre class="csharp">	<ins>public void Dispose () {</ins>
		<ins>CoreUtils.Destroy(material);</ins>
	<ins>}</ins></pre>
						
						<p>Now <code class="csharp">CustomRenderPipeline</code> must provide a shader when it constructs its renderer. So we'll do it in its own constructor method, also adding a parameter for the camera renderer shader to it.</p>
						
						<pre class="csharp">	CameraRenderer renderer<ins>;</ins> <del>// = new CameraRenderer();</del>

	&hellip;

	public CustomRenderPipeline (
		bool allowHDR,
		bool useDynamicBatching, bool useGPUInstancing, bool useSRPBatcher,
		bool useLightsPerObject, ShadowSettings shadowSettings,
		PostFXSettings postFXSettings, int colorLUTResolution<ins>, Shader cameraRendererShader</ins>
	) {
		&hellip;
		<ins>renderer = new CameraRenderer(cameraRendererShader);</ins>
	}</pre>
						
						<p>And it must also invoke <code class="csharp">Dispose</code> on the renderer from now on when it is disposed itself. We've already created a <code class="csharp">Dispose</code> method for it, but for editor code only. Rename that version to <code class="csharp">DisposeForEditor</code> and only have it reset the light mapping delegate.</p>
						
						<pre class="csharp">	<ins>partial void DisposeForEditor ();</ins>

#if UNITY_EDITOR
	
	&hellip;
	
	<ins>partial void DisposeForEditor</ins> () {
		<del>//base.Dispose(disposing);</del>
		Lightmapping.ResetDelegate();
	}</pre>
						
						<p>Then add a new <code class="csharp">Dispose</code> method that isn't editor-only, which invokes its base implementation, the version for the editor, and finally disposes the renderer.</p>
						
						<pre class="csharp">	<ins>protected override void Dispose (bool disposing) {</ins>
		<ins>base.Dispose(disposing);</ins>
		<ins>DisposeForEditor();</ins>
		<ins>renderer.Dispose();</ins>
	<ins>}</ins></pre>
						
						<p>And at the top level <code class="csharp">CustomRenderPipelineAsset</code> must get a shader configuration property and pass it to the pipeline constructor. Then we can finally hook up the shader.</p>
						
						<pre class="csharp">	<ins>[SerializeField]</ins>
	<ins>Shader cameraRendererShader = default;</ins>

	protected override RenderPipeline CreatePipeline () {
		return new CustomRenderPipeline(
			allowHDR, useDynamicBatching, useGPUInstancing, useSRPBatcher,
			useLightsPerObject, shadows, postFXSettings, (int)colorLUTResolution<ins>,</ins>
			<ins>cameraRendererShader</ins>
		);
	}</pre>
						
						<figure>
							<img src="soft-particles/camera-renderer-shader.png" width="320" height="42">
							<figcaption>Camera renderer shader assigned.</figcaption>
						</figure>
						
						<p>At this point <code class="csharp">CameraRenderer</code> has a functional material. Also add the <em translate="no">_SourceTexture</em> identifier to it and the give it a <code class="csharp">Draw</code> method similar to the one in <code class="csharp">PostFXStack</code>, except without a parameter for a pass, as we only have a single pass at this point.</p>
						
						<pre class="csharp">	static int
		colorAttachmentId = Shader.PropertyToID("_CameraColorAttachment"),
		depthAttachmentId = Shader.PropertyToID("_CameraDepthAttachment"),
		depthTextureId = Shader.PropertyToID("_CameraDepthTexture")<ins>,</ins>
		<ins>sourceTextureId = Shader.PropertyToID("_SourceTexture")</ins>;
	
	&hellip;
	
	<ins>void Draw (RenderTargetIdentifier from, RenderTargetIdentifier to) {</ins>
		<ins>buffer.SetGlobalTexture(sourceTextureId, from);</ins>
		<ins>buffer.SetRenderTarget(</ins>
			<ins>to, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store</ins>
		<ins>);</ins>
		<ins>buffer.DrawProcedural(</ins>
			<ins>Matrix4x4.identity, material, 0, MeshTopology.Triangles, 3</ins>
		<ins>);</ins>
	<ins>}</ins></pre>
						
						<p>To finally fix our renderer copy the color attachment to the camera target in <code class="csharp">Render</code> by invoking <code class="csharp">Draw</code>, if post FX aren't active but we do use an intermediate buffer.</p>
						
						<pre class="csharp">		if (postFXStack.IsActive) {
			postFXStack.Render(colorAttachmentId);
		}
		<ins>else if (useIntermediateBuffer) {</ins>
			<ins>Draw(colorAttachmentId, BuiltinRenderTextureType.CameraTarget);</ins>
			<ins>ExecuteBuffer();</ins>
		<ins>}</ins></pre>
					</section>
					
					<section>
						<h3>Reconstructing View-Space Depth</h3>
						
						<p>To sample the depth texture we need the UV coordinates of the fragment, which are in screen space. We can find those by dividing its position by the screen pixel dimensions, which Unity makes available via the XY components of <code>float4 _ScreenParams</code>, so add it to <em translate="no">UnityInput</em>.</p>
						
						<pre translate="no">float4 unity_OrthoParams;
float4 _ProjectionParams;
<ins>float4 _ScreenParams;</ins></pre>
						
						<p>Then we can add the fragment UV and buffer depth to <code>Fragment</code>. Retrieve the buffer depth by sampling the camera depth texture with a point clamp sampler via the <code>SAMPLE_DEPTH_TEXTURE_LOD</code> macro. This macro does the same as <code>SAMPLE_TEXTURE2D_LOD</code> but only returns the R channel.</p>
						
						<pre translate="no"><ins>TEXTURE2D(_CameraDepthTexture);</ins>

struct Fragment {
	float2 positionSS;
	<ins>float2 screenUV;</ins>
	float depth;
	<ins>float bufferDepth;</ins>
};

Fragment GetFragment (float4 positionCS_SS) {
	Fragment f;
	f.positionSS = positionSS.xy;
	<ins>f.screenUV = f.positionSS / _ScreenParams.xy;</ins>
	f.depth = IsOrthographicCamera() ?
		OrthographicDepthBufferToLinear(positionSS.z) : positionSS.w;
	<ins>f.bufferDepth =</ins>
		<ins>SAMPLE_DEPTH_TEXTURE_LOD(_CameraDepthTexture, sampler_point_clamp, f.screenUV, 0);</ins>
	return f;
}</pre>
						
						<p>This gives us the raw depth buffer value. To convert it to view-space depth we can again invoke <code>OrthographicDepthBufferToLinear</code> in case of an orthographic camera, like for the current fragment's depth. The perspective depth also has to be converted, for which we can use <code>LinearEyeDepth</code>. It needs <code>_ZBufferParams</code> as a second argument.</p>
						
						<pre translate="no">	f.bufferDepth = LOAD_TEXTURE2D(_CameraDepthTexture, f.positionSS).r;
	<ins>f.bufferDepth = IsOrthographicCamera() ?</ins>
		<ins>OrthographicDepthBufferToLinear(f.bufferDepth) :</ins>
		<ins>LinearEyeDepth(f.bufferDepth, _ZBufferParams);</ins></pre>
						
						<p><code>_ZBufferParams</code> is another <code>float4</code> made available by Unity that contains conversion factors from raw to linear depth. Add it to <em translate="no">UnityInput</em>.</p>
						
						<pre translate="no">float4 unity_OrthoParams;
float4 _ProjectionParams;
float4 _ScreenParams;
<ins>float4 _ZBufferParams;</ins></pre>
						
						<p>To check whether we sample the buffer depth correctly return it scaled in <code>UnlitPassFragment</code>, like we tested the fragment depth earlier.</p>
						
						<pre translate="no">	InputConfig config = GetInputConfig(input.positionCS_SS, input.baseUV);
	<ins>return float4(config.fragment.bufferDepth.xxx / 20.0, 1.0);</ins></pre>
						
						<figure>
							<img src="soft-particles/buffered-depth-perspective.png" width="440" height="280" alt="perspective">
							<img src="soft-particles/buffered-depth-orthographic.png" width="440" height="280" alt="orthographic">
							<figcaption>Buffer depth, perspective and orthographic projections.</figcaption>
						</figure>
						
						<p>Remove the debug visualization once it's clear that the sampled depth is correct.</p>
						
						<pre translate="no">	<del>//return float4(config.fragment.bufferDepth.xxx / 20.0, 1.0);</del></pre>
					</section>
					
					<section>
						<h3>Optional Depth Texture</h3>
						
						<p>Copying depth requires extra work, especially when no post FX are used as that also requires intermediate buffers and an extra copy to the camera target. So let's make it configurable whether our RP supports copying depth. We'll create a new <code>CameraBufferSettings</code> struct for this, put in its own file, used to group all settings related to the camera buffers. Besides a toggle for copying depth also put the toggle to allow HDR in it as well. And also introduce a separate toggle to control whether depth is copied when rendering reflections. This is useful because reflections are rendered without post FX and particle systems don't show up in reflections either, so copying depth for reflections is expensive and likely useless. We do make it possible because depth could be used for other effects as well, which might be visible in reflections. Even then, keep in mind that the depth buffer is distinct per cube map reflection face, so there will be depth seams along the cube map edges.</p>
						
						<pre class="csharp"><ins>[System.Serializable]</ins>
<ins>public struct CameraBufferSettings {</ins>

	<ins>public bool allowHDR;</ins>

	<ins>public bool copyDepth, copyDepthReflections;</ins>
<ins>}</ins></pre>
						
						<p>Replace the current HDR toggle of <code>CustomRenderPipelineAsset</code> with these camera buffer settings.</p>
						
						<pre class="csharp">	<del>//[SerializeField]</del>
	<del>//bool allowHDR = true;</del>

	<ins>[SerializeField]</ins>
	<ins>CameraBufferSettings cameraBuffer = new CameraBufferSettings {</ins>
		<ins>allowHDR = true</ins>
	<ins>};</ins>

	protected override RenderPipeline CreatePipeline () {
		return new CustomRenderPipeline(
			<ins>cameraBuffer</ins>, useDynamicBatching, useGPUInstancing, useSRPBatcher,
			useLightsPerObject, shadows, postFXSettings, (int)colorLUTResolution,
			cameraRendererShader
		);
	}</pre>
						
						<p>Apply the change to <code>CustomRenderPipeline</code> as well.</p>
						
						<pre class="csharp">	<del>//bool allowHDR;</del>
	<ins>CameraBufferSettings cameraBufferSettings;</ins>

	&hellip;

	public CustomRenderPipeline (
		<ins>CameraBufferSettings cameraBufferSettings</ins>,
		bool useDynamicBatching, bool useGPUInstancing, bool useSRPBatcher,
		bool useLightsPerObject, ShadowSettings shadowSettings,
		PostFXSettings postFXSettings, int colorLUTResolution, Shader cameraRendererShader
	) {
		this.colorLUTResolution = colorLUTResolution;
		<del>//this.allowHDR = allowHDR;</del>
		<ins>this.cameraBufferSettings = cameraBufferSettings;</ins>
		&hellip;
	}

	protected override void Render (ScriptableRenderContext context, Camera[] cameras) {
		foreach (Camera camera in cameras) {
			renderer.Render(
				context, camera, <ins>cameraBufferSettings</ins>,
				useDynamicBatching, useGPUInstancing, useLightsPerObject,
				shadowSettings, postFXSettings, colorLUTResolution
			);
		}
	}</pre>
						
						<p><code>CameraRenderer.Render</code> now has to use the appropriate settings depending on whether it's rendering reflections or not.</p>
						
						<pre class="csharp">	public void Render (
		ScriptableRenderContext context, Camera camera,
		<ins>CameraBufferSettings bufferSettings</ins>,
		bool useDynamicBatching, bool useGPUInstancing, bool useLightsPerObject,
		ShadowSettings shadowSettings, PostFXSettings postFXSettings,
		int colorLUTResolution
	) {
		&hellip;
		
		<del>//useDepthTexture = true;</del>
		<ins>if (camera.cameraType == CameraType.Reflection) {</ins>
			<ins>useDepthTexture = bufferSettings.copyDepthReflection;</ins>
		<ins>}</ins>
		<ins>else {</ins>
			<ins>useDepthTexture = bufferSettings.copyDepth;</ins>
		<ins>}</ins>

		&hellip;
		useHDR = <ins>bufferSettings.allowHDR</ins> &amp;&amp; camera.allowHDR;

		&hellip;
	}</pre>
						
						<figure>
							<img src="soft-particles/camera-buffer-settings.png" width="320" height="122">
							<figcaption>Camera buffer settings, with HDR and non-reflection copy depth enabled.</figcaption>
						</figure>
						
						<p>Besides the settings for the entire RP we can also add a copy-depth toggle to <code>CameraSettings</code>, enabled by default.</p>
						
						<pre class="csharp">	<ins>public bool copyDepth = true;</ins></pre>
						
						<figure>
							<img src="soft-particles/camera-copy-depth.png" width="320" height="100">
							<figcaption>Camera copy depth toggle.</figcaption>
						</figure>
						
						<p>Then for regular cameras a depth texture is only used if both the RP and the camera have it enabled, similar to how HDR is controlled.</p>
						
						<pre class="csharp">		if (camera.cameraType == CameraType.Reflection) {
			useDepthTexture = bufferSettings.copyDepthReflection;
		}
		else {
			useDepthTexture = bufferSettings.copyDepth <ins>&amp;&amp; cameraSettings.copyDepth</ins>;
		}</pre>
					</section>
					
					<section>
						<h3>Missing Texture</h3>
						
						<p>As the depth texture is optional it might not exist. When a shader samples it anyway the result will be random. It could be either an empty texture or an old copy, potentially of another camera. It's also possible that a shader samples the depth texture too early, during the opaque rendering phase. The least we can do is make sure that invalid samples will produce consistent results. We do this by creating a default missing texture in the constructor method of <code class="csharp">CameraRender</code>. There is no <code class="shader">CoreUtils</code> method for textures, so we'll set its hide flags to <code class="csharp">HideFlags.HideAndDontSave</code> ourselves. Name it <em translate="no">Missing</em> so it's obvious that a wrong texture is used when inspecting shader properties via the frame debugger. Make it a simple 1&times;1 texture with all channels set to 0.5. Also destroy it appropriately when the renderer is disposed.</p>
						
						<pre class="csharp">	<ins>Texture2D missingTexture;</ins>

	public CameraRenderer (Shader shader) {
		material = CoreUtils.CreateEngineMaterial(shader);
		<ins>missingTexture = new Texture2D(1, 1) {</ins>
			<ins>hideFlags = HideFlags.HideAndDontSave,</ins>
			<ins>name = "Missing"</ins>
		<ins>};</ins>
		<ins>missingTexture.SetPixel(0, 0, Color.white * 0.5f);</ins>
		<ins>missingTexture.Apply(true, true);</ins>
	}

	public void Dispose () {
		CoreUtils.Destroy(material);
		<ins>CoreUtils.Destroy(missingTexture);</ins>
	}</pre>
						
						<p>Use the missing texture for the depth texture at the end of <code class="csharp">Setup</code>.</p>
						
						<pre class="csharp">	void Setup () {
		&hellip;
		buffer.BeginSample(SampleName);
		<ins>buffer.SetGlobalTexture(depthTextureId, missingTexture);</ins>
		ExecuteBuffer();
	}</pre>
					</section>
					
					<section>
						<h3>Fading Particles Nearby Background</h3>
						
						<p>Now that we have a functional depth texture we can move on to finally support soft particles. The first step is to add shader properties for a soft particles keyword toggle, a distance, and a range to <em translate="no">UnlitParticles</em>, similar to the near fade properties. In this case the distance is measured from whatever is behind the particles, so we set it to zero by default.</p>
						
						<pre translate="no">		<ins>[Toggle(_SOFT_PARTICLES)] _SoftParticles ("Soft Particles", Float) = 0</ins>
		<ins>_SoftParticlesDistance ("Soft Particles Distance", Range(0.0, 10.0)) = 0</ins>
		<ins>_SoftParticlesRange ("Soft Particles Range", Range(0.01, 10.0)) = 1</ins></pre>
						
						<p>Add the shader feature for it as well.</p>
						
						<pre translate="no">			<ins>#pragma shader_feature _SOFT_PARTICLES</ins></pre>
						
						<p>Like for near fading, set an appropriate config field to <code>true</code> in <code>UnlitPassFragment</code> if the keyword is defined.</p>
						
						<pre translate="no">	#if defined(_NEAR_FADE)
		config.nearFade = true;
	#endif
	<ins>#if defined(_SOFT_PARTICLES)</ins>
		<ins>config.softParticles = true;</ins>
	<ins>#endif</ins></pre>
						
						<p>In <em translate="no">UnlitInput</em>, add the new shader properties to <code>UnityPerMaterial</code> and the field to <code>InputConfig</code>.</p>
						
						<pre translate="no">UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	&hellip;
	<ins>UNITY_DEFINE_INSTANCED_PROP(float, _SoftParticlesDistance)</ins>
	<ins>UNITY_DEFINE_INSTANCED_PROP(float, _SoftParticlesRange)</ins>
	UNITY_DEFINE_INSTANCED_PROP(float, _Cutoff)
	UNITY_DEFINE_INSTANCED_PROP(float, _ZWrite)
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)

#define INPUT_PROP(name) UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, name)

struct InputConfig {
	&hellip;
	<ins>bool softParticles;</ins>
};

InputConfig GetInputConfig (float4 positionCC_SS, float2 baseUV) {
	&hellip;
	<ins>c.softParticles = false;</ins>
	return c;
}</pre>
						
						<p>Then apply another near attenuation in <code>GetBase</code>, this time based on the fragment's buffer depth minus its own depth.</p>
						
						<pre translate="no">	if (c.nearFade) {
		float nearAttenuation = (c.fragment.depth - INPUT_PROP(_NearFadeDistance)) /
			INPUT_PROP(_NearFadeRange);
		baseMap.a *= saturate(nearAttenuation);
	}
	<ins>if (c.softParticles) {</ins>
		<ins>float depthDelta = c.fragment.bufferDepth - c.fragment.depth;</ins>
		<ins>float nearAttenuation = (depthDelta - INPUT_PROP(_SoftParticlesDistance)) /</ins>
			<ins>INPUT_PROP(_SoftParticlesRange);</ins>
		<ins>baseMap.a *= saturate(nearAttenuation);</ins>
	<ins>}</ins></pre>
						
						<figure>
							<div class="vid" style="width: 250px; height:216px;"><iframe src='https://gfycat.com/ifr/marrieduglyaardvark?controls=0'></iframe></div>
							<figcaption>Soft particles, adjusting fade range.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>No Copy Texture Support</h3>
						
						<p>This all works fine, but only as long as direct copying of textures via <code class="csharp">CopyTexture</code> is supported, at least at a basic level. This is mostly the case, but not for WebGL 2.0. So if we also want to support WebGL 2.0 we have fall back to copying via our shader instead, which is less efficient but at least works.</p>
						
						<p>Keep track of whether <code class="csharp">CopyTexture</code> is supported via a static boolean field in <code>CameraRenderer</code>. Initially set it to <code>false</code> so we can test the fallback approach even though our development machines all support it.</p>
						
						<pre class="csharp">	<ins>static bool copyTextureSupported = false;</ins></pre>						
						
						<p>In <code class="csharp">CopyAttachments</code> copy the depth depth via <code class="csharp">CopyTexture</code> if supported, otherwise fall back to using our <code class="csharp">Draw</code> method.</p>
						
						<pre class="csharp">	void CopyAttachments () {
		if (useDepthTexture) {
			buffer.GetTemporaryRT(
				depthTextureId, camera.pixelWidth, camera.pixelHeight,
				32, FilterMode.Point, RenderTextureFormat.Depth
			);
			<ins>if (copyTextureSupported) {</ins>
				buffer.CopyTexture(depthAttachmentId, depthTextureId);
			<ins>}</ins>
			<ins>else {</ins>
				<ins>Draw(depthAttachmentId, depthTextureId);</ins>
			<ins>}</ins>
			ExecuteBuffer();
		}
	}</pre>
						
						<p>This initially fails to produce correct results because <code class="csharp">Draw</code> changes the render target, so further drawing goes wrong. We have to set the render target back to the camera buffer afterwards, loading our attachments again.</p>
						
						<pre class="csharp">			if (copyTextureSupported) {
				buffer.CopyTexture(depthAttachmentId, depthTextureId);
			}
			else {
				Draw(depthAttachmentId, depthTextureId);
				<ins>buffer.SetRenderTarget(</ins>
					<ins>colorAttachmentId,</ins>
					<ins>RenderBufferLoadAction.Load, RenderBufferStoreAction.Store,</ins>
					<ins>depthAttachmentId,</ins>
					<ins>RenderBufferLoadAction.Load, RenderBufferStoreAction.Store</ins>
				<ins>);</ins>
			}</pre>
						
						<p>The second thing that goes wrong is that the depth doesn't get copied at all, because our copy pass only writes to the default shader target, which is for color data, not depth. To copy depth instead we need to add a second copy-depth pass to the <em translate="no">CameraRenderer</em> shader that writes depth instead of color. We do that by settings its <code>ColorMask</code> to zero and turning <code>ZWrite</code> on. It also needs a special fragment function, which we'll name <code>CopyDepthPassFragment</code>.</p>
						
						<pre translate="no">		<ins>Pass {</ins>
			<ins>Name "Copy Depth"</ins>

			<ins>ColorMask 0</ins>
			<ins>ZWrite On</ins>
			
			<ins>HLSLPROGRAM</ins>
				<ins>#pragma target 3.5</ins>
				<ins>#pragma vertex DefaultPassVertex</ins>
				<ins>#pragma fragment CopyDepthPassFragment</ins>
			<ins>ENDHLSL</ins>
		<ins>}</ins></pre>
						
						<p>The new fragment function has to sample depth and return it as a single <code>float</code> with the <code>SV_DEPTH</code> semantic, instead of a <code>float4</code> with the <code>SV_TARGET</code> semantic. This way we sample the raw depth buffer value and directly use it for the new depth of the fragment.</p>
						
						<pre translate="no">float4 CopyPassFragment (Varyings input) : SV_TARGET {
	return SAMPLE_TEXTURE2D_LOD(_SourceTexture, sampler_linear_clamp, input.screenUV, 0);
}

<ins>float CopyDepthPassFragment (Varyings input) : SV_DEPTH {</ins>
	<ins>return SAMPLE_DEPTH_TEXTURE_LOD(_SourceTexture, sampler_point_clamp, input.screenUV, 0);</ins>
<ins>}</ins></pre>
						
						<p>Next, go back to <code>CameraRenderer</code> and add a boolean parameter to <code>Draw</code> to indicate whether we're drawing from and to depth, set to <code>false</code> by default. If so, use the second pass instead of the first pass.</p>
						
						<pre class="csharp">	public void Draw (
		RenderTargetIdentifier from, RenderTargetIdentifier to<ins>, bool isDepth = false</ins>
	) {
		&hellip;
		buffer.DrawProcedural(
			Matrix4x4.identity, material, <ins>isDepth ? 1 :</ins> 0, MeshTopology.Triangles, 3
		);
	}</pre>
						
						<p>Then indicate that we're working with depth when copying the depth buffer.</p>
						
						<pre class="csharp">				Draw(depthAttachmentId, depthTextureId<ins>, true</ins>);</pre>
						
						<p>After veryfying that this approach also works, determine whether <code class="csharp">CopyTexture</code> is supported by checking <code class="csharp">SystemInfo.copyTextureSupport</code>. Any level of support greater than none is sufficient.</p>
						
						<pre class="csharp">	static bool copyTextureSupported =
		<ins>SystemInfo.copyTextureSupport > CopyTextureSupport.None</ins>;</pre>
					</section>
					
					<section>
						<h3>Gizmos and Depth</h3>
						
						<p>Now that we have a way to draw depth, we can use it to make our gizmos depth-aware again in combination with post FX or when using a depth texture. In <code class="csharp">DrawGizmosBeforeFX</code>, before drawing the first gizmos, copy depth to the camera target if we use an intermediate buffer.</p>
						
						<pre class="csharp">	partial void DrawGizmosBeforeFX () {
		if (Handles.ShouldRenderGizmos()) {
			<ins>if (useIntermediateBuffer) {</ins>
				<ins>Draw(depthAttachmentId, BuiltinRenderTextureType.CameraTarget, true);</ins>
				<ins>ExecuteBuffer();</ins>
			<ins>}</ins>
			context.DrawGizmos(camera, GizmoSubset.PreImageEffects);
		}
	}</pre>
						
						<figure>
							<img src="soft-particles/gizmos-depth.png" width="120" height="120">
							<figcaption>Gizmos recognizing depth.</figcaption>
						</figure>
					</section>
				</section>
				
				<section>
					<h2>Distortion</h2>
					
					<p>Another feature of Unity's particles that we'll also support is distortion, which can be used to create effects like atmospheric refraction caused by heat. This requires sampling of the color buffer, like we're already sampling the depth buffer, but with the addition of a UV offset.</p>
					
					<section>
						<h3>Color Copy Texture</h3>
						
						<p>We begin by adding toggles for copying color to <code class="csharp">CameraBufferSettings</code>, again a separate one for regular and for reflection cameras.</p>
						
						<pre class="csharp">	public bool <ins>copyColor, copyColorReflection,</ins> copyDepth, copyDepthReflection;</pre>
						
						<figure>
							<img src="soft-particles/camera-buffer-settings-color.png" width="320" height="120">
							<figcaption>Copying color and depth.</figcaption>
						</figure>
						
						<p>Make copying color configurable per camera as well.</p>
						
						<pre class="csharp">	public bool <ins>copyColor = true,</ins> copyDepth = true;</pre>
						
						<figure>
							<img src="soft-particles/camera-copy-color.png" width="320" height="60">
							<figcaption>Also enabled for camera.</figcaption>
						</figure>
						
						<p><code class="csharp">CameraRendering</code> now also has to keep track of the identifier for a color texture and whether a color texture is used.</p>
						
						<pre class="csharp">		<ins>colorTextureId = Shader.PropertyToID("_CameraColorTexture"),</ins>
		depthTextureId = Shader.PropertyToID("_CameraDepthTexture"),
		sourceTextureId = Shader.PropertyToID("_SourceTexture");
	
	&hellip;
	
	bool <ins>useColorTexture,</ins> useDepthTexture, useIntermediateBuffer;

	&hellip;

	public void Render (&hellip;) {
		&hellip;

		if (camera.cameraType == CameraType.Reflection) {
			<ins>useColorTexture = bufferSettings.copyColorReflection;</ins>
			useDepthTexture = bufferSettings.copyDepthReflection;
		}
		else {
			<ins>useColorTexture = bufferSettings.copyColor &amp;&amp; cameraSettings.copyColor;</ins>
			useDepthTexture = bufferSettings.copyDepth &amp;&amp; cameraSettings.copyDepth;
		}

		&hellip;
	}</pre>
						
						<p>Whether we use an intermediate buffer now also depends on whether a color texture is used. And we should also initially set the color texture to the missing texture. Release it when cleaning up as well.</p>
						
						<pre class="csharp">	void Setup () {
		&hellip;

		useIntermediateBuffer =
			<ins>useColorTexture ||</ins> useDepthTexture || postFXStack.IsActive;
		&hellip;
		buffer.BeginSample(SampleName);
		<ins>buffer.SetGlobalTexture(colorTextureId, missingTexture);</ins>
		buffer.SetGlobalTexture(depthTextureId, missingTexture);
		ExecuteBuffer();

	}

	void Cleanup () {
		lighting.Cleanup();
		if (useIntermediateBuffer) {
			buffer.ReleaseTemporaryRT(colorAttachmentId);
			buffer.ReleaseTemporaryRT(depthAttachmentId);
			<ins>if (useColorTexture) {</ins>
				<ins>buffer.ReleaseTemporaryRT(colorTextureId);</ins>
			<ins>}</ins>
			if (useDepthTexture) {
				buffer.ReleaseTemporaryRT(depthTextureId);
			}
		}
	}</pre>
						
						<p>We now need to copy the camera attachments when either a color or a depth texture is used, or both. Let's make the invocation of <code class="csharp">CopyAttachments</code> dependent on that.</p>
						
						<pre class="csharp">		context.DrawSkybox(camera);
		<ins>if (useColorTexture || useDepthTexture) {</ins>
			CopyAttachments();
		<ins>}</ins></pre>
						
						<p>Then we can have it copy both textures separately and afterwards reset the render target and execute the buffer once.</p>
						
						<pre class="csharp">	void CopyAttachments () {
		<ins>if (useColorTexture) {</ins>
			<ins>buffer.GetTemporaryRT(</ins>
				<ins>colorTextureId, camera.pixelWidth, camera.pixelHeight,</ins>
				<ins>0, FilterMode.Bilinear, useHDR ?</ins>
					<ins>RenderTextureFormat.DefaultHDR : RenderTextureFormat.Default</ins>
			<ins>);</ins>
			<ins>if (copyTextureSupported) {</ins>
				<ins>buffer.CopyTexture(colorAttachmentId, colorTextureId);</ins>
			<ins>}</ins>
			<ins>else {</ins>
				<ins>Draw(colorAttachmentId, colorTextureId);</ins>
			<ins>}</ins>
		<ins>}</ins>
		if (useDepthTexture) {
			buffer.GetTemporaryRT(
				depthTextureId, camera.pixelWidth, camera.pixelHeight,
				32, FilterMode.Point, RenderTextureFormat.Depth
			);
			if (copyTextureSupported) {
				buffer.CopyTexture(depthAttachmentId, depthTextureId);
			}
			else {
				Draw(depthAttachmentId, depthTextureId, true);
				<del>//buffer.SetRenderTarget(&hellip;);</del>
			}
			<del>//ExecuteBuffer();</del>
		}
		<ins>if (!copyTextureSupported) {</ins>
			<ins>buffer.SetRenderTarget(</ins>
				<ins>colorAttachmentId,</ins>
				<ins>RenderBufferLoadAction.Load, RenderBufferStoreAction.Store,</ins>
				<ins>depthAttachmentId,</ins>
				<ins>RenderBufferLoadAction.Load, RenderBufferStoreAction.Store</ins>
			<ins>);</ins>
		<ins>}</ins>
		<ins>ExecuteBuffer();</ins>
	}</pre>
						
					</section>
					
					<section>
						<h3>Sampling the Buffer Color</h3>
						
						<p>To sample the camera color texture add it to <em translate="no">Fragment</em>. We won't add a buffer color property to <code>Fragment</code>, as we're not interested in the color at its exact location. Instead we introduce a <code>GetBufferColor</code> function that takes a fragment and UV offset as parameters, retuning the sampled color.</p>
						
						<pre translate="no"><ins>TEXTURE2D(_CameraColorTexture);</ins>
TEXTURE2D(_CameraDepthTexture);

struct Fragment { &hellip; };

Fragment GetFragment (float4 positionCS_SS) { &hellip; }

<ins>float4 GetBufferColor (Fragment fragment, float2 uvOffset = float2(0.0, 0.0)) {</ins>
	<ins>float2 uv = fragment.screenUV + uvOffset;</ins>
	<ins>return SAMPLE_TEXTURE2D_LOD(_CameraColorTexture, sampler_linear_clamp, uv);</ins>
<ins>}</ins></pre>
						
						<p>To test this return the buffer color with a small offset like 5% in both dimensions in <code>UnlitPassFragment</code>.</p>
						
						<pre translate="no">	InputConfig config = GetInputConfig(input.positionCS_SS, input.baseUV);
	<ins>return GetBufferColor(config.fragment, 0.05);</ins></pre>
						
						<figure>
							<img src="distortion/color-buffer-with-offset.png" width="440" height="280">
							<figcaption>Sampling camera color buffer with offset.</figcaption>
						</figure>
						
						<p>Note that because the colors are copied after the opaque phase transparent objects are missing from it. Thus the particles erase all transparent objects that were drawn before them, including each other. At the same time depth plays no role in this case, so colors of fragments that are closer to the camera plane than the fragment itself get copied as well. Remove the debug visualization when it's clear that it works.</p>
						
						<pre translate="no">	<del>//return GetBufferColor(config.fragment, 0.05);</del></pre>
						
						<aside>
							<h3>Could sampling in front of a fragment be avoided?</h3>
							<div>
								<p>Yes, up to a point. For an example, see the <a href="../../flow/looking-through-water/index.html">Flow / Looking Through Water</a> tutorial.</p>
							</div>
						</aside>
					</section>
					
					<section>
						<h3>Distortion Vectors</h3>
						
						<p>To create a useful distortion effect we need a map of smoothly transitioning distortion vectors. <a href="distortion/particles-single-distortion.png">Here</a> is a simple map for a single round particle. It's a normal map, so import it as such.</p>
						
						<figure>
							<img src="distortion/particles-single-distortion.png" width="128" height="128">
							<figcaption>Particle distortion map.</figcaption>
						</figure>
						
						<p>Add a keyword toggle shader property to <em translate="no">UnlitParticles</em>, along with distortion map and strength properties. The distortion will be applied as a screen-space UV offset, so small values are required. Let's use a strength range of 0&ndash;0.2, with 0.1 as the default.</p>
						
						<pre translate="no">		<ins>[Toggle(_DISTORTION)] _Distortion ("Distortion", Float) = 0</ins>
		<ins>[NoScaleOffset] _DistortionMap("Distortion Vectors", 2D) = "bumb" {}</ins>
		<ins>_DistortionStrength("Distortion Strength", Range(0.0, 0.2)) = 0.1</ins></pre>
						
						<figure>
							<img src="distortion/distortion-enabled.png" width="320" height="134">
							<figcaption>Distortion enabled.</figcaption>
						</figure>
						
						<p>Add the required shader feature.</p>
						
						<pre translate="no">			<ins>#pragma shader_feature _DISTORTION</ins></pre>
						
						<p>Then add the distortion map and strength properties to <em translate="no">UnlitInput</em>.</p>
						
						<pre translate="no">TEXTURE2D(_BaseMap);
<ins>TEXTURE2D(_DistortionMap);</ins>
SAMPLER(sampler_BaseMap);

UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)
	&hellip;
	UNITY_DEFINE_INSTANCED_PROP(float, _SoftParticlesRange)
	<ins>UNITY_DEFINE_INSTANCED_PROP(float, _DistortionStrength)</ins>
	&hellip;
UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)</pre>
						
						<p>Introduce a new <code>GetDistortion</code> function that returns a <code>float2</code> vector. Have it sample the distortion map and apply flipbook blending as for the base map, then decode the normal scaled by the distortion strength. We only need the XY components of the vector, so discard Z.</p>
						
						<pre translate="no"><ins>float2 GetDistortion (InputConfig c) {</ins>
	<ins>float4 rawMap = SAMPLE_TEXTURE2D(_DistortionMap, sampler_BaseMap, c.baseUV);</ins>
	<ins>if (c.flipbookBlending) {</ins>
		<ins>rawMap = lerp(</ins>
			<ins>rawMap, SAMPLE_TEXTURE2D(_DistortionMap, sampler_BaseMap, c.flipbookUVB.xy),</ins>
			<ins>c.flipbookUVB.z</ins>
		<ins>);</ins>
	<ins>}</ins>
	<ins>return DecodeNormal(rawMap, INPUT_PROP(_DistortionStrength)).xy;</ins>
<ins>}</ins></pre>
						
						<p>In <code>UnlitPassFragment</code>, if distortion is enabled retrieve it and use it as an offset to get the buffer color, overriding the base color. Do this after clipping.</p>
						
						<pre translate="no">	float4 base = GetBase(config);
	#if defined(_CLIPPING)
		clip(base.a - GetCutoff(config));
	#endif
	<ins>#if defined(_DISTORTION)</ins>
		<ins>float2 distortion = GetDistortion(config);</ins>
		<ins>base = GetBufferColor(config.fragment, distortion);</ins>
	<ins>#endif</ins></pre>
						
						<figure>
							<img src="distortion/distorted-color-buffer.png" width="440" height="280">
							<figcaption>Distorted color buffer.</figcaption>
						</figure>
						
						<p>The result is that particles radially distort the color texture, except in their corners because the distortion vectors are zero there. But the distortion effect should depend on the visual strength of the particles, which is controlled by the original base alpha. So modulate the distortion offset vector with the base alpha.</p>
						
						<pre translate="no">		float2 distortion = GetDistortion(config) <ins>* base.a</ins>;</pre>
						
						<figure>
							<img src="distortion/modulated-distortion.png" width="440" height="280">
							<figcaption>Modulated distortion.</figcaption>
						</figure>
						
						<p>At this point we still get hard edges betraying that particles completely overlap each other and are rectangular. We hide that by keeping the original alpha of the particles.</p>
						
						<pre translate="no">		base<ins>.rgb</ins> = GetBufferColor(config.fragment, distortion)<ins>.rgb</ins>;</pre>
						
						<figure>
							<img src="distortion/faded-distortion.png" width="440" height="280">
							<figcaption>Faded distortion.</figcaption>
						</figure>
						
						<p>Now the distorted color texture samples fade as well, which makes the undistorted background and other particles partially visible again. The result is a smooth mess that doesn't make physical sense but is enough to provide the illusion of atmospheric refraction. This can be improved further by tweaking the distortion strength along with smoothly fading particles in and out by adjusting their color during their lifetime. Also, the offset vectors are aligned with the screen and aren't affected by the orientation of the particle. So if the particles are set to rotate during their lifetime their individual distortion patterns will appear to twist.</p>
						
						<figure>
							<div class="vid" style="width: 250px; height:216px;"><iframe src='https://gfycat.com/ifr/accomplishedcompetentdotterel?controls=0'></iframe></div>
							<figcaption>Distortion effect.</figcaption>
						</figure>
						
					</section>
					
					<section>
						<h3>Distortion Blend</h3>
						
						<p>Currently when distortion is enabled we completely replace the original color of the particles, only keeping their alpha. The particle color can be combined with the distorted color buffer in various ways. We'll add a simple distortion blend shader property, to interpolate between the particle's own color and the distortion that it causes, using the same approach as Unity's particle shaders.</p>
						
						<pre translate="no">		_DistortionStrength("Distortion Strength", Range(0.0, 0.2)) = 0.1
		<ins>_DistortionBlend("Distortion Blend", Range(0.0, 1.0)) = 1</ins></pre>
						
						<figure>
							<img src="distortion/distortion-blend.png" width="320" height="42">
							<figcaption>Distortion blend slider.</figcaption>
						</figure>
						
						<p>Add the property to <em translate="no">UnlitInput</em> along with a function to get it.</p>
						
						<pre translate="no">	UNITY_DEFINE_INSTANCED_PROP(float, _DistortionStrength)
	<ins>UNITY_DEFINE_INSTANCED_PROP(float, _DistortionBlend)</ins>

&hellip;

<ins>float GetDistortionBlend (InputConfig c) {</ins>
	<ins>return INPUT_PROP(_DistortionBlend);</ins>
<ins>}</ins></pre>
						
						<p>The idea is that when the blend slider is at 1 we only see the distortion. Lowering it makes the particle color appear, but it won't completely hide the distortion. Instead we interpolate from distortion to particle color based on its alpha minus the blend slider, saturated. Thus when distortion is enabled the particle's own color will always be weaker and appear smaller compared to when distortion is disabled, unless where it's fully opaque. Perform the interpolation in <code>UnlitPassFragment</code>.</p>
						
						<pre translate="no">	#if defined(_DISTORTION)
		float2 distortion = GetDistortion(config) * base.a;
		base.rgb = <ins>lerp(</ins>
			GetBufferColor(config.fragment, distortion).rgb<ins>, base.rgb,</ins>
			<ins>saturate(base.a - GetDistortionBlend(config))</ins>
		<ins>)</ins>;
	#endif</pre>
						
						<p>This looks better for more complex particles, like our flipbook example. So <a href="distortion/particles-flipbook-distortion.png">here</a> is a distortion texture for the flipbook.</p>
						
						<figure>
							<img src="distortion/particles-flipbook-distortion.png" width="512" height="512">
							<figcaption>Distortion map for particle flipbook.</figcaption>
						</figure>
						
						<p>This can be used to create interesting distortion effects. Realistic effects would be subtle, as a little distortion is enough when the system is in motion. But for demonstration purposes I made the effects strong so they're visually obvious, even in screenshots.</p>
						
						<figure>
							<div class="vid" style="width: 250px; height:216px;"><iframe src='https://gfycat.com/ifr/digitalcontentirrawaddydolphin?controls=0'></iframe></div>
							<figcaption>Distortion with flipbook and post FX.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Fixing Nonstandard Cameras</h3>
						
						<p>Our current approach works when we only use a single camera, but it fails when rendering to an intermediate texture without post FX. This happens because we're performing a regular copy to the camera target, which ignores the viewport and final blend modes. So <code>CameraRenderer</code> also needs a <code>FinalPass</code> method. It's a copy of <code>PostFXStack.FinalPass</code>, except that we'll use the regular copy pass, so we should set the blend mode back to one-zero afterwards to not affect other copy actions. The source texture is always the color attachment and the final blend mode becomes a parameter.</p>
						
						<pre class="csharp">	void DrawFinal (<ins>CameraSettings.FinalBlendMode finalBlendMode</ins>) {
		buffer.SetGlobalFloat(srcBlendId, (float)finalBlendMode.source);
		buffer.SetGlobalFloat(dstBlendId, (float)finalBlendMode.destination);
		buffer.SetGlobalTexture(<ins>sourceTextureId</ins>, <ins>colorAttachmentId</ins>);
		buffer.SetRenderTarget(
			BuiltinRenderTextureType.CameraTarget,
			finalBlendMode.destination == BlendMode.Zero ?
				RenderBufferLoadAction.DontCare : RenderBufferLoadAction.Load,
			RenderBufferStoreAction.Store
		);
		buffer.SetViewport(camera.pixelRect);
		buffer.DrawProcedural(
			Matrix4x4.identity, material, <ins>0</ins>, MeshTopology.Triangles, 3
		);
		<ins>buffer.SetGlobalFloat(srcBlendId, 1f);</ins>
		<ins>buffer.SetGlobalFloat(dstBlendId, 0f);</ins>
	}</pre>
						
						<p>In this case we'll name the blend mode shader properties <em translate="no">_CameraSrcBlend</em> and <em translate="no">_CameraSrcBlend</em>.</p>
						
						<pre class="csharp">		sourceTextureId = Shader.PropertyToID("_SourceTexture")<ins>,</ins>
		<ins>srcBlendId = Shader.PropertyToID("_CameraSrcBlend"),</ins>
		<ins>dstBlendId = Shader.PropertyToID("_CameraDstBlend")</ins>;</pre>
						
						<p>Adjust the copy pass of <em translate="no">CameraRenderer</em> to rely on these properties.</p>
						
						<pre translate="no">		Pass {
			Name "Copy"

			<ins>Blend [_CameraSrcBlend] [_CameraDstBlend]</ins>

			HLSLPROGRAM
				#pragma target 3.5
				#pragma vertex DefaultPassVertex
				#pragma fragment CopyPassFragment
			ENDHLSL
		}</pre>
						
						<p>Finally, invoke <code class="csharp">DrawFinal</code> instead of <code class="csharp">Draw</code> in <code class="csharp">Render</code>.</p>
						
						<pre class="csharp">		if (postFXStack.IsActive) {
			postFXStack.Render(colorAttachmentId);
		}
		else if (useIntermediateBuffer) {
			<ins>DrawFinal(cameraSettings.finalBlendMode);</ins>
			ExecuteBuffer();
		}</pre>
						
						<p>Note that the color and depth textures only contain what the current camera rendered. Distortion particles and similar effects won't pick up data from other cameras.</p>
						
						<p>The next tutorial is <a href="../render-scale/index.html">Render Scale</a>.</p>
					</section>
					
					<a href="../../license/index.html" class="license">license</a>
					<a href="https://bitbucket.org/catlikecodingunitytutorials/custom-srp-15-particles/" class="repository">repository</a>
					<a href="Particles.pdf" download rel="nofollow">PDF</a>
				</section>
				
			</article>
		</main>

		<footer>
			<p>Enjoying the <a href="../../../tutorials">tutorials</a>? Are they useful? Want more?</p>
			<p><b><a href="https://www.patreon.com/catlikecoding">Please support me on Patreon!</a></b></p>
			<p><a href="https://www.patreon.com/catlikecoding"><img src="../../become-a-patron.png" alt="Become my patron!" width="217" height="51"></a></p>
			<p><b><a href="../../donating.html">Or make a direct donation</a>!</b></p>
			<p>made by <a href="../../../../about/index.html" rel="author">Jasper Flick</a></p>
		</footer>
		
		<script src="../../tutorials.js"></script>
	</body>
</html>