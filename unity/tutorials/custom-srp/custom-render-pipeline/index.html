<!DOCTYPE html>
<html lang="en">
	<head prefix="og: http://ogp.me/ns#">
		<meta charset="utf-8">
		<meta property="og:url" content="https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/">
		<meta property="og:type" content="article">
		<meta property="og:image:width" content="1024">
		<meta property="og:image:height" content="512">
		<meta property="og:image" content="https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/tutorial-image.jpg">
		<meta property="og:title" content="Custom Render Pipeline">
		<meta property="og:description" content="A Unity tutorial about creating a custom scriptable render pipeline.">
		<meta property="twitter:card" content="summary_large_image">
		<meta property="twitter:creator" content="@catlikecoding">
		<meta name="viewport" content="width=768">
		<title>Custom Render Pipeline</title>
		<link href="../../tutorials.css" rel="stylesheet">
		<link rel="manifest" href="../../../../site.webmanifest">
		<link rel="mask-icon" href="../../../../safari-pinned-tab.svg" color="#aa0000">

		<script type="application/ld+json">{
			"@context": "http://schema.org",
			"@type": "WebPage",
			"mainEntity": {
				"@type": "TechArticle",
				"@id": "https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/#article",
				"headline": "Custom Render Pipeline",
				"alternativeHeadline": "Taking Control of Rendering",
				"datePublished": "2019-09-26",
				"author": { "@type": "Person", "name": "Jasper Flick", "@id": "https://catlikecoding.com/jasper-flick/#person" },
				"publisher": { "@type": "Organization", "name": "Catlike Coding", "@id": "https://catlikecoding.com/#organization" },
				"description": "A Unity tutorial about creating a custom scriptable render pipeline.",
				"image": "https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/tutorial-image.jpg",
				"dependencies": "Unity 2019.2.6f1",
				"proficiencyLevel": "Expert"
			},
			"breadcrumb": {
				"@type": "BreadcrumbList",
				"itemListElement": [
					{ "@type": "ListItem", "position": 1, "item": { "@id": "https://catlikecoding.com/unity/", "name": "Unity" }},
					{ "@type": "ListItem", "position": 2, "item": { "@id": "https://catlikecoding.com/unity/tutorials/", "name": "Tutorials" }},
					{ "@type": "ListItem", "position": 3, "item": { "@id": "https://catlikecoding.com/unity/tutorials/custom-srp/", "name": "Custom SRP" }}
				]
			}
		}</script>
		<script>
			var customTypes = {
				CameraRenderer: 1,
				CustomRenderPipeline: 1,
				CustomRenderPipelineAsset: 1
			};
		</script>
	</head>
	<body>
		<header>
			<a href="../../../../index.html"><img src="../../../../catlike-coding-logo.svg" alt="Catlike Coding" width="45" height="45"></a>
			<nav>
				<ol>
					<li><a href="../../../../index.html">Catlike Coding</a></li>
					<li><a href="../../../index.html">Unity</a></li>
					<li><a href="../../../tutorials">Tutorials</a></li>
					<li><a href="../index.html">Custom SRP</a></li>
				</ol>
			</nav>
		</header>
		
		<main>
			<article>
				<header>
					<h1>Custom Render Pipeline</h1>
					<p>Taking Control of Rendering</p>
					<ul>
						<li>Create a render pipeline asset and instance.</li>
						<li>Render a camera's view.</li>
						<li>Perform culling, filtering, and sorting.</li>
						<li>Separate opaque, transparent, and invalid passes.</li>
						<li>Work with more than one camera.</li>
					</ul>
				</header>
				
				<p>This is the first part of a tutorial series about creating a <a href="../index.html">custom scriptable render pipeline</a>. It covers the initial creation of a bare-bones render pipeline that we will expand in the future.</p>
				
				<p>This series assumes that you've worked through at least the <a href="../../object-management/index.html">Object Management</a> series and the <a href="../../procedural-grid/index.html">Procedural Grid</a> tutorial.</p>
								
				
				<p>This tutorial is made with Unity 2019.2.6f1.</p>
				
				<aside>
					<h3>What about the other SRP series?</h3>
					<div>
						<p>I have <a href="../../scriptable-render-pipeline/index.html">another tutorial series</a> covering the scriptable render pipeline, but that one uses the experimental SRP API which only works with Unity 2018. This series is for Unity 2019 and later. This series takes a different and more modern approach but will cover at lot of the same topics. It's still useful to work through the 2018 series if you don't want to wait until this one has caught up with it.</p>
					</div>
				</aside>
				
				<figure>
					<img src="tutorial-image.jpg" width="512" height="256">
					<figcaption>Rendering with a custom render pipeline.</figcaption>
				</figure>
								
				<section>
					<h2>A new Render Pipeline</h2>
					
					<p>To render anything, Unity has to determine what shapes have to be drawn, where, when, and with what settings. This can get very complex, depending on how many effects are involved. Lights, shadows, transparency, image effects, volumetric effects, and so on all have to be dealt with in the correct order to arrive at the final image. This is what a render pipeline does.</p>
					
					<p>In the past Unity only supported a few built-in ways to render things. Unity 2018 introduced scriptable render pipelines&mdash;RPs for short&mdash;making it possible to do whatever we want, while still being able to rely on Unity for fundamental steps like culling. Unity 2018 also added two experimental RPs made with this new approach: the Lightweight RP and the High Definition RP. In Unity 2019 the Lightweight RP is no longer experimental and got rebranded to the Universal RP in Unity 2019.3.</p>
					
					<p>The Universal RP is destined to replace the current legacy RP as the default. The idea is that it is a one-size-fits-most RP that will also be fairly easy to customize. Rather than customizing that RP this series will create an entire RP from scratch.</p>
					
					<p>This tutorial lays the foundation with a minimal RP that draws unlit shapes using forward rendering. Once that's working, we can extend our pipeline in later tutorials, adding lighting, shadows, different rendering methods, and more advanced features.</p>
					
					<section>
						<h3>Project Setup</h3>
						
						<p>Create a new 3D project in Unity 2019.2.6 or later. We'll create our own pipeline, so don't select one of the RP project templates. Once the project is open you can go to the package manager and remove all packages that you don't need. We'll only use the <em translate="no">Unity UI</em> package in this tutorial to experiment with drawing the UI, so you can keep that one.</p>
						
						<p>We're going to exclusively work in linear color space, but Unity 2019.2 still uses gamma space as the default. Go to the player settings via <em translate="no">Edit / Project Settings</em> and then <em translate="no">Player</em>, then switch <em translate="no">Color Space</em> under the <em translate="no">Other Settings</em> section to <em translate="no">Linear</em>.</p>
						
						<figure>
							<img src="a-new-render-pipeline/color-space.png" width="320" height="62">
							<figcaption>Color space set to linear.</figcaption>
						</figure>
						
						<p>Fill the default scene with a few objects, using a mix of standard, unlit opaque and transparent materials. The <em translate="no">Unlit/Transparent</em> shader only works with a texture, so <a href="a-new-render-pipeline/sphere-alpha-map.png">here</a> is a UV sphere map for that.</p>
						
						<figure>
							<img src="a-new-render-pipeline/sphere-alpha-map.png" width="256" height="128" style="background-color: black">
							<figcaption>UV sphere alpha map, on black background.</figcaption>
						</figure>
						
						<p>I put a few cubes in my test scene, all of which are opaque. The red ones use a material with the <em translate="no">Standard</em> shader while the green and yellow ones use a material with the <em translate="no">Unlit/Color</em> shader. The blue spheres use the <em translate="no">Standard</em> shader with <em translate="no">Rendering Mode</em> set to <em translate="no">Transparent</em>, while the white spheres use the <em translate="no">Unlit/Transparent</em> shader.</p>
						
						<figure>
							<img src="a-new-render-pipeline/scene.png" width="430" height="170">
							<figcaption>Test scene.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Pipeline Asset</h3>
						
						<p>Currently, Unity uses the default render pipeline. To replace it with a custom render pipeline we first have to create an asset type for it. We'll use roughly the same folder structure that Unity uses for the Universal RP. Create a <em translate="no">Custom RP</em> asset folder with a <em translate="no">Runtime</em> child folder. Put a new C# script in there for the <code>CustomRenderPipelineAsset</code> type.</p>
						
						<figure>
							<img src="a-new-render-pipeline/folder-structure.png" width="228" height="68">
							<figcaption>Folder structure.</figcaption>
						</figure>
						
						<p>The asset type must extend <code>RenderPipelineAsset</code> from the <code>UnityEngine.Rendering</code> namespace.</p>
						
						<pre translate="no"><ins>using UnityEngine;</ins>
<ins>using UnityEngine.Rendering;</ins>

<ins>public class CustomRenderPipelineAsset : RenderPipelineAsset {}</ins></pre>
						
						<p>The main purpose of the RP asset is to give Unity a way to get a hold of a pipeline object instance that is responsible for rendering. The asset itself is just a handle and a place to store settings. We don't have any settings yet, so all we have to do is give Unity a way to get our pipeline object instance. That's done by overriding the abstract <code>CreatePipeline</code> method, which should return a <code>RenderPipeline</code> instance. But we haven't defined a custom RP type yet, so begin by returning <code>null</code>.</p>
						
						<p>The <code>CreatePipeline</code> method is defined with the  <code>protected</code> access modifier, which means that only the class that defined the method&mdash;which is <code>RenderPipelineAsset</code>&mdash;and those that extend it can access it.</p>
						
						<pre translate="no">	<ins>protected override RenderPipeline CreatePipeline () {</ins>
		<ins>return null;</ins>
	<ins>}</ins></pre>
						
						<p>Now we need to add an asset of this type to our project. To make that possible, add a <code>CreateAssetMenu</code> attribute to <code>CustomRenderPipelineAsset</code>.</p>
						
						<pre translate="no"><ins>[CreateAssetMenu]</ins>
public class CustomRenderPipelineAsset : RenderPipelineAsset { &hellip; }</pre>
						
						<p>That puts an entry in the <em translate="no">Asset / Create</em> menu. Let's be tidy and put it in a <em translate="no">Rendering</em> submenu. We do that by setting the <code>menuName</code> property of the attribute to <em translate="no">Rendering/Custom Render Pipeline</em>. This property can be set directly after the attribute type, within round brackets.</p>
						
						<pre translate="no">[CreateAssetMenu<ins>(menuName = "Rendering/Custom Render Pipeline")</ins>]
public class CustomRenderPipelineAsset : RenderPipelineAsset { &hellip; }</pre>
						
						<p>Use the new menu item to add the asset to the project, then go to the <em translate="no">Graphics</em> project settings and select it under <em translate="no">Scriptable Render Pipeline Settings</em>.</p>
						
						<figure>
							<img src="a-new-render-pipeline/srp-settings.png" width="320" height="90">
							<figcaption>Custom RP selected.</figcaption>
						</figure>
						
						<p>Replacing the default RP changed a few things. First, a lot of options have disappeared from the graphics settings, which is mentioned in an info panel. Second, we've disabled the default RP without providing a valid replacement, so nothing gets rendered anymore. The game window, scene window, and material previews are no longer functional. If you open the frame debugger&mdash;via <em translate="no">Window / Analysis / Frame Debugger</em>&mdash;and enable it, you will see that indeed nothing gets drawn in the game window.</p>
					</section>
					
					<section>
						<h3>Render Pipeline Instance</h3>
						
						<p>Create a <code>CustomRenderPipeline</code> class and put its script file in the same folder as <code>CustomRenderPipelineAsset</code>. This will be the type used for the RP instance that our asset returns, thus it must extend <code>RenderPipeline</code>.</p>
						
						<pre translate="no"><ins>using UnityEngine;</ins>
<ins>using UnityEngine.Rendering;</ins>

<ins>public class CustomRenderPipeline : RenderPipeline {}</ins></pre>
						
						<p><code>RenderPipeline</code> defines a protected abstract <code>Render</code> method that we have to override to create a concrete pipeline. It has two parameters: a <code>ScriptableRenderContext</code> and a <code>Camera</code> array. Leave the method empty for now.</p>
						
						<pre translate="no">	<ins>protected override void Render (</ins>
		<ins>ScriptableRenderContext context, Camera[] cameras</ins>
	<ins>) {}</ins></pre>
						
						<p>Make <code>CustomRenderPipelineAsset.CreatePipeline</code> return a new instance of <code>CustomRenderPipeline</code>. That will get us a valid and functional pipeline, although it doesn't render anything yet.</p>
						
						<pre translate="no">	protected override RenderPipeline CreatePipeline () {
		return <ins>new CustomRenderPipeline()</ins>;
	}</pre>
					</section>
				</section>
				
				<section>
					<h2>Rendering</h2>
					
					<p>Each frame Unity invokes <code>Render</code> on the RP instance. It passes along a context struct that provides a connection to the native engine, which we can use for rendering. It also passes an array of cameras, as there can be multiple active cameras in the scene. It is the RP's responsibility to render all those cameras in the order that they are provided.</p>
					
					<section>
						<h3>Camera Renderer</h3>
						
						<p>Each camera gets rendered independently. So rather than have <code>CustomRenderPipeline</code> render all camera's we'll forward that responsibility to a new class dedicated to rendering one camera. Name it <code>CameraRenderer</code> and give it a public <code>Render</code> method with a context and a camera parameter. Let's store these parameters in fields for convenience.</p>
						
						<pre translate="no"><ins>using UnityEngine;</ins>
<ins>using UnityEngine.Rendering;</ins>

<ins>public class CameraRenderer {</ins>

	<ins>ScriptableRenderContext context;</ins>

	<ins>Camera camera;</ins>

	<ins>public void Render (ScriptableRenderContext context, Camera camera) {</ins>
		<ins>this.context = context;</ins>
		<ins>this.camera = camera;</ins>
	<ins>}</ins>
<ins>}</ins></pre>
						
						<p>Have <code>CustomRenderPipeline</code> create an instance of the renderer when it gets created, then use it to render all cameras in a loop.</p>
						
						<pre translate="no">	<ins>CameraRenderer renderer = new CameraRenderer();</ins>

	protected override void Render (
		ScriptableRenderContext context, Camera[] cameras
	) {
		<ins>foreach (Camera camera in cameras) {</ins>
			<ins>renderer.Render(context, camera);</ins>
		<ins>}</ins>
	}</pre>
						
						<p>Our camera renderer is roughly equivalent to the scriptable renderers of the Universal RP. This approach will make it simple to support different rendering approaches per camera in the future, for example one for the first-person view and one for a 3D map overlay, or forward vs. deferred rendering. But for now we'll render all cameras the same way.</p>
					</section>
					
					<section>
						<h3>Drawing the Skybox</h3>
						
						<p>The job of <code>CameraRenderer.Render</code> is to draw all geometry that its camera can see. Isolate that specific task in a separate <code>DrawVisibleGeometry</code> method for clarity. We'll begin by having it draw the default the skybox, which can be done by invoking <code>DrawSkybox</code> on the context with the camera as an argument.</p>
						
						<pre translate="no">	public void Render (ScriptableRenderContext context, Camera camera) {
		this.context = context;
		this.camera = camera;

		<ins>DrawVisibleGeometry();</ins>
	}

	<ins>void DrawVisibleGeometry () {</ins>
		<ins>context.DrawSkybox(camera);</ins>
	<ins>}</ins></pre>
						
						<p>This does not yet make the skybox appear. That's because the commands that we issue to the context are buffered. We have to submit the queued work for execution, by invoking <code>Submit</code> on the context. Let's do this in a separate <code>Submit</code> method, invoked after <code>DrawVisibleGeometry</code>.</p>
						
						<pre translate="no">	public void Render (ScriptableRenderContext context, Camera camera) {
		this.context = context;
		this.camera = camera;

		DrawVisibleGeometry();
		<ins>Submit();</ins>
	}

	<ins>void Submit () {</ins>
		<ins>context.Submit();</ins>
	<ins>}</ins></pre>
						
						<p>The skybox finally appears in both the game and scene window. You can also see an entry for it in the frame debugger when you enable it. It's listed as <em translate="no">Camera.RenderSkybox</em>, which has a single <em translate="no">Draw Mesh</em> item under it, which represents the actual draw call. This corresponds to the rendering of the game window. The frame debugger doesn't report drawing in other windows.</p>
						
						<figure>
							<img src="rendering/skybox.png" width="430" height="170" alt="scene"><br>
							<img src="rendering/skybox-debugger.png" width="198" height="34" alt="debugger">
							<figcaption>Skybox gets drawn.</figcaption>
						</figure>
						
						<p>Note that the orientation of the camera currently doesn't affect how the skybox gets rendered. We pass the camera to <code>DrawSkybox</code>, but that's only used to determine whether the skybox should be drawn at all, which is controlled via the camera's clear flags.</p>
						
						<p>To correctly render the skybox&mdash;and the entire scene&mdash;we have to set up the view-projection matrix. This transformation matrix combines the camera's position and orientation&mdash;the view matrix&mdash;with the camera's perspective or orthographic projection&mdash;the projection matrix. It is known in shaders as <em translate="no">unity_MatrixVP</em>, one of the shader properties used when geometry gets drawn. You can inspect this matrix in the frame debugger's <em translate="no">ShaderProperties</em> section when a draw call is selected.</p>
						
						<p>At the moment, the <em translate="no">unity_MatrixVP</em> matrix is always the same. We have to apply the camera's properties to the context, via the <code>SetupCameraProperties</code> method. That sets up the matrix as well as some other properties. Do this before invoking <code>DrawVisibleGeometry</code>, in a separate <code>Setup</code> method.</p>
						
						<pre translate="no">	public void Render (ScriptableRenderContext context, Camera camera) {
		this.context = context;
		this.camera = camera;

		<ins>Setup();</ins>
		DrawVisibleGeometry();
		Submit();
	}

	<ins>void Setup () {</ins>
		<ins>context.SetupCameraProperties(camera);</ins>
	<ins>}</ins></pre>
						
						<figure>
							<img src="rendering/skybox-aligned.png" width="430" height="170">
							<figcaption>Skybox, correctly aligned.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Command Buffers</h3>
						
						<p>The context delays the actual rendering until we submit it. Before that, we configure it and add commands to it for later execution. Some tasks&mdash;like drawing the skybox&mdash;can be issued via a dedicated method, but other commands have to be issued indirectly, via a separate command buffer. We need such a buffer to draw the other geometry in the scene.</p>
						
						<p>To get a buffer we have to create a new <code>CommandBuffer</code> object instance. We need only one buffer, so create one by default for <code>CameraRenderer</code> and store a reference to it in a field. Also give the buffer a name so we can recognize it in the frame debugger. <em translate="no">Render Camera</em> will do.</p>
						
						<pre translate="no">	<ins>const string bufferName = "Render Camera";</ins>

	<ins>CommandBuffer buffer = new CommandBuffer {</ins>
		<ins>name = bufferName</ins>
	<ins>};</ins></pre>
						
						<aside>
							<h3>How does that object initializer syntax work?</h3>
							<div>
								<p>It's as if we've written <code>buffer.name = bufferName;</code> as a separate statement after invoking the constructor. But when creating a new object, you can append a code block to the constructor's invocation. Then you can set the object's fields and properties in the block without having to reference the object instance explicitly. It makes explicit that the instances should only be used after those fields and properties have been set. Besides that, it makes initialization possible where only a single statement is allowed&mdash;for example a field initialization, which we're using here&mdash;without requiring constructors with many parameter variants.</p>
								
								<p>Note that we omitted the empty parameter list of the constructor invocation, which is allowed when object initializer syntax is used.</p>
							</div>
						</aside>
						
						<p>We can use command buffers to inject profiler samples, which will show up both in the profiler and the frame debugger. This is done by invoking <code>BeginSample</code> and <code>EndSample</code> at the appropriate points, which is at the beginning of <code>Setup</code> and <code>Submit</code> in our case. Both methods must be provided with the same sample name, for which we'll use the buffer's name.</p>
						
						<pre translate="no">	void Setup () {
		<ins>buffer.BeginSample(bufferName);</ins>
		context.SetupCameraProperties(camera);
	}

	void Submit () {
		<ins>buffer.EndSample(bufferName);</ins>
		context.Submit();
	}</pre>
						
						<p>To execute the buffer, invoke <code>ExecuteCommandBuffer</code> on the context with the buffer as an argument. That copies the commands from the buffer but doesn't clear it, we have to do that explicitly afterwards if we want to reuse it. Because execution and clearing is always done together it's handy to add a method that does both.</p>
						
						<pre translate="no">	void Setup () {
		buffer.BeginSample(bufferName);
		<ins>ExecuteBuffer();</ins>
		context.SetupCameraProperties(camera);
	}

	void Submit () {
		buffer.EndSample(bufferName);
		<ins>ExecuteBuffer();</ins>
		context.Submit();
	}

	<ins>void ExecuteBuffer () {</ins>
		<ins>context.ExecuteCommandBuffer(buffer);</ins>
		<ins>buffer.Clear();</ins>
	<ins>}</ins></pre>
						
						<p>The <em translate="no">Camera.RenderSkyBox</em> sample now gets nested inside <em translate="no">Render Camera</em>.</p>
						
						<figure>
							<img src="rendering/render-camera-sample.png" width="198" height="50">
							<figcaption>Render camera sample.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Clearing the Render Target</h3>
						
						<p>Whatever we draw ends up getting rendered to the camera's render target, which is the frame buffer by default but could also be a render texture. Whatever was drawn to that target earlier is still there, which could interfere with the image that we are rendering now. To guarantee proper rendering we have to clear the render target to get rid of its old contents. That's done by invoking <code>ClearRenderTarget</code> on the command buffer, which belongs in the <code>Setup</code> method.</p>
						
						<p><code>CommandBuffer.ClearRenderTarget</code> requires at least three arguments. The first two indicate whether the depth and color data should be cleared, which is true for both. The third argument is the color used to clearing, for which we'll use <code>Color.clear</code>.</p>
						
						<pre translate="no">	void Setup () {
		buffer.BeginSample(bufferName);
		<ins>buffer.ClearRenderTarget(true, true, Color.clear);</ins>
		ExecuteBuffer();
		context.SetupCameraProperties(camera);
	}</pre>
						
						<figure>
							<img src="rendering/clearing-nested-sample.png" width="198" height="66">
							<figcaption>Clearing, with nested sample.</figcaption>
						</figure>
						
						<p>The frame debugger now shows a <em translate="no">Draw GL</em> entry for the clear action, which shows up nested in an additional level of <em translate="no">Render Camera</em>. That happens because <code>ClearRenderTarget</code> wraps the clearing in a sample with the command buffer's name. We can get rid of the redundant nesting by clearing before beginning our own sample. That results in two adjacent <em translate="no">Render Camera</em> sample scopes, which get merged.</p>
						
						<pre translate="no">	void Setup () {
		<ins>buffer.ClearRenderTarget(true, true, Color.clear);</ins>
		buffer.BeginSample(bufferName);
		<del>//buffer.ClearRenderTarget(true, true, Color.clear);</del>
		ExecuteBuffer();
		context.SetupCameraProperties(camera);
	}</pre>
						
						<figure>
							<img src="rendering/clearing-one-sample.png" width="198" height="50">
							<figcaption>Clearing, without nesting.</figcaption>
						</figure>
						
						<p>The <em translate="no">Draw GL</em> entry represent drawing a full-screen quad with the <em translate="no">Hidden/InternalClear</em> shader that writes to the render target, which isn't the most efficient way to clear it. This approach is used because we're clearing before setting up the camera properties. If we swap the order of those two steps we get the quick way to clear.</p>
						
						<pre translate="no">	void Setup () {
		<ins>context.SetupCameraProperties(camera);</ins>
		buffer.ClearRenderTarget(true, true, Color.clear);
		buffer.BeginSample(bufferName);
		ExecuteBuffer();
		<del>//context.SetupCameraProperties(camera);</del>
	}</pre>
						
						<figure>
							<img src="rendering/clearing-correct.png" width="198" height="50">
							<figcaption>Correct clearing.</figcaption>
						</figure>
						
						<p>Now we see <em translate="no">Clear (color+Z+stencil)</em>, which indicates that both the color and depth buffers get cleared. Z represents the depth buffer and the stencil data is part the same buffer.</p>
					</section>
					
					<section>
						<h3>Culling</h3>
						
						<p>We're currently seeing the skybox, but not any of the objects that we put in the scene. Rather than drawing every object, we're only going to render those that are visible to the camera. We do that by starting with all objects with renderer components in the scene and then culling those that fall outside of the view frustum of the camera.</p>
						
						<p>Figuring out what can be culled requires us to keep track of multiple camera settings and matrices, for which we can use the <code>ScriptableCullingParameters</code> struct. Instead of filling it ourselves, we can invoke <code>TryGetCullingParameters</code> on the camera. It returns whether the parameters could be successfully retrieved, as it might fail for degenerate camera settings. To get hold of the parameter data we have to supply it as an output argument, by writing <code>out</code> in front of it. Do this in a separate <code>Cull</code> method that returns either success or failure.</p>

						
						<pre translate="no">	<ins>bool Cull () {</ins>
		<ins>ScriptableCullingParameters p</ins>
		<ins>if (camera.TryGetCullingParameters(out p)) {</ins>
			<ins>return true;</ins>
		<ins>}</ins>
		<ins>return false;</ins>
	<ins>}</ins></pre>
						
						<aside>
							<h3>Why do we have to write <code>out</code>?</h3>
							<div>
								<p>When a struct parameter is defined as an output parameter it acts like an object reference, pointing to the place on the memory stack where the argument resides. When the method changes the parameter it affects that value, not a copy.</p>
								
								<p>The <code>out</code> keyword tells us that the method is responsible for correctly setting the parameter, replacing the previous value.</p>
								
								<p>Try-get methods are a common way to both indicate success or failure and produce a result.</p>
							</div>
						</aside>
						
						<p>It is possible to inline the variable declaration inside the argument list when used as an output argument, so let's do that.</p>
						
						<pre translate="no">	bool Cull () {
		<del>//ScriptableCullingParameters p</del>
		if (camera.TryGetCullingParameters(out <ins>ScriptableCullingParameters</ins> p)) {
			return true;
		}
		return false;
	}</pre>
						
						<p>Invoke <code>Cull</code> before <code>Setup</code> in <code>Render</code> and abort if it failed.</p>
						
						<pre translate="no">	public void Render (ScriptableRenderContext context, Camera camera) {
		this.context = context;
		this.camera = camera;

		<ins>if (!Cull()) {</ins>
			<ins>return;</ins>
		<ins>}</ins>

		Setup();
		DrawVisibleGeometry();
		Submit();
	}</pre>
						
						<p>Actual culling is done by invoking <code>Cull</code> on the context, which produces a <code>CullingResults</code> struct. Do this in <code>Cull</code> if successful and store the results in a field. In this case we have to pass the culling parameters as a reference argument, by writing <code>ref</code> in front of it.</p>
						
						<pre translate="no">	<ins>CullingResults cullingResults;</ins>

	&hellip;
	
	bool Cull () {
		if (camera.TryGetCullingParameters(out ScriptableCullingParameters p)) {
			<ins>cullingResults = context.Cull(ref p);</ins>
			return true;
		}
		return false;
	}</pre>
						
						<aside>
							<h3>Why do we have to use <code>ref</code>?</h3>
							<div>
								<p>The <code>ref</code> keyword works just like <code>out</code>, except that the method is not required to assign something to it. Whoever invokes the method is responsible for properly initializing the value first. So it can be used for input and optionally for output.</p>
								
								<p>In this case <code>ref</code> is used as an optimization, to prevent passing a copy of the <code>ScriptableCullingParameters</code> struct, which is quite large. It being a struct instead of an object is another optimization, to prevent memory allocations.</p>
							</div>
						</aside>
					</section>
					
					<section>
						<h3>Drawing Geometry</h3>
						
						<p>Once we know what is visible we can move on to rendering those things. That is done by invoking <code>DrawRenderers</code> on the context with the culling results as an argument, telling it which renderers to use. Besides that, we have to supply drawing settings and filtering settings. Both are structs&mdash;<code>DrawingSettings</code> and <code>FilteringSettings</code>&mdash;for which we'll initially use their default constructors. Both have to be passed by reference. Do this in <code>DrawVisibleGeometry</code>, before drawing the skybox.</p>
						
						<pre translate="no">	void DrawVisibleGeometry () {
		<ins>var drawingSettings = new DrawingSettings();</ins>
		<ins>var filteringSettings = new FilteringSettings();</ins>

		<ins>context.DrawRenderers(</ins>
			<ins>cullingResults, ref drawingSettings, ref filteringSettings</ins>
		<ins>);</ins>

		context.DrawSkybox(camera);
	}</pre>
						
						<p>We don't see anything yet because we also have to indicate which kind of shader passes are allowed. As we only support unlit shaders in this tutorial we have to fetch the shader tag ID for the <em translate="no">SRPDefaultUnlit</em> pass, which we can do once and cache it in a static field.</p>
						
						<pre translate="no">	<ins>static ShaderTagId unlitShaderTagId = new ShaderTagId("SRPDefaultUnlit");</ins></pre>
						
						<p>Provide it as the first argument of the <code>DrawingSettings</code> constructor, along with a new <code>SortingSettings</code> struct value. Pass the camera to the constructor of the sorting settings, as it's used to determine whether orthographic or distance-based sorting applies.</p>
						
						<pre translate="no">	void DrawVisibleGeometry () {
		<ins>var sortingSettings = new SortingSettings(camera);</ins>
		var drawingSettings = new DrawingSettings(
			<ins>unlitShaderTagId, sortingSettings</ins>
		);
		&hellip;
	}</pre>
						
						<p>Besides that we also have to indicate which render queues are allowed. Pass <code>RenderQueueRange.all</code> to the <code>FilteringSettings</code> constructor so we include everything.</p>
						
						<pre translate="no">		var filteringSettings = new FilteringSettings(<ins>RenderQueueRange.all</ins>);</pre>
						
						<figure>
							<img src="rendering/drawing-unlit.png" width="430" height="170" alt="scene"><br>
							<img src="rendering/drawing-unlit-debugger.png" width="198" height="290" alt="debugger">
							<figcaption>Drawing unlit geometry.</figcaption>
						</figure>
						
						<p>Only the visible objects that use the unlit shader get drawn. All the draw calls are listed in the frame debugger, grouped under <em translate="no">RenderLoop.Draw</em>. There's something weird going on with transparent objects, but let's first look at the order in which the objects are drawn. That's shown by the frame debugger and you can step through the draw calls by selecting one after the other or using the arrow keys.</p>
						
						<figure>
							<div class="vid" style="width: 380px; height:140px;"><iframe src='https://gfycat.com/ifr/bothanyindianskimmer'></iframe></div>
							<figcaption>Stepping through the frame debugger.</figcaption>
						</figure>
						
						<p>The drawing order is haphazard. We can force a specific draw order by setting the <code>criteria</code> property of the sorting settings. Let's use <code>SortingCriteria.CommonOpaque</code>.</p>
						
						<pre translate="no">		var sortingSettings = new SortingSettings(camera) <ins>{</ins>
			<ins>criteria = SortingCriteria.CommonOpaque</ins>
		<ins>}</ins>;</pre>
						
						<figure>
							<div class="vid" style="width: 380px; height:140px;"><iframe src='https://gfycat.com/ifr/rawimpishalpinegoat'></iframe></div><br>
							<img src="rendering/sorting.png" width="198" height="290" alt="debugger">
							<figcaption>Common opaque sorting.</figcaption>
						</figure>
						
						<p>Objects now get more-or-less drawn front-to-back, which is ideal for opaque objects. If something ends up drawn behind something else its hidden fragments can be skipped, which speeds up rendering. The common opaque sorting option also takes some other criteria into consideration, including the render queue and materials.</p>
						
					</section>
					
					<section>
						<h3>Drawing Opaque and Transparent Geometry Separately</h3>
						
						<p>The frame debugger shows us that transparent objects get drawn, but the skybox gets drawn over everything that doesn't end up in front of an opaque object. The skybox gets drawn after the opaque geometry so all its hidden fragments can get skipped, but it overwrites transparent geometry. That happens because transparent shaders do not write to the depth buffer. They don't hide whatever's behind them, because we can see through them. The solution is to first drawn opaque objects, then the skybox, and only then transparent objects.</p>
						
						<p>We can eliminate the transparent objects from the initial <code>DrawRenderers</code> invocation by switching to <code>RenderQueueRange.opaque</code>.</p>
						
						<pre translate="no">		var filteringSettings = new FilteringSettings(RenderQueueRange.<ins>opaque</ins>);</pre>
						
						<p>Then after drawing the skybox invoke <code>DrawRenderers</code> again. But before doing so change the render queue range to <code>RenderQueueRange.transparent</code>. Also change the sorting criteria to <code>SortingCriteria.CommonTransparent</code> and again set the sorting of the drawing settings. That reverses the draw order of the transparent objects.</p>
						
						<pre translate="no">		context.DrawSkybox(camera);

		<ins>sortingSettings.criteria = SortingCriteria.CommonTransparent;</ins>
		<ins>drawingSettings.sortingSettings = sortingSettings;</ins>
		<ins>filteringSettings.renderQueueRange = RenderQueueRange.transparent;</ins>

		<ins>context.DrawRenderers(</ins>
			<ins>cullingResults, ref drawingSettings, ref filteringSettings</ins>
		<ins>);</ins></pre>
						
						<figure>
							<div class="vid" style="width: 380px; height:140px;"><iframe src='https://gfycat.com/ifr/disloyalremarkableavocet'></iframe></div><br>
							<img src="rendering/opaque-skybox-transparent.png" width="198" height="304" alt="debugger">
							<figcaption>Opaque, then skybox, then transparent.</figcaption>
						</figure>
						
						<aside>
							<h3>Why is the draw order reversed?</h3>
							<div>
								<p>As transparent objects do not write to the depth buffer sorting them front-to-back has no performance benefit. But when transparent objects end up visually behind each other they have to be drawn back-to-front to correctly blend.</p>
								
								<p>Unfortunately back-to-front sorting does not guarantee correct blending, because sorting is per-object and only based on the object's position. Intersecting and large transparent objects can still produce incorrect results. This can sometimes be solved by cutting the geometry in smaller parts.</p>
							</div>
						</aside>
					</section>
				</section>
				
				<section>
					<h2>Editor Rendering</h2>
					
					<p>Our RP correctly draws unlit objects, but there are a few things that we can do to improve the experience of working with it in the Unity editor.</p>
					
					<section>
						<h3>Drawing Legacy Shaders</h3>
						
						<p>Because our pipeline only supports unlit shaders passes, objects that use different passes are not rendered, making them invisible. While this is correct, it hides the fact that some objects in the scene use the wrong shader. So let's render them anyway, but separately.</p>
						
						<p>If someone were to start with a default Unity project and later switch to our RP then they might have objects with the wrong shader in their scenes. To cover all Unity's default shaders we have to use shaders tag IDs for the <em translate="no">Always</em>, <em translate="no">ForwardBase</em>, <em translate="no">PrepassBase</em>, <em translate="no">Vertex</em>, <em translate="no">VertexLMRGBM</em>, and <em translate="no">VertexLM</em> passes. Keep track of these in a static array.</p>
						
						<pre translate="no">	<ins>static ShaderTagId[] legacyShaderTagIds = {</ins>
		<ins>new ShaderTagId("Always"),</ins>
		<ins>new ShaderTagId("ForwardBase"),</ins>
		<ins>new ShaderTagId("PrepassBase"),</ins>
		<ins>new ShaderTagId("Vertex"),</ins>
		<ins>new ShaderTagId("VertexLMRGBM"),</ins>
		<ins>new ShaderTagId("VertexLM")</ins>
	<ins>};</ins></pre>
						
						<p>Draw all unsupported shaders in a separate method after the visible geometry, starting with just the first pass. As these are invalid passes the results will be wrong anyway so we don't care about the other settings. We can get default filtering settings via the <code>FilteringSettings.defaultValue</code> property.</p>
						
						<pre translate="no">	public void Render (ScriptableRenderContext context, Camera camera) {
		&hellip;

		Setup();
		DrawVisibleGeometry();
		<ins>DrawUnsupportedShaders();</ins>
		Submit();
	}

	&hellip;

	<ins>void DrawUnsupportedShaders () {</ins>
		<ins>var drawingSettings = new DrawingSettings(</ins>
			<ins>legacyShaderTagIds[0], new SortingSettings(camera)</ins>
		<ins>);</ins>
		<ins>var filteringSettings = FilteringSettings.defaultValue;</ins>
		<ins>context.DrawRenderers(</ins>
			<ins>cullingResults, ref drawingSettings, ref filteringSettings</ins>
		<ins>);</ins>
	<ins>}</ins></pre>
						
						<p>We can draw multiple passes by invoking <code>SetShaderPassName</code> on the drawing settings with a draw order index and tag as arguments. Do this for all passes in the array, starting at the second as we already set the first pass when constructing the drawing settings.</p>
						
						<pre translate="no">		var drawingSettings = new DrawingSettings(
			legacyShaderTagIds[0], new SortingSettings(camera)
		);
		<ins>for (int i = 1; i &lt; legacyShaderTagIds.Length; i++) {</ins>
			<ins>drawingSettings.SetShaderPassName(i, legacyShaderTagIds[i]);</ins>
		<ins>}</ins></pre>
						
						<figure>
							<img src="editor-rendering/standard-black.png" width="430" height="170">
							<figcaption>Standard shader renders black.</figcaption>
						</figure>
						
						<p>Objects rendered with the standard shader show up, but they're now solid black because our RP hasn't set up the required shader properties for them.</p>
					</section>
					
					<section>
						<h3>Error Material</h3>
						
						<p>To clearly indicate which objects use unsupported shaders we'll draw them with Unity's error shader. Construct a new material with that shader as an argument, which we can find by invoking <code>Shader.Find</code> with the <em translate="no">Hidden/InternalErrorShader</em> string as an argument. Cache the material via a static field so we won't create a new one each frame. Then assign it to the <code>overrideMaterial</code> property of the drawing settings.</p>
						
						<pre translate="no">	<ins>static Material errorMaterial;</ins>

	&hellip;

	void DrawUnsupportedShaders () {
		<ins>if (errorMaterial == null) {</ins>
			<ins>errorMaterial =</ins>
				<ins>new Material(Shader.Find("Hidden/InternalErrorShader"));</ins>
		<ins>}</ins>
		var drawingSettings = new DrawingSettings(
			legacyShaderTagIds[0], new SortingSettings(camera)
		) <ins>{</ins>
			<ins>overrideMaterial = errorMaterial</ins>
		<ins>}</ins>;
		&hellip;
	}</pre>
						
						<figure>
							<img src="editor-rendering/standard-magenta.png" width="430" height="170">
							<figcaption>Rendered with magenta error shader.</figcaption>
						</figure>
						
						<p>Now all invalid objects are visible and obviously wrong.</p>
					</section>
					
					<section>
						<h3>Partial Class</h3>
						
						<p>Drawing invalid objects is useful for development but is not meant for released apps. So let's put all editor-only code for <code>CameraRenderer</code> in a separate partial class file. Begin by duplicating the original <em translate="no">CameraRenderer</em> script asset and renaming it to <em translate="no">CameraRenderer.Editor</em>.</p>
						
						<figure>
							<img src="editor-rendering/two-assets.png" width="228" height="98">
							<figcaption>One class, two script assets.</figcaption>
						</figure>
						
						<p>Then turn the original <code>CameraRenderer</code> into a partial class and remove the tag array, error material, and <code>DrawUnsupportedShaders</code> method from it.</p>
						
						<pre translate="no">public <ins>partial</ins> class CameraRenderer { &hellip; }</pre>
						
						<aside>
							<h3>What are partial classes?</h3>
							<div>
								<p>It's a way to split a class—or struct—definition into multiple parts, stored in different files. The only purpose is to organize code. The typical use case is to keep automatically-generated code separate from manually-written code. As far as the compiler is concerned, it's all part of the same class definition. They were introduced in the <a href="../../object-management/more-complex-levels/index.html">Object Management, More Complex Levels</a> tutorial.</p>
							</div>
						</aside>
						
						<p>Clean the other partial class file so it only contains what we removed from the other.</p>
						
						<pre translate="no">using UnityEngine;
using UnityEngine.Rendering;

<ins>partial</ins> class CameraRenderer {

	static ShaderTagId[] legacyShaderTagIds = {	&hellip; };

	static Material errorMaterial;

	void DrawUnsupportedShaders () { &hellip; }
}</pre>
						
						<p>The content of the editor part only needs to exist in the editor, so make it conditional on <em translate="no">UNITY_EDITOR</em>.</p>
						
						<pre translate="no">partial class CameraRenderer {

<ins>#if UNITY_EDITOR</ins>

	static ShaderTagId[] legacyShaderTagIds = { &hellip; }
	};

	static Material errorMaterial;

	void DrawUnsupportedShaders () { &hellip; }

<ins>#endif</ins>
}</pre>
						
						<p>However, making a build will fail at this point, because the other part always contains the invocation of <code>DrawUnsupportedShaders</code>, which now only exists while in the editor. To solve this we make that method partial as well. We do that by always declaring the method signature with <code>partial</code> in front of it, similar to an abstract method declaration. We can do that in any part of the class definition, so let's put it in the editor part. The full method declaration must be marked with <code>partial</code> as well.</p>
						
						<pre translate="no">	<ins>partial void DrawUnsupportedShaders ();</ins>

#if UNITY_EDITOR

	&hellip;

	<ins>partial</ins> void DrawUnsupportedShaders () { &hellip; }

#endif</pre>
						
						<p>Compilation for a build now succeeds. The compiler will strip out the invocation of all partial methods that didn't end up with a full declaration.</p>
						
						<aside>
							<h3>Can we make the invalid objects appear in development builds?</h3>
							<div>
								<p>Yes, you can base the conditional compilation on <code>UNITY_EDITOR || DEVELOPMENT_BUILD</code> instead. Then <code>DrawUnsupportedShaders</code> exists in development builds as well and still not in release builds. But I'll consistently limit everything development-related to the editor only in this series.</p>
							</div>
						</aside>
					</section>
					
					<section>
						<h3>Drawing Gizmos</h3>
						
						<p>Currently our RP doesn't draw gizmos, neither in the scene window nor in the game window if they are enabled there.</p>
						
						<figure>
							<img src="editor-rendering/without-gizmos.png" width="330" height="160">
							<figcaption>Scene without gizmos.</figcaption>
						</figure>
						
						<p>We can check whether gizmos should be drawn by invoking <code>UnityEditor.Handles.ShouldRenderGizmos</code>. If so, we have to invoke <code>DrawGizmos</code> on the context with the camera as an argument, plus a second argument to indicate which gizmo subset should be drawn. There are two subsets, for before and after image effects. As we don't support image effects at this point we'll invoke both. Do this in a new editor-only <code>DrawGizmos</code> method.</p>
						
						<pre translate="no"><ins>using UnityEditor;</ins>
using UnityEngine;
using UnityEngine.Rendering;

partial class CameraRenderer {
	
	<ins>partial void DrawGizmos ();</ins>

	partial void DrawUnsupportedShaders ();

#if UNITY_EDITOR

	&hellip;

	<ins>partial void DrawGizmos () {</ins>
		<ins>if (Handles.ShouldRenderGizmos()) {</ins>
			<ins>context.DrawGizmos(camera, GizmoSubset.PreImageEffects);</ins>
			<ins>context.DrawGizmos(camera, GizmoSubset.PostImageEffects);</ins>
		<ins>}</ins>
	<ins>}</ins>

	partial void DrawUnsupportedShaders () { &hellip; }

#endif
}</pre>
						
						<p>The gizmos should be drawn after everything else.</p>
						
						<pre translate="no">	public void Render (ScriptableRenderContext context, Camera camera) {
		&hellip;

		Setup();
		DrawVisibleGeometry();
		DrawUnsupportedShaders();
		<ins>DrawGizmos();</ins>
		Submit();
	}</pre>
						
						<figure>
							<img src="editor-rendering/with-gizmos.png" width="330" height="160">
							<figcaption>Scene with gizmos.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Drawing Unity UI</h3>
						
						<p>Another thing that requires our attention is Unity's in-game user interface. For example, create a simple UI by adding a button via <em translate="no">GameObject / UI / Button</em>. It will show up in the game window, but not the scene window.</p>
						
						<figure>
							<img src="editor-rendering/ui-button.png" width="380" height="130">
							<figcaption>UI button in game window.</figcaption>
						</figure>
						
						<aside>
							<h3>Why can't I create a UI button?</h3>
							<div>
								<p>You need to have the <em translate="no">Unity UI</em> package in your project.</p>
							</div>
						</aside>
						
						<p>The frame debugger shows us that the UI is rendered separately, not by our RP.</p>
						
						<figure>
							<img src="editor-rendering/ui-debugger.png" width="198" height="162">
							<figcaption>UI in frame debugger.</figcaption>
						</figure>
						
						<p>At least, that's the case when the <em translate="no">Render Mode</em> of the canvas component is set to <em translate="no">Screen Space - Overlay</em>, which is the default. Changing it to <em translate="no">Screen Space - Camera</em> and using the main camera as its <em translate="no">Render Camera</em> will make it part of the transparent geometry.</p>
						
						<figure>
							<img src="editor-rendering/ui-camera-debugger.png" width="198" height="156">
							<figcaption>Screen-space-camera UI in frame debugger.</figcaption>
						</figure>
						
						<p>The UI always uses the <em translate="no">World Space</em> mode when it gets rendered in the scene window, which is why it usually ends up very large. But while we can edit the UI via the scene window it doesn't get drawn.</p>
						
						<figure>
							<img src="editor-rendering/invisible-ui-scene.png" width="210" height="70">
							<figcaption>UI invisible in scene window.</figcaption>
						</figure>
						
						<p>We have to explicitly add the UI to the world geometry when rendering for the scene window, by invoking <code>ScriptableRenderContext.EmitWorldGeometryForSceneView</code> with the camera as an argument. Do this in a new editor-only <code>PrepareForSceneWindow</code> method. We're rendering with the scene camera when its <code>cameraType</code> property is equal to <code>CameraType.SceneView</code>.</p>
						
						<pre translate="no">	<ins>partial void PrepareForSceneWindow ();</ins>

#if UNITY_EDITOR

	&hellip;

	<ins>partial void PrepareForSceneWindow () {</ins>
		<ins>if (camera.cameraType == CameraType.SceneView) {</ins>
			<ins>ScriptableRenderContext.EmitWorldGeometryForSceneView(camera);</ins>
		<ins>}</ins>
	<ins>}</ins></pre>
						
						<p>As that might add geometry to the scene it has to be done before culling.</p>
						
						<pre translate="no">		<ins>PrepareForSceneWindow();</ins>
		if (!Cull()) {
			return;
		}</pre>
						
						<figure>
							<img src="editor-rendering/visible-ui-scene.png" width="210" height="70">
							<figcaption>UI visible in scene window.</figcaption>
						</figure>
					</section>
				</section>
				
				<section>
					<h2>Multiple Cameras</h2>
					
					<p>It is possible to have more that one active camera in the scene. If so, we have to make sure that they work together.</p>
					
					<section>
						<h3>Two Cameras</h3>
						
						<p>Each camera has a <em translate="no">Depth</em> value, which is &minus;1 for the default main camera. They get rendered in increasing order of depth. To see this, duplicate the <em translate="no">Main Camera</em>, rename it to <em translate="no">Secondary Camera</em>, and set its <em translate="no">Depth</em> to 0. It's also a good idea to give it another tag, as <em translate="no">MainCamera</em> is supposed to be used by only a single camera.</p>
						
						<figure>
							<img src="multiple-cameras/two-cameras-sample-sample.png" width="198" height="146">
							<figcaption>Both cameras grouped in a single sample scope.</figcaption>
						</figure>
						
						<p>The scene now gets rendered twice. The resulting image is still the same because the render target gets cleared in between. The frame debugger shows this, but because adjacent sample scopes with the same name get merged we end up with a single <em translate="no">Render Camera</em> scope.</p>
						
						<p>It's clearer if each camera gets its own scope. To make that possible, add an editor-only <code>PrepareBuffer</code> method that makes the buffer's name equal to the camera's.</p>
						
						<pre translate="no">	<ins>partial void PrepareBuffer ();</ins>

#if UNITY_EDITOR

	&hellip;
	
	<ins>partial void PrepareBuffer () {</ins>
		<ins>buffer.name = camera.name;</ins>
	<ins>}</ins>

#endif</pre>
						
						<p>Invoke it before we prepare the scene window.</p>
						
						<pre translate="no">		<ins>PrepareBuffer();</ins>
		PrepareForSceneWindow();</pre>
						
						<figure>
							<img src="multiple-cameras/separate-samples.png" width="198" height="34">
							<figcaption>Separate samples per camera.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Dealing with Changing Buffer Names</h3>
						
						<p>Although the frame debugger now shows a separate sample hierarchy per camera, when we enter play mode Unity's console will get filled with messages warning us that <em translate="no">BeginSample</em> and <em translate="no">EndSample</em> counts must match. It gets confused because we're using different names for the samples and their buffer. Besides that, we also end up allocating memory each time we access the camera's <code>name</code> property, so we don't want to do that in builds.</p>
						
						<p>To tackle both issues we'll add a <code>SampleName</code> string property. If we're in the editor we set it in <code>PrepareBuffer</code> along with the buffer's name, otherwise it's simply a constant alias for the <em translate="no">Render Camera</em> string.</p>
						
						<pre translate="no">#if UNITY_EDITOR

	&hellip;

	<ins>string SampleName { get; set; }</ins>
	
	&hellip;
	
	partial void PrepareBuffer () {
		buffer.name = <ins>SampleName =</ins> camera.name;
	}

<ins>#else</ins>

	<ins>const string SampleName = bufferName;</ins>

#endif</pre>
						
						<p>Use <code>SampleName</code> for the sample in <code>Setup</code> and <code>Submit</code>.</p>
						
						<pre translate="no">	void Setup () {
		context.SetupCameraProperties(camera);
		buffer.ClearRenderTarget(true, true, Color.clear);
		buffer.BeginSample(<ins>SampleName</ins>);
		ExecuteBuffer();
	}

	void Submit () {
		buffer.EndSample(<ins>SampleName</ins>);
		ExecuteBuffer();
		context.Submit();
	}</pre>
						
						<p>We can see the difference by checking the profiler&mdash;opened via <em translate="no">Window / Analysis / Profiler</em>&mdash;and playing in the editor first. Switch to <em translate="no">Hierarchy</em> mode and sort by the <em translate="no">GC Alloc</em> column. You'll see an entry for two invocations of <em translate="no">GC.Alloc</em>, allocating 100 bytes in total, which is causes by the retrieval of the camera names. Further down you'll see those names show up as samples: <em translate="no">Main Camera</em> and <em translate="no">Secondary Camera</em>.
						
						<figure>
							<img src="multiple-cameras/profiler-cg-alloc.png" width="460" height="120">
							<figcaption>Profiler with separate samples and 100B allocations.</figcaption>
						</figure>
						
						<p>Next, make a build with <em translate="no">Development Build</em> and <em translate="no">Autoconnect Profiler</em> enabled. Play the build and make sure that the profiler is connected and recording. In this case we don't get the 100 bytes of allocation and we get the single <em translate="no">Render Camera</em> sample instead.</p>
						
						<figure>
							<img src="multiple-cameras/profiler-build.png" width="460" height="152">
							<figcaption>Profiling build.</figcaption>
						</figure>
						
						<aside>
							<h3>What are the other 48 bytes allocated for?</h3>
							<div>
								<p>It's for the cameras array, over which we have no control. Its size depends on how many cameras get rendered.</p>
							</div>
						</aside>
						
						<p>We can make it clear that we're allocating memory only in the editor and not in builds by wrapping the camera name retrieval in a profiler sample named <em translate="no">Editor Only</em>. In this case we need to invoke <code>Profiler.BeginSample</code> and <code>Profiler.EndSample</code> from the <code>UnityEngine.Profiling</code> namespace. Only <code>BeginSample</code> needs to be passed the name.</p>
						
						<pre translate="no">using UnityEditor;
using UnityEngine;
<ins>using UnityEngine.Profiling;</ins>
using UnityEngine.Rendering;

partial class CameraRenderer {

	&hellip;
	
#if UNITY_EDITOR

	&hellip;

	partial void PrepareBuffer () {
		<ins>Profiler.BeginSample("Editor Only");</ins>
		buffer.name = SampleName = camera.name;
		<ins>Profiler.EndSample();</ins>
	}

#else

	string SampleName => bufferName;

#endif
}</pre>
						
						<figure>
							<img src="multiple-cameras/editor-only-allocations.png" width="460" height="122">
							<figcaption>Editor-only allocations made obvious.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Layers</h3>
						
						<p>Cameras can also be configured to only see things on certain layers. This is done by adjusting their <em translate="no">Culling Mask</em>. To see this in action let's move all objects that use the standard shader to the <em translate="no">Ignore Raycast</em> layer.</p>
						
						<figure>
							<img src="multiple-cameras/ignore-raycast-layer.png" width="320" height="44">
							<figcaption>Layer switched to <em translate="no">Ignore Raycast</em>.</figcaption>
						</figure>
						
						<p>Exclude that layer from the culling mask of <em translate="no">Main Camera</em>.</p>
						
						<figure>
							<img src="multiple-cameras/culling-ignore-raycast.png" width="320" height="194">
							<figcaption>Culling the <em translate="no">Ignore Raycast</em> layer.</figcaption>
						</figure>
						
						<p>And make it the only layer seen by <em translate="no">Secondary Camera</em>.</p>
						
						<figure>
							<img src="multiple-cameras/only-ignore-raycast.png" width="320" height="268">
							<figcaption>Culling everything but the <em translate="no">Ignore Raycast</em> layer.</figcaption>
						</figure>
						
						<p>Because <em translate="no">Secondary Camera</em> renders last we end up seeing only the invalid objects.</p>
						
						<figure>
							<img src="multiple-cameras/only-ignore-raycast-game.png" width="380" height="140">
							<figcaption>Only <em translate="no">Ignore Raycast</em> layer visible in game window.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Clear Flags</h3>
						
						<p>We can combine the results of both cameras by adjusting the clear flags of the second one that gets rendered. They're defined by a <code>CameraClearFlags</code> enum which we can retrieve via the camera's <code>clearFlags</code> property. Do this in <code>Setup</code> before clearing.</p>
						
						<pre translate="no">	void Setup () {
		context.SetupCameraProperties(camera);
		<ins>CameraClearFlags flags = camera.clearFlags;</ins>
		buffer.ClearRenderTarget(true, true, Color.clear);
		buffer.BeginSample(SampleName);
		ExecuteBuffer();
	}</pre>
						
						<p>The <code>CameraClearFlags</code> enum defines four values. From 1 to 4 they are <code>Skybox</code>, <code>Color€</code>, <code>Depth</code>, and <code>Nothing</code>. These aren't actually independent flag values but represent a decreasing amount of clearing. The depth buffer has to be cleared in all cases except the last one, so when the flags value is at most <code>Depth</code>.</p>
						
						<pre translate="no">		buffer.ClearRenderTarget(
			<ins>flags &lt;= CameraClearFlags.Depth</ins>, true, Color.clear
		);</pre>
						
						<p>We only really need to clear the color buffer when flags are set to <code>Color€</code>, because in the case of <code>Skybox</code> we end up replacing all previous color data anyway.</p>
						
						<pre translate="no">		buffer.ClearRenderTarget(
			flags &lt;= CameraClearFlags.Depth,
			<ins>flags == CameraClearFlags.Color€</ins>,
			Color.clear
		);</pre>
						
						<p>And if we're clearing to a solid color we have to use the camera's background color. But because we're rendering in linear color space we have to convert that color to linear space, so we end up needing <code>camera.backgroundColor.linear</code>. In all other cases the color doesn't matter, so we can suffice with <code>Color.clear</code>.</p>
						
						<pre translate="no">		buffer.ClearRenderTarget(
			flags &lt;= CameraClearFlags.Depth,
			flags == CameraClearFlags.Color€,
			<ins>flags == CameraClearFlags.Color€ ?</ins>
				<ins>camera.backgroundColor.linear :</ins> Color.clear
		);</pre>
						
						<p>Because <em translate="no">Main Camera</em> is the first to render, its <em translate="no">Clear Flags</em> should be set to either <code>Skybox</code> or <code>Color€</code>. When the frame debugger is enabled we always begin with a clear buffer, but this is not guaranteed in general.</p>
						
						<p>The clear flags of <em translate="no">Secondary Camera</em> determines how the rendering of both cameras gets combined. In the case of skybox or color the previous results get completely replaced. When only depth is cleared <em translate="no">Secondary Camera</em> renders as normal except that it doesn't draw a skybox, so the previous results show up as the background. When nothing gets cleared the depth buffer is retained, so unlit objects end up occluding invalid objects as if they were drawn by the same camera. However, transparent objects drawn by the previous camera have no depth information, so are drawn over, just like the skybox did earlier.</p>
						
						<figure>
							<img src="multiple-cameras/clear-color.png" width="380" height="140" alt="color"><br>
							<img src="multiple-cameras/clear-depth.png" width="380" height="140" alt="depth"><br>
							<img src="multiple-cameras/clear-nothing.png" width="380" height="140" alt="nothing">
							<figcaption>Clear color, depth-only, and nothing.</figcaption>
						</figure>
						
						<p>By adjusting the camera's <em translate="no">Viewport Rect</em> it is also possible to reduce the rendered area to only a fraction of the entire render target. The rest of the render target remains unaffected. In this case clearing happens with the <em translate="no">Hidden/InternalClear</em> shader. The stencil buffer is used to limit rendering to the viewport area.</p>
						
						<figure>
							<img src="multiple-cameras/reduced-viewport.png" width="380" height="140">
							<figcaption>Reduced viewport of secondary camera, clearing color.</figcaption>
						</figure>
						
						<p>Note that rendering more than one camera per frame means culling, setup, sorting, etc. has to be done multiple times as well. Using one camera per unique point of view is typically the most efficient approach.</p>
						
						<p>The next tutorial is <a href="../draw-calls/index.html">Draw Calls</a>.</p>
					</section>
					
					<a href="../../license/index.html" class="license">license</a>
					<a href="https://bitbucket.org/catlikecodingunitytutorials/custom-srp-01-custom-render-pipeline/" class="repository">repository</a>
					<a href="Custom-Render-Pipeline.pdf" download rel="nofollow">PDF</a>
				</section>
				
			</article>
		</main>

		<footer>
			<p>Enjoying the <a href="../../../tutorials">tutorials</a>? Are they useful? Want more?</p>
			<p><b><a href="https://www.patreon.com/catlikecoding">Please support me on Patreon!</a></b></p>
			<p><a href="https://www.patreon.com/catlikecoding"><img src="../../become-a-patron.png" alt="Become my patron!" width="217" height="51"></a></p>
			<p><b><a href="../../donating.html">Or make a direct donation</a>!</b></p>
			<p>made by <a href="../../../../about/index.html" rel="author">Jasper Flick</a></p>
		</footer>
		
		<script src="../../tutorials.js"></script>
	</body>
</html>