<!DOCTYPE html>
<html lang="en">
	<head prefix="og: http://ogp.me/ns#">
		<meta charset="utf-8">
		<meta property="og:url" content="https://catlikecoding.com/unity/tutorials/rendering/part-2/">
		<meta property="og:type" content="article">
		<meta property="og:image:width" content="1024">
		<meta property="og:image:height" content="512">
		<meta property="og:image" content="https://catlikecoding.com/unity/tutorials/rendering/part-2/tutorial-image.png">
		<meta property="og:title" content="Rendering 2">
		<meta property="og:description" content="A Unity Rendering tutorial about shader fundamentals. Part 2 of 20.">
		<meta property="twitter:card" content="summary_large_image">
		<meta property="twitter:creator" content="@catlikecoding">
		<meta name="viewport" content="width=768">
		<title>Rendering 2</title>
		<link href="../../tutorials.css" rel="stylesheet">

				<link rel="manifest" href="https://catlikecoding.com/site.webmanifest">
		<link rel="mask-icon" href="https://catlikecoding.com/safari-pinned-tab.svg" color="#aa0000">

		<script type="application/ld+json">{
			"@context": "http://schema.org",
			"@type": "WebPage",
			"mainEntity": {
				"@type": "TechArticle",
				"@id": "https://catlikecoding.com/unity/tutorials/rendering/part-2/#article",
				"headline": "Rendering 2",
				"alternativeHeadline": "Shader Fundamentals",
				"datePublished": "2016-03-29",
				"author": { "@type": "Person", "name": "Jasper Flick", "@id": "https://catlikecoding.com/jasper-flick/#person" },
				"publisher": { "@type": "Organization", "name": "Catlike Coding", "@id": "https://catlikecoding.com/#organization" },
				"description": "A Unity Rendering tutorial about shader fundamentals. Part 2 of 20.",
				"image": "https://catlikecoding.com/unity/tutorials/rendering/part-2/tutorial-image.png",
				"dependencies": "Unity 5.4.0b10",
				"proficiencyLevel": "Beginner"
			},
			"breadcrumb": {
				"@type": "BreadcrumbList",
				"itemListElement": [
					{ "@type": "ListItem", "position": 1, "item": { "@id": "https://catlikecoding.com/unity/", "name": "Unity" }},
					{ "@type": "ListItem", "position": 2, "item": { "@id": "https://catlikecoding.com/unity/tutorials/", "name": "Tutorials" }},
					{ "@type": "ListItem", "position": 3, "item": { "@id": "https://catlikecoding.com/unity/tutorials/rendering/", "name": "Rendering" }}
				]
			}
		}</script>
		<script>
			var customTypes = {
			};
			
			var hasAnimations = true;
			var hasMath = true;
		</script>
	</head>
	<body>
		<header>
			<a href="https://catlikecoding.com"><img src="https://catlikecoding.com/catlike-coding-logo.svg" alt="Catlike Coding" width="45" height="45"></a>
			<nav>
				<ol>
					<li><a href="https://catlikecoding.com">Catlike Coding</a></li>
					<li><a href="../../../">Unity</a></li>
					<li><a href="../../../tutorials/">Tutorials</a></li>
					<li><a href="../../../tutorials/rendering/">Rendering</a></li>
				</ol>
			</nav>
		</header>
		
		<main>
			<article>
				<header>
					<h1>Rendering 2</h1>
					<p>Shader Fundamentals</p>
					<ul>
						<li>Transform vertices.</li>
						<li>Color pixels.</li>
						<li>Use shader properties.</li>
						<li>Pass data from vertices to fragments.</li>
						<li>Inspect compiled shader code.</li>
						<li>Sample a texture, with tiling and offset.</li>
					</ul>
				</header>

				<p>This is the second part of a tutorial series about rendering. The <a href="../../../tutorials/rendering/part-1">first part</a> was about matrices. This time we'll write our first shader and import a texture.</p>
				
				<p>This tutorials was made using Unity 5.4.0b10.</p>
				
				<figure>
					<img src="tutorial-image.png" width="375" height="325">
					<figcaption>Texturing a sphere.</figcaption>
				</figure>
				
				<section>
					<h2>Default Scene</h2>
					
					<p>When you create a new scene in Unity, you start with a default camera and directional light. Create a simple sphere via <i>GameObject / 3D Object / Sphere</i>, put it at the origin, and place the camera just in front of it.</p>
					
					<figure>
						<img src="default-scene/default-spere.png" width="250" height="250">
						<figcaption>Default sphere in default scene.</figcaption>
					</figure>
					
					<p>This is a very simple scene, yet there is already a lot of complex rendering going on. To get a good grip on the rendering process, it helps to get rid of all the fancy stuff and fist concern us with the fundamentals only.</p>
					
					<section>
						<h3>Stripping It Down</h3>
						
						<p>Have a look at the lighting settings for the scene, via <i>Window / Lighting</i>. This will summon a lighting window with three tabs. We're only interested in the <i>Scene</i> tab, which is active by default.</p>
						
						<figure>
							<img src="default-scene/default-lighting.png" width="320" height="360">
							<figcaption>Default lighting settings.</figcaption>
						</figure>
						
						<p>There is a section about environmental lighting, where you can select a skybox. This skybox is currently used for the scene background, for ambient lighting, and for reflections. Set it to none so it is switched off.</p>
						
						<p>While you're at it, you can also switch off the precomputed and real-time global illumination panels. We're not going to use those anytime soon.</p>
						
						<figure>
							<img src="default-scene/simplified-lighting.png" width="320" height="280">
							<figcaption>No more skybox.</figcaption>
						</figure>
						
						<p>Without a skybox, the ambient source automatically switches to a solid color. The default color is dark gray with a very slight blue tint. Reflections become solid black, as indicated by a warning box.</p>
						
						<p>As you might expect, the sphere has become darker and the background is now a solid color. However, the background is dark blue. Where does that color come from?</p>
						
						<figure>
							<img src="default-scene/without-skybox.png" width="250" height="250">
							<figcaption>Simplified lighting.</figcaption>
						</figure>
						
						<p>The background color is defined per camera. It renders the skybox by default, but it too falls back to a solid color.</p>
						
						<figure>
							<img src="default-scene/camera.png" width="320" height="328">
							<figcaption>Default camera settings.</figcaption>
						</figure>
						
						<aside>
							<h3>Why is the background color alpha 5 instead of 255?</h3>
							<div>
								<p>No idea why that's the default, really. But it doesn't matter. This color replaces the previous image completely. It doesn't mix.</p>
							</div>
						</aside>
						
						<p>To further simplify the rendering, deactivate the directional light object, or delete it. This will get rid of the direct lighting in the scene, as well as the shadows that would be cast by it. What's left is the solid background, with the silhouette of the sphere in the ambient color.</p>
						
						<figure>
							<img src="default-scene/without-light.png" width="250" height="250">
							<figcaption>In the dark.</figcaption>
						</figure>
					</section>
					
					<a href="default-scene/default-scene.unitypackage" download rel="nofollow">unitypackage</a>
				</section>
				
				<section>
					<h2>From Object to Image</h2>
					
					<p>Our very simple scene is drawn in two steps. First, the image is filled with the background color of the camera. Then our sphere's silhouette is drawn on top of that. </p>
					
					<p>How does Unity know that it has to draw a sphere? We have a sphere object, and this object has a mesh renderer component. If this object lies inside the camera's view, it should be rendered. Unity verifies this by checking whether the object's bounding box intersects the camera's view frustum.</p>
					
					<aside>
						<h3>What's a bounding box?</h3>
						<div>
							<p>Take any mesh. Now find the smallest box in which that mesh would fit. That's a bounding box. It is automatically derived from the mesh of an object.</p>
							
							<p>You can consider bounding boxes simple approximations of the volume occupied by a mesh. If you cannot see the box, you certainly cannot see the mesh.</p>
						</div>
					</aside>
					
					<figure>
						<img src="from-object-to-image/sphere-object.png" width="320" height="388">
						<figcaption>Default sphere object.</figcaption>
					</figure>
					
					<p>The transform component is used to alter the position, orientation, and size of the mesh and bounding box. Actually, the entire transformation hierarchy is used, as described in <a href="../../../tutorials/rendering/part-1">part 1, Matrices</a>. If the object ends up in the camera's view, it is scheduled for rendering.</p>
					
					<p>Finally, the GPU is tasked with rendering the object's mesh. The specific rendering instructions are defined by the object's material. The material references a shader &ndash; which is a GPU program &ndash; plus any settings it might have.</p>
					
					<figure>
						<img src="from-object-to-image/cpu-gpu.png" width="350" height="150">
						<figcaption>Who controls what.</figcaption>
					</figure>
					
					<p>Our object currently has the default material, which uses Unity's Standard shader. We're going to replace it with our own shader, which we'll build from the ground up.</p>
					
					<section>
						<h3>Your First Shader</h3>
						
						<p>Create a new shader via <i>Assets / Create / Shader / Unlit Shader</i> and name it something like <i>My First Shader</i>.</p>
						
						<figure>
							<img src="from-object-to-image/my-first-shader.png" width="134" height="38">
							<figcaption>Your first shader.</figcaption>
						</figure>
						
						<p>Open the shader file and delete its contents, so we can start from scratch.</p>
						
						<p>A shader is defined with the <code class="shader">Shader</code> keyword. It is followed by a string that describes the shader menu item that you can use to select this shader. It doesn't need to match the file name. After that comes the block with the shader's contents.</p>
						
						<pre translate="no" class="shader"><mark>Shader "Custom/My First Shader" {</mark>

<mark>}</mark></pre>
						
						<p>Save the file. You will get a warning that the shader is not supported, because it has no sub-shaders or fallbacks. That's because it's empty.</p>
						
						<p>Although the shader is nonfunctional, we can already assign it to a material. So create a new material via <i>Assets / Create / Material</i> and select our shader from the shader menu.</p>
						
						<figure>
							<img alt="assets" src="from-object-to-image/my-material.png" width="134" height="70">
							<img alt="material" src="from-object-to-image/material-with-shader.png" width="320" height="70">
							<figcaption>Material with your shader.</figcaption>
						</figure>
						
						<p>Change our sphere object so it uses our own material, instead of the default material. The sphere will become magenta. This happens because Unity will switch to an error shader, which uses this color to draw your attention to the problem.</p>
						
						<figure>
							<img alt="object" src="from-object-to-image/object-with-material.png" width="320" height="214">
							<img alt="sphere" src="from-object-to-image/sphere-with-material.png" width="250" height="250">
							<figcaption>Material with your shader.</figcaption>
						</figure>
						
						<p>The shader error mentioned sub-shaders. You can use these to group multiple shader variants together. This allows you to provide different sub-shaders for different build platforms or levels of detail. For example, you could have one sub-shader for desktops and another for mobiles. We need just one sub-shader block.</p>
						
						<pre translate="no" class="shader">Shader "Custom/My First Shader" {

	<mark>SubShader {</mark>
		
	<mark>}</mark>
}</pre>
						
						<p>The sub-shader has to contain at least one pass. A shader pass is where an object actually gets rendered. We'll use one pass, but it's possible to have more. Having more than one pass means that the object gets rendered multiple times, which is required for a lot of effects.</p>
						
						<pre translate="no" class="shader">Shader "Custom/My First Shader" {

	SubShader {

		<mark>Pass {</mark>

		<mark>}</mark>
	}
}</pre>
						
						<p>Our sphere might now become white, as we're using the default behavior of an empty pass. If that happens, it means that we no longer have any shader errors. However, you might still see old errors in the console. They tend to stick around, not getting cleared when a shader recompiles without errors.</p>
						
						<figure>
							<img src="from-object-to-image/sphere-with-pass.png" width="250" height="250">
							<figcaption>A white sphere.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Shader Programs</h3>
						
						<p>It is now time to write our own shader program. We do so with Unity's shading language, which is a variant of the HLSL and CG shading languages. We have to indicate the start of our code with the <code class="shader">CGPROGRAM</code> keyword. And we have to terminate with the <code class="shader">ENDCG</code> keyword.</p>
						
						<pre translate="no" class="shader">		Pass {
			<mark>CGPROGRAM</mark>

			<mark>ENDCG</mark>
		}</pre>
						
						<aside>
							<h3>Why are those keywords needed?</h3>
							<div>
								<p>Shader passes can contain other statements besides the shader program. So the program has to be separated somehow. Why not use another block for that? No idea. You will encounter more oddities like this. They are often old design decisions that made sense once, but no longer. Due to backwards compatibility, we're still stuck with them.</p>
							</div>
						</aside>
						
						<p>The shader compiler is now complaining that our shader doesn't have vertex and fragment programs. Shaders consist of two programs each. The vertex program is responsible for processing the vertex data of a mesh. This includes the conversion from object space to display space, just like we did in <a href="../../../tutorials/rendering/part-1">part 1, Matrices</a>. The fragment program is responsible for coloring individual pixels that lie inside the mesh's triangles.</p>
						
						<figure>
							<img src="from-object-to-image/vertex-fragment-programs.png" width="435" height="250">
							<figcaption>Vertex and fragment program.</figcaption>
						</figure>
							
						<p>We have to tell the compiler which programs to use, via pragma directives.</p>
						
						<pre translate="no" class="shader">			CGPROGRAM

			<mark>#pragma vertex MyVertexProgram</mark>
			<mark>#pragma fragment MyFragmentProgram</mark>

			ENDCG</pre>
						
						<aside>
							<h3>What's a pragma?</h3>
							<div>
								<p>The word <i>pragma</i> comes from Greek and refers to an action, or something that needs to be done. It's used in many programming languages to issue special compiler directives.</p>
							</div>
						</aside>
						
						<p>The compiler again complains, this time because it cannot find the programs that we specified. That's because we haven't defined them yet.</p>
						
						<p>The vertex and fragment programs are written as methods, quite like in C#, though they're typically referred to as functions. Let's simply create two empty void methods with the appropriate names.</p>
						
						<pre translate="no" class="shader">			CGPROGRAM

			#pragma vertex MyVertexProgram
			#pragma fragment MyFragmentProgram

			<mark>void MyVertexProgram () {</mark>

			<mark>}</mark>

			<mark>void MyFragmentProgram () {</mark>

			<mark>}</mark>

			ENDCG</pre>
						
						<p>At this point the shader will compile, and the sphere will disappear. Or you will still get errors. It depends on which rendering platform your editor is using. If you're using Direct3D 9, you'll probably get errors.</p>
					</section>
					
					<section>
						<h3>Shader Compilation</h3>
						
						<p>Unity's shader compiler takes our code and transforms it into a different program, depending on the target platform. Different platforms require different solutions. For example, Direct3D for Windows, OpenGL for Macs, OpenGL ES for mobiles, and so on. We're not dealing with a single compiler here, but multiple.</p>
						
						<p>Which compiler you end up using depends on what you're targeting. And as these compilers are not identical, you can end up with different results per platform. For example, our empty programs work fine with OpenGL and Direct3D 11, but fail when targeting Direct3D 9.</p>
						
						<p>Select the shader in the editor and look at the inspector window. It displays some information about the shader, including the current compiler errors. There is also a <i>Compiled code</i> entry with a <i>Compile and show code</i> button and a dropdown menu. If you click the button, Unity will compile the shader and open its output in your editor, so you can inspect the generated code.</p>
						
						<figure>
							<img src="from-object-to-image/shader-inspector.png" width="320" height="490">
							<figcaption>Shader inspector, with errors for all platforms.</figcaption>
						</figure>
						
						<p>You can select which platforms you manually compile the shader for, via the dropdown menu. The default is to compile for the graphics device that's used by your editor. You can manually compile for other platforms as well, either your current build platform, all platforms you have licenses for, or a custom selection. This enables you to quickly make sure that your shader compiles on multiple platforms, without having to make complete builds.</p>
						
						<figure>
							<img src="from-object-to-image/compilation-selection.png" width="210" height="244">
							<figcaption>Selecting OpenGLCore.</figcaption>
						</figure>
						
						<p>To compile the selected programs, close the pop-up and click the <i>Compile and show code</i> button. Clicking the little <i>Show</i> button inside the pop-up will show you the used shader variants, which is not useful right now.</p>
						
						<p>For example, here is the resulting code when our shader is compiled for OpenGlCore.</p>
						
						<pre translate="no" class="shader">// Compiled shader for custom platforms, uncompressed size: 0.5KB

// Skipping shader variants that would not be included into build of current scene.

Shader "Custom/My First Shader" {
SubShader { 
 Pass {
  GpuProgramID 16807
Program "vp" {
SubProgram "glcore " {
"#ifdef VERTEX
#version 150
#extension GL_ARB_explicit_attrib_location : require
#extension GL_ARB_shader_bit_encoding : enable
void main()
{
    return;
}
#endif
#ifdef FRAGMENT
#version 150
#extension GL_ARB_explicit_attrib_location : require
#extension GL_ARB_shader_bit_encoding : enable
void main()
{
    return;
}
#endif
"
}
}
Program "fp" {
SubProgram "glcore " {
"// shader disassembly not supported on glcore"
}
}
 }
}
}</pre>
						
						<p>The generated code is split into two blocks, vp and fp, for the vertex and fragment programs. However, in the case of OpenGL both programs end up in the vp block. The two main functions correspond two our empty methods. So let's focus on those and ignore the other code.</p>
						
						<pre translate="no" class="shader">#ifdef VERTEX
void main()
{
    return;
}
#endif
#ifdef FRAGMENT
void main()
{
    return;
}
#endif</pre>
						
						<p>And here is the generated code for Direct3D 11, stripped down to the interesting parts. It looks quite different, but it's obvious that the code doesn't do much.</p>
						
						<pre translate="no" class="shader">Program "vp" {
SubProgram "d3d11 " {
      vs_4_0
   0: ret 
}
}
Program "fp" {
SubProgram "d3d11 " {
      ps_4_0
   0: ret 
}
}</pre>
						
						<p>As we work on our programs, I will often show the compiled code for OpenGLCore and D3D11, so you can get an idea of what's happening under the hood.</p>
						
					</section>
					
					<section>
						<h3>Including Other Files</h3>
						
						<p>To produce a functional shader you need a lot of boilerplate code. Code that defines common variables, functions, and other things. Were this a C# program, we'd put that code in other classes. But shaders don't have classes. They're just one big file with all the code, without the grouping provided by classes or namespaces.</p>
						
						<p>Fortunately, we can split the code into multiple files. You can use the <code class="shader">#include</code> directive to load a different file's contents into the current file. A typical file to include is <i>UnityCG.cginc</i>, so let's do that.</p>
						
						<pre translate="no" class="shader">			CGPROGRAM

			#pragma vertex MyVertexProgram
			#pragma fragment MyFragmentProgram

			<mark>#include "UnityCG.cginc"</mark>

			void MyVertexProgram () {

			}

			void MyFragmentProgram () {

			}

			ENDCG</pre>
						
						<p><i>UnityCG.cginc</i> is one of the shader include files that are bundled with Unity. It includes a few other essential files, and contains some generic functionality.</p>
						
						<figure>
							<img src="from-object-to-image/include-files.png" width="375" height="150">
							<figcaption>Include file hierarchy, starting at UnityCG.</figcaption>
						</figure>
						
						<p><i>UnityShaderVariables.cginc</i> defines a whole bunch of shader variables that are necessary for rendering, like transformation, camera, and light data. These are all set by Unity when needed.</p>
						
						<p><i>HLSLSupport.cginc</i> sets things up so you can use the same code no matter which platform you're targeting. So you don't need to worry about using platform-specific data types and such.</p>
						
						<p><i>UnityInstancing.cginc</i> is specifically for instancing support, which is a specific rendering technique to reduce draw calls. Although it doesn't include the file directly, it depends on <i>UnityShaderVariables</i>.</p>
						
						<p>Note that the contents of these files are effectively copied into your own file, replacing the including directive. This happens during a pre-processing step, which carries out all the pre-processing directives. Those directives are all statements that start with a hash, like <code class="shader">#include</code> and <code class="shader">#pragma</code>. After that step is finished, the code is processed again, and it is actually compiled.</p>
						
						<aside>
							<h3>What happens when you include a file more than once?</h3>
							<div>
								<p>Its contents get copied into your code more than once. You typically don't want to do this, as you'll likely get compiler errors due to duplicate definitions.</p>
								<p>There is an include file programming convention which guards against redefinitions. We'll use it when we'll write our own include files. But that's for a future tutorial.</p>
							</div>
						</aside>
						
					</section>
					
					<section>
						<h3>Producing Output</h3>
						
						<p>To render something, our shader programs have to produce results. The vertex program has to return the final coordinates of a vertex. How many coordinates? Four, because we're using 4 by 4 transformation matrices, as described in <a href="../../../tutorials/rendering/part-1">part 1, Matrices</a>.</p>
						
						<p>Change the function's type from <code class="shader">void</code> to <code class="shader">float4</code>. A <code class="shader">float4</code> is simply a collection of four floating-point numbers. Just return 0 for now.</p>
						
						<pre translate="no" class="shader">			<mark>float4</mark> MyVertexProgram () {
				<mark>return 0;</mark>
			}</pre>
						
						<aside>
							<h3>Is 0 a valid value to return?</h3>
							<div>
								<p>When using a single value like this, the compiled will repeat it for all float components. You can also be explicit and return <code class="shader">float4(0, 0, 0, 0)</code> if you like.</p>
							</div>
						</aside>
						
						<p>We're now getting an error about missing semantics. The compiler sees that we're returning a collection of four floats, but it doesn't know what that data represents. So it doesn't know what the GPU should do with it. We have to be very specific about the output of our program.</p>
						
						<p>In this case, we're trying to output the position of the vertex. We have to indicate this by attaching the <code class="shader">SV_POSITION</code> semantic to our method. <i>SV</i> stands for system value, and <i>POSITION</i> for the final vertex position.</p>
						
						<pre translate="no" class="shader">			float4 MyVertexProgram () <mark>: SV_POSITION</mark> {
				return 0;
			}</pre>
						
						<p>The fragment program is supposed to output an RGBA color value for one pixel. We can use a <code class="shader">float4</code> for that as well. Returning 0 will produce solid back.</p>
						
						<pre translate="no" class="shader">			<mark>float4</mark> MyFragmentProgram () {
				<mark>return 0;</mark>
			}</pre>
						
						<aside>
							<h3>Wouldn't 0 alpha be fully transparent?</h3>
							<div>
								<p>It would be, except that our shader actually ignores the alpha channel. We're working with an opaque shader right now. If we were writing a shader with support for transparency, you'd be right. We'll do that in a future tutorial.</p>
							</div>
						</aside>
						
						<p>The fragment program requires semantics as well. In this case, we have to indicate where the final color should be written to. We use <code class="shader">SV_TARGET</code>, which is the default shader target. This is the frame buffer, which contains the image that we are generating.</p>
						
						<pre translate="no" class="shader">			float4 MyFragmentProgram () <mark>: SV_TARGET</mark> {
				return 0;
			}</pre>
						
						<p>But wait, the output of the vertex program is used as input for the fragment program. This suggests that the fragment program should get a parameter that matches the vertex program's output.</p>
						
						<pre translate="no" class="shader">			float4 MyFragmentProgram (<ins>float4 position</ins>) : SV_TARGET {
				return 0;
			}</pre>
						
						<p>It doesn't matter what name we give to the parameter, but we have to make sure to use the correct semantic.</p>
						
						<pre translate="no" class="shader">			float4 MyFragmentProgram (
				float4 position <ins>: SV_POSITION</ins>
			) : SV_TARGET {
				return 0;
			}</pre>
						
						<aside>
							<h3>Can we omit the position parameter?</h3>
							<div>
								<p>As we're not using it, we might as well leave it out. However, this confuses some shader compilers, when multiple parameters are involved. So it is best to exactly match the fragment program input with the vertex program output.</p>
							</div>
						</aside>
						
						<p>Our shader once again compiles without errors, but the sphere has disappeared. This shouldn't be surprising, because we collapse all its vertices to a single point.</p>
						
						<p>If you look at the compiled OpenGLCore programs, you'll see that they now write to output values. And our single values have indeed been replaced with four-component vectors.</p>
						
						<pre translate="no" class="shader">#ifdef VERTEX
void main()
{
    gl_Position = vec4(0.0, 0.0, 0.0, 0.0);
    return;
}
#endif
#ifdef FRAGMENT
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0 = vec4(0.0, 0.0, 0.0, 0.0);
    return;
}
#endif</pre>
						
						<p>The same is true for the D3D11 programs, although the syntax is different.</p>
						
						<pre translate="no">Program "vp" {
SubProgram "d3d11 " {
      vs_4_0
      dcl_output_siv o0.xyzw, position
   0: mov o0.xyzw, l(0,0,0,0)
   1: ret 
}
}
Program "fp" {
SubProgram "d3d11 " {
      ps_4_0
      dcl_output o0.xyzw
   0: mov o0.xyzw, l(0,0,0,0)
   1: ret 
}
}</pre>
					</section>
					
					<section>
						<h3>Transforming Vertices</h3>
						
						<p>To get our sphere back, our vertex program has to produce a correct vertex position. To do so, we need to know the object-space position of the vertex. We can access it by adding a variable with the <code class="shader">POSITION</code> semantic to our function. The position will then be provided as homogeneous coordinates of the form `[[x],[y],[z],[1]]`, so its type is <code class="shader">float4</code>.</p>
						
						<pre translate="no" class="shader">			float4 MyVertexProgram (<mark>float4 position : POSITION</mark>) : SV_POSITION {
				return 0;
			}</pre>
						
						<p>Let's start by directly returning this position.</p>
						
						<pre translate="no" class="shader">			float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return <mark>position</mark>;
			}</pre>
						
						<p>The compiled vertex programs will now have a vertex input and copy it to their output.</p>
						
						<pre translate="no" class="shader">in  vec4 in_POSITION0;
void main()
{
    gl_Position = in_POSITION0;
    return;
}</pre>

						<pre translate="no" class="shader">
Bind "vertex" Vertex
      vs_4_0
      dcl_input v0.xyzw
      dcl_output_siv o0.xyzw, position
   0: mov o0.xyzw, v0.xyzw
   1: ret</pre>
						
						<figure>
							<img src="from-object-to-image/raw-position.png" width="250" height="250">
							<figcaption>Raw vertex positions.</figcaption>
						</figure>
						
						<p>A black sphere will become visible, but it will be distorted. That's because we're using the object-space positions as if they were display positions. As such, moving the sphere around will make no difference, visually.</p>
						
						<p>We have to multiply the raw vertex position with the model-view-projection matrix. This matrix combines the object's transform hierarchy with the camera transformation and projection, like we did in <a href="../../../tutorials/rendering/part-1">part 1, Matrices</a>.</p>
						
						<p>The 4 by 4 MVP matrix is defined in <i>UnityShaderVariables</i> as <code class="shader"></code><code class="shader">UNITY_MATRIX_MVP</code>. We can use the <code class="shader">mul</code> function to multiply it with the vertex position. This will correctly project our sphere onto the display. You can also move, rotate, and scale it and the image will change as expected.</p>
						
						<pre translate="no" class="shader">			float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return <mark>mul(UNITY_MATRIX_MVP,</mark> position<mark>)</mark>;
			}</pre>
						
						<figure>
							<img src="from-object-to-image/correct-position.png" width="250" height="250">
							<figcaption>Correctly positioned.</figcaption>
						</figure>
						
						<p>If you check the OpenGLCore vertex program, you will notice that a lot of uniform variables have suddenly appeared. Even though they aren't used, and will be ignored, accessing the matrix triggered the compiler to include the whole bunch.</p>
						
						<aside>
							<h3>What are uniform variables?</h3>
							<div>
								<p>Uniform means that a variable has the same value for all vertices and fragments of a mesh. So it is uniform across all its vertices and fragments.</p>
								
								<p>You could explicitly mark variables as uniform in your own shader programs, but it is not required.</p>
							</div>
						</aside>
						
						<p>You will also see the matrix multiplication, encoded as a bunch of multiplications and additions.</p>
						
						<pre translate="no" class="shader">uniform 	vec4 _Time;
uniform 	vec4 _SinTime;
uniform 	vec4 _CosTime;
uniform 	vec4 unity_DeltaTime;
uniform 	vec3 _WorldSpaceCameraPos;
&hellip;
in  vec4 in_POSITION0;
vec4 t0;
void main()
{
    t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1];
    t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0;
    t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0;
    gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0;
    return;
}</pre>
						
						<p>The D3D11 compiler doesn't bother with including unused variables. It encodes the matrix multiplication with a <code class="shader">mul</code> and three <code class="shader">mad</code> instructions. The mad instruction represents a multiplication followed by an addition.</p>
						
						<pre translate="no" class="shader">Bind "vertex" Vertex
ConstBuffer "UnityPerDraw" 352
Matrix 0 [glstate_matrix_mvp]
BindCB  "UnityPerDraw" 0
      vs_4_0
      dcl_constantbuffer cb0[4], immediateIndexed
      dcl_input v0.xyzw
      dcl_output_siv o0.xyzw, position
      dcl_temps 1
   0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw
   1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw
   2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw
   3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw
   4: ret</pre>
					</section>
					
					<a href="from-object-to-image/from-object-to-image.unitypackage" download rel="nofollow">unitypackage</a>
				</section>
				
				<section>
					<h2>Coloring Pixels</h2>
					
					<p>Now that we got the shape right, let's add some color. The simplest is to use a constant color, for example yellow.</p>
					
					<pre translate="no" class="shader">			float4 MyFragmentProgram (
				float4 position : SV_POSITION
			) : SV_TARGET {
				return <mark>float4(1, 1, 0, 1)</mark>;
			}</pre>
					
					<figure>
						<img src="coloring-pixels/yellow-sphere.png" width="250" height="250">
						<figcaption>Yellow sphere.</figcaption>
					</figure>
					
					<p>Of course you don't always want yellow objects. Ideally, our shader would support any color. Then you could use the material to configure which color to apply. This is done via shader properties.</p>
					
					<section>
						<h3>Shader Properties</h3>
						
						<p>Shader properties are declared in a separate block. Add it at the top of the shader.</p>
						
						<pre translate="no" class="shader">Shader "Custom/My First Shader" {

	<mark>Properties {</mark>
	<mark>}</mark>

	SubShader {
		&hellip;
	}
}</pre>
						
						<p>Put a property named <i>_Tint</i> inside the new block. You could give it any name, but the convention is to start with an underscore followed by a capital letter, and lowercase after that. The idea is that nothing else uses this convention, which prevents accidental duplicate names.</p>
						
						<pre translate="no" class="shader">	Properties {
		<mark>_Tint</mark>
	}</pre>
						<p>The property name must be followed by a string and a type, in parenthesis, as if you're invoking a method. The string is used to label the property in the material inspector. In this case, the type is <code class="shader">Color</code>.</p>
						
						<pre translate="no" class="shader">	Properties {
		_Tint <mark>("Tint", Color)</mark>
	}</pre>
						
						<p>The last part of the property declaration is the assignment of a default value. Let's set it to white.</p>
						
						<pre translate="no" class="shader">	Properties {
		_Tint ("Tint", Color) <mark>= (1, 1, 1, 1)</mark>
	}</pre>
						
						<p>Our tint property should now show up in the properties section of our shader's inspector.</p>
						
						<figure>
							<img src="coloring-pixels/shader-tint.png" width="320" height="40">
							<figcaption>Shader Properties.</figcaption>
						</figure>
						
						<p>When you select your material, you will see the new <i>Tint</i> property, set to white. You can change it to any color you like, for example green.</p>
						
						<figure>
							<img src="coloring-pixels/material-tint.png" width="320" height="70">
							<figcaption>Material Properties.</figcaption>
						</figure>
						
					</section>
					
					<section>
						<h3>Accessing Properties</h3>
						
						<p>To actually use the property, we have to add a variable to the shader code. Its name has to exactly match the property name, so it'll be <code class="shader">_Tint</code>. We can then simply return that variable in our fragment program.</p>
						
						<pre translate="no" class="shader">			#include "UnityCG.cginc"

			<mark>float4 _Tint;</mark>

			float4 MyVertexProgram (float4 position : POSITION) : SV_POSITION {
				return mul(UNITY_MATRIX_MVP, position);
			}

			float4 MyFragmentProgram (
				float4 position : SV_POSITION
			) : SV_TARGET {
				return <mark>_Tint</mark>;
			}</pre>
						
						<p>Note that the variable has to be defined before it can be used. While you could change the order of fields and methods in a C# class without issues, this is not true for shaders. The compiler works from top to bottom. It will not look ahead.</p>
						
						<p>The compiled fragment programs now include the tint variable.</p>
						
						<pre translate="no" class="shader">uniform 	vec4 _Time;
uniform 	vec4 _SinTime;
uniform 	vec4 _CosTime;
uniform 	vec4 unity_DeltaTime;
uniform 	vec3 _WorldSpaceCameraPos;
&hellip;
uniform 	vec4 _Tint;
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0 = _Tint;
    return;
}</pre>
						
						<pre translate="no" class="shader">ConstBuffer "$Globals" 112
Vector 96 [_Tint]
BindCB  "$Globals" 0
      ps_4_0
      dcl_constantbuffer cb0[7], immediateIndexed
      dcl_output o0.xyzw
   0: mov o0.xyzw, cb0[6].xyzw
   1: ret</pre>
						
						<figure>
							<img src="coloring-pixels/green-sphere.png" width="250" height="250">
							<figcaption>Green sphere.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>From Vertex To Fragment</h3>
						
						<p>So far we've given all pixels the same color, but that is quite limiting. Usually, vertex data plays a big role. For example, we could interpret the position as a color. However, the transformed position isn't very useful. So let's instead use the local position in the mesh as a color. How do we pass that extra data from the vertex program to the fragment program?</p>
						
						<p>The GPU creates images by rasterizing triangles. It takes three processed vertices and interpolates between them. For every pixel covered by the triangle, it invokes the fragment program, passing along the interpolated data.</p>
						
						<figure>
							<img src="coloring-pixels/interpolation.png" width="440" height="385">
							<figcaption>Interpolating vertex data.</figcaption>
						</figure>
						
						<p> So the output of the vertex program isn't directly used as input for the fragment program at all. The interpolation process sits in between. Here the <code class="shader">SV_POSITION</code> data gets interpolated, but other things can be interpolated as well.</p>
						
						<p>To access the interpolated local position, add a parameter to the fragment program. As we only need the X, Y, and Z components, we can suffice with a <code class="shader">float3</code>. We can then output the position as if it were a color. We do have to provide the fourth color component, which can simply remain 1.</p>
						
						<pre translate="no" class="shader">			float4 MyFragmentProgram (
				float4 position : SV_POSITION<ins>,</ins>
				<mark>float3 localPosition</mark>
			) : SV_TARGET {
				return float4(<mark>localPosition</mark>, 1);
			}</pre>
						
						<p>Once again we have to use semantics to tell the compiler how to interpret this data. We'll use <code class="shader">TEXCOORD0</code>.</p>
						
						<pre translate="no" class="shader">			float4 MyFragmentProgram (
				float4 position : SV_POSITION,
				float3 localPosition <mark>: TEXCOORD0</mark>
			) : SV_TARGET {
				return float4(localPosition, 1);
			}</pre>
						
						<aside>
							<h3>We're not working with texture coordinates, so why TEXCOORD0?</h3>
							<div>
								<p>There are no generic semantics for interpolated data. Everyone just uses the texture coordinate semantics for everything that's interpolated and is not the vertex position. <code class="shader">TEXCOORD0</code>, <code class="shader">TEXCOORD1</code>, <code class="shader">TEXCOORD2</code>, and so on. It's done for compatibility reasons.</p>
								
								<p>There are also special color semantics, but those are rarely used and they're not available on all platforms.</p>
							</div>
						</aside>
						
						<p>The compiled fragment shaders will now use the interpolated data instead of the uniform tint.</p>
						
						<pre translate="no" class="shader">in  vec3 vs_TEXCOORD0;
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0.xyz = vs_TEXCOORD0.xyz;
    SV_TARGET0.w = 1.0;
    return;
}</pre>
						
						<pre translate="no" class="shader">     ps_4_0
      dcl_input_ps linear v0.xyz
      dcl_output o0.xyzw
   0: mov o0.xyz, v0.xyzx
   1: mov o0.w, l(1.000000)
   2: ret</pre>
						
						<p>Of course the vertex program has to output the local position for this to work. We can do that by adding an output parameter to it, with the same <code class="shader">TEXCOORD0</code> semantic. The parameter names of the vertex and fragment functions do not need to match. It's all about the semantics.</p>
						
						<pre translate="no" class="shader">			float4 MyVertexProgram (
				float4 position : POSITION<mark>,</mark>
				<mark>out float3 localPosition : TEXCOORD0</mark>
			) : SV_POSITION {
				return mul(UNITY_MATRIX_MVP, position);
			}</pre>
						
						<p>To pass the data through the vertex program, copy the X, Y, and Z components from <code class="shader">position</code> to <code class="shader">localPosition</code>.</p>
						
						<pre translate="no" class="shader">			float4 MyVertexProgram (
				float4 position : POSITION,
				out float3 localPosition : TEXCOORD0
			) : SV_POSITION {
				<mark>localPosition = position.xyz;</mark>
				return mul(UNITY_MATRIX_MVP, position);
			}</pre>
						
						<aside>
							<h3>What does <code class="shader">.xyz</code> do?</h3>
							<div>
								<p>This is known as a swizzle operation. It's just like accessing a single component of a vector, but more flexible. You can use it to filter, reorder, and repeat float components. For example <code class="shader">.x</code>, <code class="shader">.xy</code>, <code class="shader">.yx</code>, <code class="shader">.xx</code>. In this case, we're using it to grab the first three components of the position, ignoring the fourth. All four components would be <code class="shader">.xyzw</code>. You could use color naming conventions as well, like <code class="shader">.rgba</code>.</p>
							</div>
						</aside>
						
						<p>The extra vertex program output gets included in the compiler shaders, and we'll see our sphere get colorized.</p>
						
						<pre translate="no" class="shader">in  vec4 in_POSITION0;
out vec3 vs_TEXCOORD0;
vec4 t0;
void main()
{
    t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1];
    t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0;
    t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0;
    gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0;
    vs_TEXCOORD0.xyz = in_POSITION0.xyz;
    return;
}</pre>
						
						<pre translate="no" class="shader">Bind "vertex" Vertex
ConstBuffer "UnityPerDraw" 352
Matrix 0 [glstate_matrix_mvp]
BindCB  "UnityPerDraw" 0
      vs_4_0
      dcl_constantbuffer cb0[4], immediateIndexed
      dcl_input v0.xyzw
      dcl_output_siv o0.xyzw, position
      dcl_output o1.xyz
      dcl_temps 1
   0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw
   1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw
   2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw
   3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw
   4: mov o1.xyz, v0.xyzx
   5: ret</pre>
						
						<figure>
							<img src="coloring-pixels/local-position.png" width="250" height="250">
							<figcaption>Interpreting local positions as colors.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Using Structures</h3>
						
						<p>Do you think that the parameter lists of our programs look messy? It will only get worse as we pass more and more data between them. As the vertex output should match the fragment input, it would be convenient if we could define the parameter list in one place. Fortunately, we can do so.</p>
						
						<p>We can define data structures, which are simply a collection of variables. They are akin to structs in C#, except that the syntax is a little different. Here is a struct that defines the data that we're interpolating. Note the usage of a semicolon after its definition.</p>
						
						<pre translate="no" class="shader">			<ins>struct Interpolators {</ins>
				<ins>float4 position : SV_POSITION;</ins>
				<ins>float3 localPosition : TEXCOORD0;</ins>
			<ins>};</ins></pre>
						
						<p>Using this structure makes our code a lot tidier.</p>
						
						<pre translate="no" class="shader">			float4 _Tint;
			
			struct Interpolators {
				float4 position : SV_POSITION;
				float3 localPosition : TEXCOORD0;
			};

			<ins>Interpolators</ins> MyVertexProgram (float4 position : POSITION<ins>) {</ins>
				<ins>Interpolators i;</ins>
				<ins>i.</ins>localPosition = position.xyz;
				<ins>i.position =</ins> mul(UNITY_MATRIX_MVP, position);
				<ins>return i;</ins>
			}

			float4 MyFragmentProgram (<ins>Interpolators i</ins>) : SV_TARGET {
				return float4(<ins>i.</ins>localPosition, 1);
			}</pre>
						
					</section>
					
					<section>
						<h3>Tweaking Colors</h3>
						
						<p>Because negative colors get clamped to zero, our sphere ends up rather dark. As the default sphere has an object-space radius of &frac12;, the color channels end up somewhere between &minus;&frac12; and &frac12;. We want to move them into the 0&ndash;1 range, which we can do by adding &frac12; to all channels.</p>
						
						<pre translate="no" class="shader">				return float4(i.localPosition <mark>+ 0.5</mark>, 1);</pre>
						
						<figure>
							<img src="coloring-pixels/local-position-01.png" width="250" height="250">
							<figcaption>Local position recolored.</figcaption>
						</figure>
						
						<p>We can also apply our tint by factoring it into the result.</p>
						
						<pre translate="no" class="shader">				return float4(i.localPosition + 0.5, 1) <mark>* _Tint</mark>;</pre>
						
						<pre translate="no" class="shader">uniform 	vec4 _Tint;
in  vec3 vs_TEXCOORD0;
layout(location = 0) out vec4 SV_TARGET0;
vec4 t0;
void main()
{
    t0.xyz = vs_TEXCOORD0.xyz + vec3(0.5, 0.5, 0.5);
    t0.w = 1.0;
    SV_TARGET0 = t0 * _Tint;
    return;
}</pre>
						
						<pre translate="no" class="shader">ConstBuffer "$Globals" 128
Vector 96 [_Tint]
BindCB  "$Globals" 0
      ps_4_0
      dcl_constantbuffer cb0[7], immediateIndexed
      dcl_input_ps linear v0.xyz
      dcl_output o0.xyzw
      dcl_temps 1
   0: add r0.xyz, v0.xyzx, l(0.500000, 0.500000, 0.500000, 0.000000)
   1: mov r0.w, l(1.000000)
   2: mul o0.xyzw, r0.xyzw, cb0[6].xyzw
   3: ret</pre>
						
						<figure>
							<img src="coloring-pixels/local-position-tinted.png" width="250" height="250">
							<figcaption>Local position with a red tint, so only X remains.</figcaption>
						</figure>
					</section>
					
					<a href="coloring-pixels/coloring-pixels.unitypackage" download rel="nofollow">unitypackage</a>
				</section>
				
				<section>
					<h2>Texturing</h2>
					
					<p>If you want to add more apparent details and variety to a mesh, without adding more triangles, you can use a texture. You're then projecting an image onto the mesh triangles.</p>
					
					<p>Texture coordinates are used to control the projection. These are 2D coordinate pairs that cover the entire image in a one-unit square area, regardless of the actual aspect ratio of the texture. The horizontal coordinate is known as U and the vertical coordinate as V. Hence, they're usually referred to as UV coordinates.</p>
					
					<figure>
						<img src="texturing/uv-rectangle.png" width="320" height="270">
						<figcaption>UV coordinates covering an image.</figcaption>
					</figure>
					
					<p>The U coordinate increases from left to right. So it is 0 at the left side of the image, &frac12; halfway, and 1 at the right side. The V coordinate works the same way, vertically. It increases from bottom to top, except for Direct3D, where it goes from top to bottom. You almost never need to worry about this difference.</p>
					
					<section>
						<h3>Using UV Coordinates</h3>
						
						<p>Unity's default meshes have UV coordinates suitable for texture mapping. The vertex program can access them via a parameter with the <code class="shader">TEXCOORD0</code> semantic.</p>
						
						<pre translate="no" class="shader">			Interpolators MyVertexProgram (
				float4 position : POSITION<ins>,</ins>
				<ins>float2 uv : TEXCOORD0</ins>
			) {
				Interpolators i;
				i.localPosition = position.xyz;
				i.position = mul(UNITY_MATRIX_MVP, position);
				return i;
			}</pre>
						
						<p>Our vertex program now uses more than one input parameter. Once again, we can use a struct to group them.</p>
						
						<pre translate="no" class="shader">			<ins>struct VertexData {</ins>
				<ins>float4 position : POSITION;</ins>
				<ins>float2 uv : TEXCOORD0;</ins>
			<ins>};</ins>
			
			Interpolators MyVertexProgram (<ins>VertexData v</ins>) {
				Interpolators i;
				i.localPosition = <ins>v.</ins>position.xyz;
				i.position = mul(UNITY_MATRIX_MVP, <ins>v.</ins>position);
				return i;
			}
			</pre>
						
						<p>Let's just pass the UV coordinates straight to the fragment program, replacing the local position.</p>
						
						<pre translate="no" class="shader">			struct Interpolators {
				float4 position : SV_POSITION;
				<ins>float2 uv : TEXCOORD0;</ins>
<del>//				float3 localPosition : TEXCOORD0;</del>
			};

			Interpolators MyVertexProgram (VertedData v) {
				Interpolators i;
<del>//				i.localPosition = v.position.xyz;</del>
				i.position = mul(UNITY_MATRIX_MVP, v.position);
				<ins>i.uv = v.uv;</ins>
				return i;
			}</pre>
						
						<p>We can make the UV coordinates visible, just like the local position, by interpreting them as color channels. For example, U becomes red, V becomes green, while blue is always 1.</p>

						<pre translate="no" class="shader">			float4 MyFragmentProgram (Interpolators i) : SV_TARGET {
				return <ins>float4(i.uv, 1, 1)</ins>;
			}</pre>

						<p>You'll see that the compiled vertex programs now copy the UV coordinates from the vertex data to the interpolator output.</p>

						<pre translate="no" class="shader">in  vec4 in_POSITION0;
in  vec2 in_TEXCOORD0;
out vec2 vs_TEXCOORD0;
vec4 t0;
void main()
{
    t0 = in_POSITION0.yyyy * glstate_matrix_mvp[1];
    t0 = glstate_matrix_mvp[0] * in_POSITION0.xxxx + t0;
    t0 = glstate_matrix_mvp[2] * in_POSITION0.zzzz + t0;
    gl_Position = glstate_matrix_mvp[3] * in_POSITION0.wwww + t0;
    vs_TEXCOORD0.xy = in_TEXCOORD0.xy;
    return;
}</pre>

						<pre translate="no" class="shader">Bind "vertex" Vertex
Bind "texcoord" TexCoord0
ConstBuffer "UnityPerDraw" 352
Matrix 0 [glstate_matrix_mvp]
BindCB  "UnityPerDraw" 0
      vs_4_0
      dcl_constantbuffer cb0[4], immediateIndexed
      dcl_input v0.xyzw
      dcl_input v1.xy
      dcl_output_siv o0.xyzw, position
      dcl_output o1.xy
      dcl_temps 1
   0: mul r0.xyzw, v0.yyyy, cb0[1].xyzw
   1: mad r0.xyzw, cb0[0].xyzw, v0.xxxx, r0.xyzw
   2: mad r0.xyzw, cb0[2].xyzw, v0.zzzz, r0.xyzw
   3: mad o0.xyzw, cb0[3].xyzw, v0.wwww, r0.xyzw
   4: mov o1.xy, v1.xyxx
   5: ret</pre>
						
						<p>Unity wraps the UV coordinates around its sphere, collapsing the top and bottom of the image at the poles. You'll see a seam run from the north to the south pole where the left and right sides of the image are joined. So along that seam you'll have U coordinate values of both 0 and 1. This is done by having duplicate vertices along the seam, being identical except for their U coordinates.</p>
						
						<figure>
							<img alt="frontal" src="texturing/uv-sphere.png" width="250" height="250">
							<img alt="from above" src="texturing/uv-sphere-above.png" width="250" height="250">
							<figcaption>UV as colors, head-on and from above.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Adding a Texture</h3>
						
						<p>To add a texture, you need to import an image file. Here is the one I'll use for testing purposes.</p>
						
						<figure>
							<img src="texturing/texture.png" width="256" height="256">
							<figcaption>Texture for testing.</figcaption>
						</figure>
						
						<p>You can add an image to your project by dragging it onto the project view. You could also do it via the <i>Asset / Import New Asset...</i> menu item. The image will be imported as a 2D texture with the default settings, which are fine.</p>
						
						<figure>
							<img alt="settings" src="texturing/texture-inspector.png" width="320" height="268">
							<img alt="preview" src="texturing/texture-preview.png" width="320" height="226">
							<figcaption>Imported texture with default settings.</figcaption>
						</figure>
						
						<p>To use the texture, we have to add another shader property. The type of a regular texture property is <i>2D</i>, as there are also other types of textures. The default value is a string referring one of Unity's default textures, either <i>white</i>, <i>black</i>, or <i>gray</i>.</p>
						
						<p>The convention is to name the main texture <code class="shader">_MainTex</code>, so we'll use that. This also enables you to use the convenient <code>Material.mainTexture</code> property to access it via a script, in case you need to.</p>
						
						<pre translate="no" class="shader">	Properties {
		_Tint ("Tint", Color) = (1, 1, 1, 1)
		<mark>_MainTex ("Texture", 2D) = "white" {}</mark>
	}</pre>
						
						<aside>
							<h3>What are the curly brackets for?</h3>
							<div>
								<p>There used to be texture settings for old fixed-function shaders, but they are no longer used. These settings were put inside those brackets.</p>
								
								<p>Even through they're now useless, the shader compilers still expect them and can produce errors if you omit them. Specifically, it will go wrong if you put a non-texture parameter after a texture parameter that lacks <code class="shader">{}</code>. Maybe it will be safe to omit them in a future version of Unity.</p>
							</div>
						</aside>
						
						<p>Now we can assign the texture to our material, either by dragging or via the <i>Select</i> button.</p>
						
						<figure>
							<img src="texturing/material-inspector.png" width="320" height="92">
							<figcaption>Texture assigned to our material.</figcaption>
						</figure>
						
						<p>We can access the texture in our shader by using a variable with type <code class="shader">sampler2D</code>.</p>
						
						<pre translate="no" class="shader">			float4 _Tint;
			<mark>sampler2D _MainTex;</mark></pre>
						
						<p>Sampling the texture with the UV coordinates is done in the fragment program, by using the <code class="shader">tex2D</code> function.</p>
						
						<pre translate="no" class="shader">			float4 MyFragmentProgram (Interpolators i) : SV_TARGET {
				return <ins>tex2D(_MainTex, i.uv)</ins>;
			}</pre>
						
						<pre translate="no" class="shader">uniform  sampler2D _MainTex;
in  vec2 vs_TEXCOORD0;
layout(location = 0) out vec4 SV_TARGET0;
void main()
{
    SV_TARGET0 = texture(_MainTex, vs_TEXCOORD0.xy);
    return;
}
</pre>
						
						<pre translate="no" class="shader">SetTexture 0 [_MainTex] 2D 0
      ps_4_0
      dcl_sampler s0, mode_default
      dcl_resource_texture2d (float,float,float,float) t0
      dcl_input_ps linear v0.xy
      dcl_output o0.xyzw
   0: sample o0.xyzw, v0.xyxx, t0.xyzw, s0
   1: ret</pre>
						
						<figure>
							<img alt="frontal" src="texturing/textured-sphere.png" width="250" height="250">
							<img alt="from above" src="texturing/textured-sphere-above.png" width="250" height="250">
							<figcaption>Textured sphere.</figcaption>
						</figure>
						
						<p>Now that the texture is sampled for each fragment, it will appear projected on the sphere. It is wrapped around it, as expected, but it will appear quite wobbly near the poles. Why is this so?</p>
						
						<p>The texture distortion happens because interpolation is linear across triangles. Unity's sphere only has a few triangles near the poles, where the UV coordinates are distorted most. So UV coordinates change nonlinearly from vertex to vertex, but in between vertices their change is linear. As a result, straight lines in the texture suddenly change direction at triangle boundaries.</p>
						
						<figure>
							<img src="texturing/textured-sphere-wireframe.png" width="250" height="250">
							<figcaption>Linear interpolation across triangles.</figcaption>
						</figure>
						
						<p>Different meshes have different UV coordinates, which produces different mappings. Unity's default sphere uses longitude-latitude texture mapping, while the mesh is a low-resolution cube sphere. It's sufficient for testing, but you're better off using a custom sphere mesh for better results.</p>
						
						<figure>
							<div class="vid" style="width: 220px; height:210px;"><iframe src='https://gfycat.com/ifr/BelatedCompleteCooter'></iframe></div>
							<figcaption>Different texture preview shapes.</figcaption>
						</figure>
						
						<p>Finally, we can factor in the tint to adjust the textured appearance of the sphere.</p>
						
						<pre translate="no" class="shader">				return tex2D(_MainTex, i.uv) <mark>* _Tint</mark>;</pre>
						
						<figure>
							<img src="texturing/textured-tinted.png" width="250" height="250">
							<figcaption>Textured with yellow tint.</figcaption>
						</figure>
						
					</section>
					
					<section>
						<h3>Tiling and Offset</h3>
						
						<p>After we added a texture property to our shader, the material inspector didn't just add a texture field. It also added tiling and offset controls. However, changing these 2D vectors currently has no effect.</p>
						
						<p>This extra texture data is stored in the material and can also be accessed by the shader. You do so via a variable that has the same name as the associated material, plus the <i>_ST</i> suffix. The type of this variable must be <code class="shader">float4</code>.</p>
						
						<aside>
							<h3>What does _ST mean?</h3>
							<div>
								<p>The <i>_ST</i> suffix stands for <i>Scale and Translation</i>, or something like that. Why isn't <i>_TO</i> used, referring to <i>Tiling and Offset</i>? Because Unity has always used <i>_ST</i>, and backwards compatibility mandates it stays that way, even if the terminology might have changed.</p>
							</div>
						</aside>
						
						<pre translate="no" class="shader">			sampler2D _MainTex;
			<mark>float4 _MainTex_ST;</mark></pre>
						
						<p>The tiling vector is used to scale the texture, so it is (1, 1) by default. It is stored in the XY portion of the variable. To use it, simply multiply it with the UV coordinates. This can be done either in the vertex shader or the fragment shader. It makes sense to do it in the vertex shader, so we perform the multiplications only for each vertex instead of for every fragment.</p>
						
						<pre translate="no" class="shader">			Interpolators MyVertexProgram (VertexData v) {
				Interpolators i;
				i.position = mul(UNITY_MATRIX_MVP, v.position);
				i.uv = v.uv <ins>* _MainTex_ST.xy</ins>;
				return i;
			}</pre>
						
						<figure>
							<div class="vid" style="width: 210px; height:200px;"><iframe src='https://gfycat.com/ifr/HorribleEnviousGrunion'></iframe></div>
							<figcaption>Tiling.</figcaption>
						</figure>
						
						<p>The offset portion moves the texture around and is stored in the ZW portion of the variable. It is added to the UV after scaling.</p>
						
						<pre translate="no" class="shader">				i.uv = v.uv * _MainTex_ST.xy <mark>+ _MainTex_ST.zw</mark>;</pre>
						
						<figure>
							<div class="vid" style="width: 210px; height:200px;"><iframe src='https://gfycat.com/ifr/IlliterateEarlyBarnswallow'></iframe></div>
							<figcaption>Offset.</figcaption>
						</figure>
						
						<p><i>UnityCG.cginc</i> contains a handy macro that simplifies this boilerplate for us. We can use it as a convenient shorthand.</p>
						
						<pre translate="no" class="shader">				i.uv = <mark>TRANSFORM_TEX(v.uv, _MainTex)</mark>;</pre>
						
						<aside>
							<h3>What's a macro?</h3>
							<div>
								<p>A macro is like a function, except that it is evaluated during the pre-processing step, before the code is compiled for real. This allows textual manipulation of code, like appending <i>_ST</i> to a variable name. The <code class="shader">TRANSFORM_TEX</code> macro uses this trick. In case you're curious, here's its definition.</p>
								
								<pre translate="no" class="shader">#define TRANSFORM_TEX(tex,name) (tex.xy * name##_ST.xy + name##_ST.zw)</pre>
								
								<p>Macros enable all kinds of neat tricks, but can also lead to hard to understand code and very nasty bugs. That's why there are no macros for C#.</p>
								
								<p>We'll create our own macros in a future tutorial.</p>
							</div>
						</aside>
					</section>
					
					<a href="texturing/texturing.unitypackage" download rel="nofollow">unitypackage</a>
				</section>
				
				<section>
					<h2>Texture Settings</h2>
					
					<p>So far we've used the default texture import settings. Let's have a look at a few of the options, to see what they do.</p>
					
					<figure>
						<img src="texturing/texture-inspector.png" width="320" height="268">
						<figcaption>Default import settings.</figcaption>
					</figure>
					
					<p>The <i>Wrap Mode</i> dictates what happens when sampling with UV coordinates that lie outside of the 0&ndash;1 range. When the wrap mode is set to clamped, the UV are constrained to remain inside the 0&ndash;1 range. This means that the pixels beyond the edge are the same as those that lie on the edge. When the wrap mode is set to repeat, the UV wrap around. This means that the pixels beyond the edge are the same as those on the opposite side of the texture. The default mode is to repeat the texture, which causes it to tile.</p>
					
					<p>If you don't have a tiling texture, you'd want to clamp the UV coordinates instead. This prevents the texture from repeating, instead the texture boundary will be replicated, causing it to look stretched.</p>
					
					<figure>
						<img src="texture-settings/clamp.png" width="190" height="180">
						<figcaption>Tiling at (2, 2) while clamped.</figcaption>
					</figure>
					
					<aside>
						<h3>Does the wrap mode matter when staying within the 0&ndash;1 range?</h3>
						<div>
							<p>It matters when you have UV coordinates that touch the 0 and 1 boundaries. When using bilinear or trilinear filtering, adjacent pixels are interpolated while sampling the texture. This is fine for pixels in the middle of the texture. But what are the neighboring pixels of those that lie on the edge? The answer depends on the wrap mode.</p>
							
							<p>When clamped, pixels on the edge are blended with themselves. This produces a tiny region where the pixels don't blend, which is not noticeable.</p>
							
							<p>When repeated, pixels on the edge are blended with the other side of the texture. If the sides are not similar, you'll notice a bit of the opposite side bleeding through the edge. Zoom in on a corner of a quad with the test texture on it to see the difference.</p>
							
							<figure>
								<img src="texture-settings/texture-bleeding.png" width="100" height="100">
								<figcaption>Tiling on the edge.</figcaption>
							</figure>
						</div>
					</aside>
					
					<section>
						<h3>Mipmaps and Filtering</h3>
						
						<p>What happens when the pixels of a texture &ndash; texels &ndash; don't exactly match the pixels they are projected onto? There is a mismatch, which has to be resolved somehow. How this is done is controlled by the <i>Filter Mode</i>.</p>
						
						<p>The most straightforward filtering mode is <i>Point (no filter)</i>. This means that when a texture is sampled at some UV coordinates, the nearest texel is used. This will give the texture a blocky appearance, unless texels map exactly to display pixels. So it is typically used for pixel-perfect rendering, or when a blocky style is desired.</p>
						
						<p>The default is to use bilinear filtering. When a texture is sampled somewhere in between two texels, those two texels are interpolated. As textures are 2D, this happens both along the U and the V axis. Hence bilinear filtering, not just linear filtering.</p>
						
						<p>This approach works when the texel density is less than the display pixel density, so when you're zooming in to the texture. The result will look blurry. It doesn't work in the opposite case, when you're zooming out of the texture. Adjacent display pixels will end up with samples that are more than one texel apart. This means that parts of the texture will be skipped, which will cause harsh transitions, as if the image was sharpened.</p>
						
						<p>The solution to this problem is to use a smaller texture whenever the texel density becomes too high. The smaller the texture appears on the display, the smaller a version of it should be used. These smaller versions are known as mipmaps and are automatically generated for you. Each successive mipmap has half the width and height of the previous level. So when the original texture size is 512x512, the mip maps are 256x256, 128x128, 64x64, 32x32, 16x16, 8x8, 4x4, and 2x2.</p>
						
						<aside>
							<h3>What does mipmap mean?</h3>
							<div>
								<p>The word <i>mipmap</i> is a contraction of <i>MIP map</i>. The letters <i>MIP</i> stand for the latin phrase <i>multum in parvo</i>, which translates to <i>multitude in a small space</i>. It was coined by Lance Williams when he first described the mipmapping technique.</p>
							</div>
						</aside>

						<figure>
							<div class="vid" style="width: 320px; height:254px;"><iframe src='https://gfycat.com/ifr/MemorableRipeImago'></iframe></div>
							<figcaption>Mipmap levels.</figcaption>
						</figure>
						
						<p>You can disable mipmaps if you like. First, you have the set the <i>Texture Type</i> type to <i>Advanced</i>. They you can disable the mipmaps and apply the change. A good way to see the difference is to use a flat object like a quad and look at it from an angle.</p>
						
						<figure>
							<img alt="with" src="texture-settings/bilinear-mips.png" width="370" height="170">
							<img alt="without" src="texture-settings/no-mips.png" width="370" height="170">
							<figcaption>With and without mipmaps.</figcaption>
						</figure>
						
						<p>So which mipmap level is used where, and how different do they look? We can make the transitions visible by enabling <i>Fadeout Mip Maps</i> in the advanced texture settings. When enabled, a <i>Fade Range</i> slider will show up in the inspector. It defines a mipmap range across which the mipmaps will transition to solid gray. By making this transition a single step, you will get a sharp transition to gray. The further you move the one-step range to the right, the later the transition will occur.</p>
						
						<figure>
							<img src="texture-settings/advanced-inspector.png" width="320" height="174">
							<figcaption>Advanced settings for mipmaps.</figcaption>
						</figure>
						
						<aside>
							<h3>What is the use of fading to gray?</h3>
							<div>
								<p>It is used for detail textures, which we'll cover in a future tutorial.</p>
								
								<p>You might think that it could be used for a fog effect, but this is not true. Which mipmap gets used depends on the texel vs. display pixel density, not 3D distance.</p>
							</div>
						</aside>
						
						<p>To get a good view of this effect, set the texture's <i>Aniso Level</i> to 0 for now.</p>
						
						<figure>
							<img alt="mip 3" src="texture-settings/mip-3.png" width="370" height="170">
							<img alt="mip 4" src="texture-settings/mip-4.png" width="370" height="170">
							<img alt="mip 5" src="texture-settings/mip-5.png" width="370" height="170">
							<figcaption>Successive mipmap levels.</figcaption>
						</figure>
						
						<p>Once you know where the various mipmaps levels are, you should be able to see the sudden change in texture quality between them. As the texture projection gets smaller, the texel density increases, which makes it look sharper. Until suddenly the next mipmap level kicks in, and it is becomes blurry again.</p>
						
						<p>So without mipmaps you go from blurry to sharp, to too sharp. With mipmaps you go from blurry to sharp, to suddenly blurry again, to sharp, to suddenly blurry again, and so on.</p>
						
						<p>Those blurry-sharp bands are characteristic for bilinear filtering. You can get rid of them by switching the filter mode to <i>Trilinear</i>. This works the same as bilinear filtering, but it also interpolates between adjacent mipmap levels. Hence trilinear. This makes sampling more expensive, but it smoothes the transitions between mipmap levels.</p>
						
						<figure>
							<img src="texture-settings/trilinear-mip.png" width="370" height="170">
							<figcaption>Trilinear filtering between normal and gray mipmaps.</figcaption>
						</figure>
						
						<p>Another useful technique is anisotropic filtering. You might have noticed that when you set it to 0, the texture became blurrier. This has to do with the selection of the mipmap level.</p>
						
						<aside>
							<h3>What does anisotropic mean?</h3>
							<div>
								<p>Roughly speaking, when something appears similar in different directions, then it is isotropic. For example, a featureless cube. When this is not the case, it is anisotropic. For example, a block of wood, because its grain goes in one direction and not the other.</p>
							</div>
						</aside>
						
						<p>When a texture gets projected at an angle, due to perspective, you often end up with one of its dimension being distorted much more than the other. A good example is a textured ground plane. At a distance, the forward-backward dimension of the texture will appear much smaller that the left-right dimension.</p>
						
						<p>Which mipmap level get selected is based on the worst dimension. If the difference is large, then you will get a result that is very blurry in one dimension. Anisotropic filtering mitigates this by decoupling the dimensions. Besides uniformly scaling down the texture, it also provides versions that are scaled different amounts in either dimension. So you don't just have a mipmap for 256x256, but also for 256x128, 256x64, and so on.</p>
						
						<figure>
							<img alt="without" src="texture-settings/without-aniso.png" width="410" height="70">
							<img alt="with" src="texture-settings/with-aniso.png" width="410" height="70">
							<figcaption>Without and with anisotropic filtering.</figcaption>
						</figure>
						
						<p>Note that those extra mipmaps aren't pre-generated like the regular mipmaps. Instead, they are simulated by performing extra texture samples. So they don't require more space, but are more expensive to sample.</p>
						
						<figure>
							<img src="texture-settings/aniso-mip.png" width="370" height="170">
							<figcaption>Anisotropic bilinear filtering, transitioning to gray.</figcaption>
						</figure>
						
						<p>How deep the anisotropic filtering goes is controlled by <i>Aniso Level</i>. At 0, it is disabled. At 1, it becomes enabled and provides the minimum effect. At 16, it is at its maximum. However, these settings are influence by the project's quality settings.</p>
						
						<p>You can access the quality settings via <i>Edit / Project Settings / Quality</i>. You will find a <i>Anisotropic Textures</i> setting in the <i>Rendering</i> section.</p>
						
						<figure>
							<img src="texture-settings/rendering-quality.png" width="294" height="110">
							<figcaption>Rendering quality settings.</figcaption>
						</figure>
						
						<p>When anisotropic textures are disabled, no anisotropic filtering will happen, regardless of a texture's settings. When it is set to <i>Per Texture</i>, it is fully controlled by each individual texture. It can also be set to <i>Forced On</i>, which will act as if each texture's <i>Aniso Level</i> is set to at least 9. However, a texture with an <i>Aniso Level</i> set to 0 still won't use anisotropic filtering.</p>
						
						<p>The next tutorial is <a href="../../../tutorials/rendering/part-3/">Combining Textures</a>.</p>
					</section>
					
					<a href="texture-settings/texture-settings.unitypackage" download rel="nofollow">unitypackage</a>
					<a href="Rendering-2.pdf" download rel="nofollow">PDF</a>
				</section>
				
			</article>
		</main>

		<footer>
			<p>Enjoying the <a href="../../../tutorials/">tutorials</a>? Are they useful? Want more?</p>
			<p><b><a href="https://www.patreon.com/catlikecoding">Please support me on Patreon!</a></b></p>
			<p><a href="https://www.patreon.com/catlikecoding"><img src="https://catlikecoding.com/unity/tutorials/become-a-patron.png" alt="Become my patron!" width="217" height="51"></a></p>
			<p><b><a href="../../../tutorials/donating.html">Or make a direct donation</a>!</b></p>
			<p>made by <a href="https://catlikecoding.com/jasper-flick/" rel="author">Jasper Flick</a></p>
		</footer>
		
		<script src="https://catlikecoding.com/jquery2.js"></script>
		<script src="../../tutorials.js"></script>
	</body>
</html>