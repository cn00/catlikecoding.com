<!DOCTYPE html>
<html lang="en">
	<head prefix="og: http://ogp.me/ns#">
		<meta charset="utf-8">
		<meta property="og:url" content="https://catlikecoding.com/unity/tutorials/procedural-meshes/creating-a-mesh/">
		<meta property="og:type" content="article">
		<meta property="og:image:width" content="1024">
		<meta property="og:image:height" content="512">
		<meta property="og:image" content="https://catlikecoding.com/unity/tutorials/procedural-meshes/creating-a-mesh/tutorial-image.jpg">
		<meta property="og:title" content="Creating a Mesh">
		<meta property="og:description" content="A Unity C# Procedural Meshes tutorial about creating a mesh via code, with the simple and advanced Mesh API.">
		<meta property="twitter:card" content="summary_large_image">
		<meta property="twitter:creator" content="@catlikecoding">
		<meta name="viewport" content="width=768">
		<title>Creating a Mesh</title>
		<link href="../../tutorials.css" rel="stylesheet">
		<link rel="manifest" href="../../../../site.webmanifest">
		<link rel="mask-icon" href="../../../../safari-pinned-tab.svg" color="#aa0000">
		<script type="application/ld+json">{
			"@context": "http://schema.org",
			"@type": "WebPage",
			"mainEntity": {
				"@type": "TechArticle",
				"@id": "https://catlikecoding.com/unity/tutorials/procedural-meshes/creating-a-mesh/#article",
				"headline": "Creating a Mesh",
				"alternativeHeadline": "Vertices and Triangles",
				"datePublished": "2021-10-30",
				"author": { "@type": "Person", "name": "Jasper Flick", "@id": "https://catlikecoding.com/jasper-flick/#person" },
				"publisher": { "@type": "Organization", "name": "Catlike Coding", "@id": "https://catlikecoding.com/#organization" },
				"description": "A Unity C# Procedural Meshes tutorial about creating a mesh via code, with the simple and advanced Mesh API.",
				"image": "https://catlikecoding.com/unity/tutorials/procedural-meshes/creating-a-mesh/tutorial-image.jpg",
				"dependencies": "Unity 2020.3.18f1",
				"proficiencyLevel": "Beginner"
			},
			"breadcrumb": {
				"@type": "BreadcrumbList",
				"itemListElement": [
					{ "@type": "ListItem", "position": 1, "item": { "@id": "https://catlikecoding.com/unity/", "name": "Unity" }},
					{ "@type": "ListItem", "position": 2, "item": { "@id": "https://catlikecoding.com/unity/tutorials/", "name": "Tutorials" }},
					{ "@type": "ListItem", "position": 3, "item": { "@id": "https://catlikecoding.com/unity/tutorials/procedural-meshes/", "name": "Procedural Meshes" }}
				]
			}
		}</script>
		<script>
			var customTypes = {
				AdvancedMultiStreamProceduralMesh: 1,
				AdvancedSingleStreamProceduralMesh: 1,
				SimpleProceduralMesh: 1,
				Vertex: 1
			};
			
			var hasMath = false;
		</script>
	</head>
	<body>
		<header>
			<a href="../../../../index.html"><img src="../../../../catlike-coding-logo.svg" alt="Catlike Coding" width="45" height="45"></a>
			<nav>
				<ol>
					<li><a href="../../../../index.html">Catlike Coding</a></li>
					<li><a href="../../../index.html">Unity</a></li>
					<li><a href="../../../tutorials">Tutorials</a></li>
					<li><a href="../index.html">Procedural Meshes</a></li>
				</ol>
			</nav>
		</header>
		
		<main>
			<article>
				<header>
					<h1>Creating a Mesh</h1>
					<p>Vertices and Triangles</p>
					<ul>
						<li>Generate a triangle and a quad via code.</li>
						<li>Define vertex positions, normals, tangents, and texture coordinates.</li>
						<li>Use both the simple and advanced Mesh API.</li>
						<li>Store vertex data in multiple streams or a in single stream.</li>
					</ul>
				</header>
				
				<p>This is the first tutorial in a series about <a href="../index.html">procedural meshes</a>. It comes after the <a href="../../pseudorandom-noise/index.html">Pseudorandom Noise</a> series. It introduces multiple ways to create a mesh via code, via the simple and advanced Mesh API.</p>
				
				<p>This tutorial is made with Unity 2020.3.18f1.</p>
				
				<figure>
					<img src="tutorial-image.jpg" width="512" height="256">
					<figcaption>A custom quad made with two triangles.</figcaption>
				</figure>
				
				<section>
					<h2>Constructing a Triangle</h2>
					
					<p>The typical way to show something is to render a mesh, with a specific material. Unity has a few built-in meshes of simple shapes, including a cube and a sphere. Other meshes can be bought, downloaded, or made yourself and then imported into a project. But it is also possible to create a mesh on-demand at runtime via code, which is what this series is about. Such meshes are known as procedural, because they're generated via code using specific algorithms, instead of being modeled by hand.</p>
					
					<p>Start with a new project as described in the <a href="../../basics/index.html">Basics</a> series. We'll use types from <em translate="no">Mathematics</em>, so import it via the package manager. Although we won't need it in this tutorial yet, I also already include the <em translate="no">Burst</em> package as well. Finally, I'll use URP so import <i>Universal RP</i> and create an asset for it and configure Unity to use it.</p>
					
					<section>
						<h3>Simple Procedural Mesh Component</h3>
						
						<p>There are two different ways to create a mesh procedurally: the simple and the advanced way. Each has its own API.</p>
						
						<aside>
							<h3>What does API mean?</h3>
							<div>
								<p>API stands for Application Programming Interface. In this context it refers to a collection of C# types and their members that together allow us to generate a mesh.</p>
							</div>
						</aside>
						
						<p>We'll use both approaches to generate the same mesh in turn, beginning with the simple Mesh API. This approach has always been part of Unity. Create a component type for it, naming it <code>SimpleProceduralMesh</code>.</p>
						
						<pre translate="no"><ins>using UnityEngine;</ins>

<ins>public class SimpleProceduralMesh : MonoBehaviour { }</ins></pre>
						
						<p>We'll use this custom component type to generate our mesh when we enter play mode. To draw the mesh we need a game object that also has a <code>MeshFilter</code> and a <code>MeshRenderer</code> component. We can enforce that these components are added to the same game object that we add our own component to, by giving it the <code>RequireComponent</code> attribute with both component types as arguments. To indicate that we refer to the types themselves we have to pass each to the <code>typeof</code> operator, as if it were a method invocation.</p>
						
						<pre translate="no"><ins>[RequireComponent(typeof(MeshFilter), typeof(MeshRenderer))]</ins>
public class SimpleProceduralMesh : MonoBehaviour { }</pre>
						
						<p>Create a new empty game object and attach our <code>SimpleProceduralMesh</code> to it. This will also automatically give it a <code>MeshFilter</code> and  a <code>MeshRenderer</code> component. Then create a new default URP material and assign it to our game object, because the default <code>MeshRenderer</code> component doesn't have a material set. The <code>MeshFilter</code> component also doesn't have a mesh yet, but that is correct, because we'll give it one while in play mode.</p>
						
						<figure>
							<img src="constructing-a-triangle/simple-producedural-mesh-game-object.png" width="320" height="330">
							<figcaption>Game object for simple procedural mesh.</figcaption>
						</figure>
						
						<p>We generate the mesh in the <code>OnEnable</code> method. This is done by creating a new <code>Mesh</code> object. Also name it <em translate="no">Procedural Mesh</em> by settings its <code>name</code> property.</p>
						
						<pre translate="no">public class SimpleProceduralMesh : MonoBehaviour {
	
	<ins>void OnEnable () {</ins>
		<ins>var mesh = new Mesh {</ins>
			<ins>name = "Procedural Mesh"</ins>
		<ins>};</ins>
	<ins>}</ins>
}</pre>
						
						<p>Then we assign it to the <code>mesh</code> property of our <code>MeshFilter</code> component, which we can access by invoking the generic <code>GetComponent</code> method on our component, specifically for <code>MeshFilter</code>.</p>
						
						<pre translate="no">		var mesh = new Mesh {
			name = "Procedural Mesh"
		};
		
		<ins>GetComponent&lt;MeshFilter>().mesh = mesh;</ins></pre>
						
						<figure>
							<img src="constructing-a-triangle/procedural-mesh-in-play-mode.png" width="320" height="54">
							<figcaption>Procedural mesh appears in play mode.</figcaption>
						</figure>
						
						<p>When we enter play mode now a reference to our mesh will appear in the inspector of <code>MeshFilter</code>, even though nothing gets drawn. We can access the inspector of our mesh via double-clicking on its reference, or via the <em translate="no">Properties...</em> option of the context menu that we can open for it.</p>
						
						<figure>
							<img src="constructing-a-triangle/procedural-mesh-inspector.png" width="320" height="230">
							<figcaption>Inspector window of procedural mesh.</figcaption>
						</figure>
						
						<p>This tells us the current state of our mesh. It doesn't have any vertices nor indices, it has one submesh with zero triangles, and its bounds are set to zero. So there is nothing to draw yet.</p>
						
						<aside>
							<h3>What does <em translate="no">Read/Write Enabled</em> mean?</h3>
							<div>
								<p>It indicates whether it's possible to access and modify the mesh data via C# code. This is the case by default for meshes generated via code, but it can be disabled in various ways, after you're done generating the mesh. We won't concern ourselves with this, but it can be a benefit because it allows Unity to release some CPU memory.</p>
							</div>
						</aside>
					</section>
					
					<section>
						<h3>Adding Vertices</h3>
						
						<p>A mesh contains triangles, which are the simplest surfaces that can be described in 3D. Each triangle has three corners. These are the vertices of the mesh, which we will define first.</p>
						
						<p>At its most simplest a vertex is nothing more than a position in 3D space, described with a <code>Vector3</code> value. We'll create vertices for a single triangle, using the default zero, right, and up vectors. This defines an isosceles right triangle that lies on the XY plane, with its 90&deg; corner at the origin and the other corners a single unit away in a different dimension each.</p>
						
						<figure>
							<img src="constructing-a-triangle/triangle.png" width="230" height="210">
							<figcaption>An isosceles right triangle on the XY plane.</figcaption>
						</figure>
						
						<p>There are multiple ways in which we could set the vertices via the simple Mesh API, but the simplest is to create a <code>Vector3</code> array with the desired vertices and assign it to the <code>vertices</code> property of our mesh.</p>
						
						<pre translate="no">		var mesh = new Mesh {
			name = "Procedural Mesh"
		};

		<ins>mesh.vertices = new Vector3[] {</ins>
			<ins>Vector3.zero, Vector3.right, Vector3.up</ins>
		<ins>};</ins>

		GetComponent&lt;MeshFilter>().mesh = mesh;</pre>
						
						<aside>
							<h3>Do we have to fill the mesh before assigning it to the <code>MeshFilter</code>?</h3>
							<div>
								<p>This isn't mandatory, but it makes the most sense. Also, adjusting meshes that are already in use triggers notifications whenever that mesh is changed later, so <code>MeshRenderer</code> components can adjust to the changes. So you typically finish generating the mesh before assigning it to anything.</p>
							</div>
						</aside>
						
						<figure>
							<img src="constructing-a-triangle/three-vertices.png" width="320" height="178">
							<figcaption>Three vertices.</figcaption>
						</figure>
						
						<p>Entering play mode now and then inspecting our mesh shows us that it has three vertices. Each vertex defines a position, which consists of three 32-bit <code>float</code> values, so that's three times four bytes, thus twelve bytes per vertex, for a total of 36 bytes.</p>
						
						<p>There are no triangles yet, but the mesh has already automatically derived its bounds from the vertices that we gave it.</p>
					</section>
					
					<section>
						<h3>Defining the Triangle</h3>
						
						<p>Vertices alone are not enough. We also have to describe how the triangles of the mesh are to be drawn, even for a trivial mesh that only has a single triangle. We'll do this by assigning an <code>int</code> array with triangle indices to the <code>triangles</code> property, after setting the vertices. These indices refer to the indices of the vertex positions. The most straightforward thing to do would be to list the three indices in order: 0, 1, and 2.</p>
						
						<pre translate="no">		mesh.vertices = new Vector3[] {
			Vector3.zero, Vector3.right, Vector3.up
		};

		<ins>mesh.triangles = new int[] {</ins>
			<ins>0, 1, 2</ins>
		<ins>};</ins></pre>
						
						<figure>
							<img src="constructing-a-triangle/one-triangle-three-indices.png" width="320" height="110">
							<figcaption>One triangle defined with three indices.</figcaption>
						</figure>
						
						<p>Now our mesh tells us that is has a single triangle, defined by three vertex indices. The indices always start from index zero in the triangle array because there is only a single submesh. The indices take up only 6 bytes in total instead of 12 because they are stored as <em translate="no">UInt16</em>, which matches the 16-bit <code>ushort</code> C# type, which defines an unsigned integer with only two bytes instead of four.</p>
						
						
						<p>A triangle has also finally shown up in the game and scene windows, but it isn't visible from all view directions. By default triangles are only visible when looking at their front face, not their back face. Which side you're looking at is determined by the winding order of the vertices. If you trace the edges of the triangle, going through its vertices in the order indicated by the indices, you end up going either clockwise or counterclockwise, visually. The clockwise side is the front face, so that is the visible side.</p>
						
						<figure>
							<img src="constructing-a-triangle/winding-order.png" width="464" height="248">
							<figcaption>Counterclockwise and clockwise winding orders.</figcaption>
						</figure>
						
						<p>This means that we'll only see the triangle when looking in the negative Z direction. We can turn this around by swapping the order of the second and third vertex indices. Then we can see the triangle when looking in the positive Z direction.</p>
						
						<pre translate="no">		mesh.triangles = new int[] {
			0, <ins>2, 1</ins>
		};</pre>
						
						<figure>
							<img src="constructing-a-triangle/visible-triangle.png" width="190" height="190">
							<figcaption>Visible triangle when looking along the Z axis.</figcaption>
						</figure>
						
						<aside>
							<h3>Couldn't we swap the vertex positions instead of the indices?</h3>
							<div>
								<p>Yes, that is also possible.</p>
							</div>
						</aside>
					</section>
					
					<section>
						<h3>Normal Vectors</h3>
						
						<p>Currently the lighting of our triangle is incorrect. It behaves as if it gets lit from the opposite side. This happens because we haven't defined the normal vectors yet, which are used by shaders to calculate lighting.</p>
						
						<p>A normal vector is a unit-length vector that describes the local up direction if you were standing on a surface. So these vectors point straight away from the surface. Thus the normal vector for our triangle surface should be <code>Vector3.back</code>, pointing straight down the negative Z axis in the local space of our mesh. But if no normal vectors are provided Unity uses the forward vector by default, hence our triangle appears to be lit from the wrong side.</p>
						
						<p>Although it only really makes sense to define a normal vector for a surface, a mesh defines normal vectors per vertex. The final surface normal used for shading is found by interpolating the vertex normal across the surface of the triangle. By using different normal vectors the illusion of surface curvature can be added to flat triangles. This makes it possible to give meshes the appearance of being smooth while in reality they are faceted.</p>
						
						<p>We can add normal vectors to vertices by assigning a <code>Vector3</code> array to the <code>normals</code> property of our mesh, after setting the vertex positions. Unity checks whether the arrays have the same length and will fail and complain if we supply the wrong amount of normal vectors.</p>
						
						<pre translate="no">		mesh.vertices = new Vector3[] {
			Vector3.zero, Vector3.right, Vector3.up
		};

		<ins>mesh.normals = new Vector3[] {</ins>
			<ins>Vector3.back, Vector3.back, Vector3.back</ins>
		<ins>};</ins></pre>
						
						<figure>
							<img src="constructing-a-triangle/with-normal-vectors.png" width="190" height="190">
							<figcaption>With correct lighting.</figcaption>
						</figure>
						
						<p>As a consequence of adding normal vectors the size of our vertex data has doubled to 24 bytes per vertex and 72 bytes in total.</p>
						
						<figure>
							<img src="constructing-a-triangle/positions-and-normals.png" width="320" height="62">
							<figcaption>Positions and normals.</figcaption>
						</figure>
						
					</section>
					
					<section>
						<h3>Texturing</h3>
						
						<p>Surface details can be added to the mesh by applying a texture to it. The simplest texture is an image that is used to colorize the surface. In the case of URP this is known as a base map. <a href="constructing-a-triangle/base-map.png">Here</a> is such a texture, which makes it easy to see how the texture is applied to the triangle.</p>
						
						<figure>
							<img src="constructing-a-triangle/base-map.png" width="256" height="256">
							<figcaption>Base map.</figcaption>
						</figure>
						
						<p>Download the image, then import it into your project, either by placing it into the project's <em translate="no">Assets</em> folder or via dragging and dropping the file onto the project window. Then assign it to the <em translate="no">Base Map</em> property of the material.</p>
						
						<figure>
							<img src="constructing-a-triangle/base-map-material.png" width="320" height="40">
							<figcaption>Material with base map.</figcaption>
						</figure>
						
						<p>Initially this appears to make no difference, because we haven't defined any texture coordinates yet. They're zero by default, which means that the bottom left corner of the texture is used for the entire triangle, which is white.</p>
						
						<p>As the texture is a 2D image and the triangle surface is also 2D, the texture coordinates are <code>Vector2</code> values. They specify where to sample the texture at each vertex and they will be interpolated across the triangle surface. They're normalized coordinates, so the 0&ndash;1 inclusive range covers the entire texture, per dimension.</p>
						
						<p>In Unity the origin is at the bottom left corner of textures, so the most obvious texture mapping without distortion matches the vertex positions. We add them to the mesh by assigning an array to its <code>uv</code> property. Texture coordinates are often described as UV coordinates because they're two-dimensional coordinates in texture space, named U and V instead of X and Y.</p>
						
						<pre translate="no">		mesh.vertices = new Vector3[] {
			Vector3.zero, Vector3.right, Vector3.up
		};
		
		mesh.normals = new Vector3[] {
			Vector3.back, Vector3.back, Vector3.back
		};

		<ins>mesh.uv = new Vector2[] {</ins>
			<ins>Vector2.zero, Vector2.right, Vector2.up</ins>
		<ins>};</ins></pre>
						
						<figure>
							<img src="constructing-a-triangle/textured.png" width="190" height="190">
							<figcaption>Textured triangle.</figcaption>
						</figure>
						
						<aside>
							<h3>Can a mesh have multiple texture coordinate sets per vertex?</h3>
							<div>
								<p>Yes, Unity supports up to eight sets, accessible via separate properties. It's also possible to define 1D, 3D, or 4D coordinates, but via methods only, not via properties.</p>
							</div>
						</aside>
						
						<p>The mesh inspector will list the texture coordinates as <em translate="no">UV0</em> and shows that they add 8 bytes to the vertex size.</p>
						
						<figure>
							<img src="constructing-a-triangle/positions-normals-uvs.png" width="320" height="80">
							<figcaption>With texture coordinates.</figcaption>
						</figure>
						
						<p>You could also map the texture in a different way, for example by using <code>Vector2.one</code> for the third vertex. This will distort the image, shearing it.</p>
						
						<figure>
							<img src="constructing-a-triangle/textured-alternative.png" width="190" height="190">
							<figcaption>Alternative texture mapping.</figcaption>
						</figure>
						
						<aside>
							<h3>What about vertex colors?</h3>
							<div>
								<p>You can also define colors per vertex, by assigning a <code>Color</code> array to the <code>color</code> property of the mesh, or a <code>Color32</code> array to its <code>colors32</code> property. These colors will be interpolated, just like its texture coordinates. The default URP shaders don't support vertex colors though, but you could create a shader graph that uses the vertex colors. I won't include vertex colors.</p>
							</div>
						</aside>
					</section>
					
					<section>
						<h3>Normal Mapping</h3>
						
						<p>Another common way to add surface details is via normal mapping. This is usually done via a normal map texture, which is an image that contains surface normal vectors. <a href="constructing-a-triangle/normal-map.png">Here</a> is such a texture, which describes a strong checkerboard pattern of alternating raised and lowered bevels, plus some subtle unevenness for variation.</p>
						
						<figure>
							<img src="constructing-a-triangle/normal-map.png" width="256" height="256">
							<figcaption>Normal map.</figcaption>
						</figure>
						
						<p>After importing the image, set its <em translate="no">Texture Type</em> to <em translate="no">Normal map</em>, otherwise it won't be properly interpreted by Unity.</p>
						
						<figure>
							<img src="constructing-a-triangle/normal-map-import-settings.png" width="320" height="108">
							<figcaption>Texture type set to normal map.</figcaption>
						</figure>
						
						<p>Then use it as the <em translate="no">Normal Map</em> of the material.</p>
						
						<figure>
							<img src="constructing-a-triangle/normal-map-material.png" width="320" height="22">
							<figcaption>Material with normal map.</figcaption>
						</figure>
						
						<p>Just like the vertex normal vectors, the normal map is used to adjust the surface normal vector, adding the illusion of surface variation that affects lighting, even through the triangle is still flat.</p>
						
						<figure>
							<img src="constructing-a-triangle/without-tangents.png" width="190" height="190">
							<figcaption>Normal-mapped triangle.</figcaption>
						</figure>
						
						<p>Although this already appears to work, the result is currently incorrect. What appears higher should appear lower instead, and vice versa. This happens because the vectors from the normal map exist in texture space and have to be converted to world space to affect lighting. This requires a transformation matrix, which defines a 3D space relative to the surface, known as tangent space. It consists of a right, an up, and a forward axis.</p>
						
						<p>The up axis should point away from the surface, for which the vertex normal vectors are used. Besides that we also need a right and a forward axis. The right axis should point in whatever direction we consider right, in our case simply <code>Vector3.right</code>. It is also known as the tangent axis or vector, because it must always be tangent to the surface curvature. We define these per vertex by assigning vectors to the <code>tangents</code> property of the mesh. The shader can construct the third axis itself by calculating the vector orthogonal to the normal and tangent. However, it could this in two different ways, producing a vector pointing either forward or backward. That's why the tangent vectors have to be <code>Vector4</code> values: their fourth component should be either 1 or &minus;1, to control the direction of the third axis.</p>
						
						<p>The default tangent vectors point to the right and have their fourth component set to 1. Due to how Unity's shaders construct tangent space this is incorrect, we have to use &minus;1 instead.</p>
						
						<pre translate="no">		mesh.normals = new Vector3[] {
			Vector3.back, Vector3.back, Vector3.back
		};

		<ins>mesh.tangents = new Vector4[] {</ins>
			<ins>new Vector4(1f, 0f, 0f, -1f),</ins>
			<ins>new Vector4(1f, 0f, 0f, -1f),</ins>
			<ins>new Vector4(1f, 0f, 0f, -1f)</ins>
		<ins>};</ins></pre>
						
						<figure>
							<img src="constructing-a-triangle/with-tangents.png" width="190" height="190" alt="with tangents">
							<img src="constructing-a-triangle/without-tangents.png" width="190" height="190" alt="without tangents">
							<figcaption>Correct and incorrect normal mapping.</figcaption>
						</figure>
						
						<aside>
							<h3>Why is the direction of the third axis configurable?</h3>
							<div>
								<p>This facilitates easy mirroring of normal maps, which is often used in 3D models of things with bilateral symmetry, like people.</p>
							</div>
						</aside>
						
						<p>As tangent vectors have four components our vertex size has grown by 16 bytes, to a final size of 48 bytes. That's a total of 144 bytes for our three vertices.</p>
						
						<figure>
							<img src="constructing-a-triangle/positions-normals-tangents-uvs.png" width="320" height="100">
							<figcaption>With tangent vectors.</figcaption>
						</figure>
					</section>
				</section>
				
				<section>
					<h2>Constructing a Quad</h2>
					
					<p>Meshes can contain more that a single triangle. To demonstrate this we'll turn our mesh into a quad by adding a second triangle to it.</p>
					
					<section>
						<h3>A Second Triangle</h3>
						
						<p>We can create a quad by taking two right isosceles triangles and putting them together with their hypotenuses touching. We keep our existing triangle and add a second one with its right corner at one unit from the origin in both the X and Y dimensions. We'll use the vertex order right, up, one. But to make it clear that we have two distinct triangles we'll initially slightly offset and scale up the new triangle, by increasing its coordinates from 1 to 1.1. Add the required positions to the <code>vertices</code> array.</p>
						
						<pre translate="no">		mesh.vertices = new Vector3[] {
			Vector3.zero, Vector3.right, Vector3.up<ins>,</ins>
			<ins>new Vector3(1.1f, 0f), new Vector3(0f, 1.1f), new Vector3(1.1f, 1.1f)</ins>
		};</pre>
						
						<p>Also increase the <code>normals</code> and <code>tangents</code> arrays so they have the same size, simply filling them with the same values.</p>
						
						<pre translate="no">		mesh.normals = new Vector3[] {
			Vector3.back, Vector3.back, Vector3.back<ins>,</ins>
			<ins>Vector3.back, Vector3.back, Vector3.back</ins>
		};

		mesh.tangents = new Vector4[] {
			new Vector4(1f, 0f, 0f, -1f),
			new Vector4(1f, 0f, 0f, -1f),
			new Vector4(1f, 0f, 0f, -1f)<ins>,</ins>
			<ins>new Vector4(1f, 0f, 0f, -1f),</ins>
			<ins>new Vector4(1f, 0f, 0f, -1f),</ins>
			<ins>new Vector4(1f, 0f, 0f, -1f)</ins>
		};</pre>
						
						<p>To keep our texture undistorted and matching we have to use appropriate texture coordinates for the new vertices.</p>
						
						<pre translate="no">		mesh.uv = new Vector2[] {
			Vector2.zero, Vector2.right, Vector2.up<ins>,</ins>
			<ins>Vector2.right, Vector2.up, Vector2.one</ins>
		};</pre>
						
						<p>Finally, add its indices to the <code>triangles</code> array. Due to the way we defined the new vertices we can list them in sequence.</p>
						
						<pre translate="no">		mesh.triangles = new int[] {
			0, 2, 1<ins>, 3, 4, 5</ins>
		};</pre>
						
						<figure>
							<img src="constructing-a-quad/two-triangles.png" width="210" height="210">
							<figcaption>Two triangles, clearly separate.</figcaption>
						</figure>
						
					</section>
					
					<section>
						<h3>Reusing Vertices</h3>
						
						<p>We don't need to define separate vertices per triangle, it is possible for multiple triangles to use the same vertex. We can't do this while the triangles are separate, but to finish the quad we'll push them together, which means that we can reuse the right and up vertices of the first triangle for the second one. Thus we can reduce our <code>vertex</code> array to four positions: zero, right, up, and one on the XY plane.</p>
						
						<pre translate="no">		mesh.vertices = new Vector3[] {
			Vector3.zero, Vector3.right, Vector3.up, <ins>new Vector3(1f, 1f)</ins>
			<del>//new Vector3(1.1f, 0f), new Vector3(0f, 1.1f), new Vector3(1.1f, 1.1f)</del>
		};</pre>
						
						<p>Likewise, eliminate the redundant data from the other arrays.</p>
						
						<pre translate="no">		mesh.normals = new Vector3[] {
			Vector3.back, Vector3.back, Vector3.back, <ins>Vector3.back</ins>
			<del>//Vector3.back, Vector3.back, Vector3.back,</del>
		};

		mesh.tangents = new Vector4[] {
			new Vector4(1f, 0f, 0f, -1f),
			new Vector4(1f, 0f, 0f, -1f),
			new Vector4(1f, 0f, 0f, -1f),
			<del>//new Vector4(1f, 0f, 0f, -1f),</del>
			<del>//new Vector4(1f, 0f, 0f, -1f),</del>
			new Vector4(1f, 0f, 0f, -1f)
		};

		mesh.uv = new Vector2[] {
			Vector2.zero, Vector2.right, Vector2.up, <ins>Vector2.one</ins>
			<del>//Vector2.right, Vector2.up, Vector2.one</del>
		};</pre>
						
						<p>The index list for the second triangle now becomes 1, 2, 3.</p>
						
						<pre translate="no">
		mesh.triangles = new int[] {
			0, 2, 1, <ins>1, 2, 3</ins>
		};</pre>
						
						<figure>
							<img src="constructing-a-quad/quad.png" width="190" height="190">
							<figcaption>Quad with shared vertices.</figcaption>
						</figure>
						
						<p>We can verify via the inspector that the mesh has four vertices and two triangles.</p>
						
						<figure>
							<img src="constructing-a-quad/four-vertices-two-triangles.png" width="320" height="168">
							<figcaption>Four vertices, two triangles.</figcaption>
						</figure>
					</section>
				</section>
				
				<section>
					<h2>Advanced Mesh API</h2>
					
					<p>Unity 2019 introduced an alternative advanced Mesh API, which allows more efficient generation of meshes, making it possible to skip intermediate steps and automatic verifications. Unity 2020 expanded on this API to make it work well with jobs and Burst. We'll use this last approach, even though we won't use separate jobs in this tutorial yet.</p>
					
					<section>
						<h3>Multi-Stream Approach</h3>
						
						<p>When we assign data to a mesh via the simple API Unity has to copy and convert everything to the mesh's native memory at some point. The advanced API allows us to work directly in the native memory format of the mesh, skipping conversion. This means that we must be aware of how the data of the mesh is laid out.</p>
						
						<p>The memory of the mesh is split into regions. The two regions we need to know about are the vertex region and the index region. The vertex region consists of one or more data streams, which are sequential blocks of vertex data of the same format. Unity supports up to four different vertex data streams per mesh.</p>
						
						<p>As we have vertex positions, normals, tangents, and texture coordinates we could store each in a separate stream. Let's call this the multi-stream approach.</p>
						
						<figure>
							<img src="advanced-mesh-api/multi-stream-approach.png" width="670" height="70">
							<figcaption>Multi-stream approach: first Positions, then Normals, then Tangents, and then teXture coordinates.</figcaption>
						</figure>
						
						<p>Create a new  <code>AdvancedMultiStreamProceduralMesh</code> component type that&mdash;like before with <code>SimpleProceduralMesh</code>&mdash;initially only creates an empty mesh and assigns it to the <code>MeshFilter</code>.
						
						<pre translate="no"><ins>using UnityEngine;</ins>

<ins>[RequireComponent(typeof(MeshFilter), typeof(MeshRenderer))]</ins>
<ins>public class AdvancedMultiStreamProceduralMesh : MonoBehaviour {</ins>

	<ins>void OnEnable () {</ins>
		<ins>var mesh = new Mesh {</ins>
			<ins>name = "Procedural Mesh"</ins>
		<ins>};</ins>

		<ins>GetComponent&lt;MeshFilter>().mesh = mesh;</ins>
	<ins>}</ins>
<ins>}</ins></pre>
						
						<p>Then replace the simple component of our game object with the new advanced one, or alternatively adjust a duplicate and disable the simple version, so you can compare them later.</p>
					</section>
					
					<section>
						<h3>Mesh Data</h3>
						
						<p>To write into native mesh data we have to first allocate it. We do this by invoking the static <code>Mesh.AllocateWritableMeshData</code> method. To facilitate generating meshes in batches this method returns a <code>Mesh.MeshDataArray</code> struct that acts like an array of native mesh data, available for writing. We have to tell it how many meshes we want to generate, which is just one. Do this before creating the <code>Mesh</code> object and keep track of the array via a variable.</p>
						
						<pre translate="no">		<ins>Mesh.MeshDataArray meshDataArray = Mesh.AllocateWritableMeshData(1);</ins>

		var mesh = new Mesh {
			name = "Procedural Mesh"
		};</pre>
						
						<p>Leaving the data empty for now, we finish by invoking <code>Mesh.ApplyAndDisposeWritableMeshData</code>, with the array and the mesh it applies to as arguments. We can directly apply the array to the mesh because it only has a single element. Afterwards we can no longer access the mesh data, unless we retrieve it again via <code>Mesh.AcquireReadOnlyMeshData</code>.</p>
						
						<pre translate="no">		var mesh = new Mesh {
			name = "Procedural Mesh"
		};
		<ins>Mesh.ApplyAndDisposeWritableMeshData(meshDataArray, mesh);</ins>
		GetComponent&lt;MeshFilter>().mesh = mesh;</pre>
						
						<p>If we enter play mode now the mesh's inspector will show that it is completely empty.</p>
						
						<figure>
							<img src="advanced-mesh-api/empty-mesh.png" width="320" height="70">
							<figcaption>Empty mesh.</figcaption>
						</figure>
						
						<p>To fill the mesh data we have to retrieve the single element from the array, keeping track of it via a variable. Its type is <code>Mesh.MeshData</code>.</p>
						
						<pre translate="no">		Mesh.MeshDataArray meshDataArray = Mesh.AllocateWritableMeshData(1);
		<ins>Mesh.MeshData meshData = meshDataArray[0];</ins></pre>
					</section>
					
					<section>
						<h3>Vertex Attributes</h3>
						
						<p>At this point the format of the mesh data is still undefined, we have to define it ourselves. For this we need to use types from the <code>Unity.Collections</code> and <code>UnityEngine.Rendering</code> namespaces.</p>
						
						<pre translate="no"><ins>using Unity.Collections;</ins>
using UnityEngine;
<ins>using UnityEngine.Rendering;</ins></pre>
						
						<p>Each vertex of our mesh has four attributes: a position, a normal, a tangent, and a set of texture coordinates. We'll describe these by allocating a temporary native array with <code>VertexAttributeDescriptor</code> elements. I'll store the count values in variables to make it clear what the numbers represent.</p>
						
						<pre translate="no">		<ins>int vertexAttributeCount = 4;</ins>
		
		Mesh.MeshDataArray meshDataArray = Mesh.AllocateWritableMeshData(1);
		Mesh.MeshData meshData = meshDataArray[0];
		
		<ins>var vertexAttributes = new NativeArray&lt;VertexAttributeDescriptor>(</ins>
			<ins>vertexAttributeCount, Allocator.Temp</ins>
		<ins>);</ins></pre>
						
						<p>The vertex streams of the mesh are then allocated by invoking <code>SetVertexBufferParams</code> on the mesh data, with the vertex count and the attribute definitions as arguments. After that we no longer need the attribute definition, so we dispose of it.</p>
						
						<pre translate="no">		int vertexAttributeCount = 4;
		<ins>int vertexCount = 4;</ins>

		Mesh.MeshDataArray meshDataArray = Mesh.AllocateWritableMeshData(1);
		Mesh.MeshData meshData = meshDataArray[0];

		var vertexAttributes = new NativeArray&lt;VertexAttributeDescriptor>(
			vertexAttributeCount, Allocator.Temp
		);
		<ins>meshData.SetVertexBufferParams(vertexCount, vertexAttributes);</ins>
		<ins>vertexAttributes.Dispose();</ins></pre>
						
						<aside>
							<h3>Do we need to use a native array for the attributes?</h3>
							<div>
								<p>No, but the alternative is to use a regular array, which requires managed memory allocation. Invoking <code>SetVertexBufferParams</code> with a variable number or arguments&mdash;like all other methods with an explicit variable parameter list&mdash;allocates a managed array as well, to store the arguments.</p>
							</div>
						</aside>
						
						<p>Before setting the vertex buffer parameters we have to describe the four attributes, setting each vertex attribute to a new <code>VertexAttributeDescriptor</code> struct value. We begin with the position. The constructor of <code>VertexAttributeDescriptor</code> has four optional parameters, to describe the attribute type, format, dimensionality, and the index of the stream that contains it. The default values are correct for our position, but we have to provide at least a single argument otherwise we end up using the constructor without arguments, which would be invalid. So let's explicitly set the <code>dimension</code> argument to 3, which indicates that it consists of three component values.</p>
						
						<pre translate="no">		var vertexAttributes = new NativeArray&lt;VertexAttributeDescriptor>(
			vertexAttributeCount, Allocator.Temp, NativeArrayOptions.UninitializedMemory
		);
		<ins>vertexAttributes[0] = new VertexAttributeDescriptor(dimension: 3);</ins>
		meshData.SetVertexBufferParams(vertexCount, vertexAttributes);</pre>
						
						<aside>
							<h3>How do named arguments work?</h3>
							<div>
								<p>You can indicate which argument a value is for by writing the method's parameter name in front of it, followed by a colon. This allows you to provide the arguments in a different order than the parameters are defined. You can also start the argument list with unnamed arguments&mdash;which must match the parameter order&mdash;and switch to named arguments at some point. You cannot switch back to unnamed arguments after that.</p>
							</div>
						</aside>
						
						<p>We follow this by setting the attributes for normals, tangents, and texture coordinates. The first argument for each should be <code>VertexAttribute.Normal</code>, <code>VertexAttribute.Tangent</code>, and <code>VertexAttribute.TexCoord0</code>. Also set their dimensionality appropriately and give them successive stream indices.</p>
						
						<pre translate="no">		vertexAttributes[0] = new VertexAttributeDescriptor(dimension: 3);
		<ins>vertexAttributes[1] = new VertexAttributeDescriptor(</ins>
			<ins>VertexAttribute.Normal, dimension: 3, stream: 1</ins>
		<ins>);</ins>
		<ins>vertexAttributes[2] = new VertexAttributeDescriptor(</ins>
			<ins>VertexAttribute.Tangent, dimension: 4, stream: 2</ins>
		<ins>);</ins>
		<ins>vertexAttributes[3] = new VertexAttributeDescriptor(</ins>
			<ins>VertexAttribute.TexCoord0, dimension: 2, stream: 3</ins>
		<ins>);</ins></pre>
						
						<p>The mesh inspector will now show the same vertex data layout and size as for our simple mesh example. It doesn't reveal how this data is split into streams.</p>
						
						<figure>
							<img src="advanced-mesh-api/vertex-attributes.png" width="320" height="148">
							<figcaption>Vertex attributes.</figcaption>
						</figure>
						
						<p>We can optimize our usage of the native array a bit more by skipping its memory initialization step. By default Unity fills the allocated memory block with zeros, to guard against weird values. We can skip this step by passing <code>NativeArrayOptions.UninitializedMemory</code> as a third argument to the <code>NativeArray</code> constructor.</p>
						
						<pre translate="no">		var vertexAttributes = new NativeArray&lt;VertexAttributeDescriptor>(
			vertexAttributeCount, Allocator.Temp<ins>, NativeArrayOptions.UninitializedMemory</ins>
		);</pre>
						
						<p>This means that the contents of the array are arbitrary and can be invalid, but we overwrite it all so that doesn't matter.</p>
						
					</section>
					
					<section>
						<h3>Setting Vertices</h3>
						
						<p>Although we won't use a job in this tutorial, at this point we'll switch to using Mathematics.</p>
						
						<pre translate="no"><ins>using Unity.Mathematics;</ins>

<ins>using static Unity.Mathematics.math;</ins></pre>
						
						<p>After invoking <code>SetVertexBufferParams</code> we can retrieve native arrays for the vertex streams by invoking <code>GetVertexData</code>. The native array that it returns is in reality a pointer to the relevant section of the mesh data. So it acts like a proxy and there is no separate array. This would allow a job to directly write to the mesh data, skipping an intermediate copy step from native array to mesh data.</p>
						
						<p><code>GetVertexData</code> is a generic method that returns a native array for the first stream by default, which contains the positions. So the array's element type is <code>float3</code>. Use it to set the positions, this time with <code>float3</code> instead of <code>Vector3</code>.</p>
						
						<pre translate="no">		meshData.SetVertexBufferParams(vertexCount, vertexAttributes);
		vertexAttributes.Dispose();

		<ins>NativeArray&lt;float3> positions = meshData.GetVertexData&lt;float3>();</ins>
		<ins>positions[0] = 0f;</ins>
		<ins>positions[1] = right();</ins>
		<ins>positions[2] = up();</ins>
		<ins>positions[3] = float3€(1f, 1f, 0f);</ins></pre>
						
						<p>Do the same for the rest of the vertex data, passing the appropriate stream index as an argument to <code>GetVertexData</code>.</p>
						
						<pre translate="no">		NativeArray&lt;float3> positions = meshData.GetVertexData&lt;float3>();
		positions[0] = 0f;
		positions[1] = right();
		positions[2] = up();
		positions[3] = float3€(1f, 1f, 0f);

		<ins>NativeArray&lt;float3> normals = meshData.GetVertexData&lt;float3>(1);</ins>
		<ins>normals[0] = normals[1] = normals[2] = normals[3] = back();</ins>

		<ins>NativeArray&lt;float4> tangents = meshData.GetVertexData&lt;float4>(2);</ins>
		<ins>tangents[0] = tangents[1] = tangents[2] = tangents[3] = float4€(1f, 0f, 0f, -1f);</ins>

		<ins>NativeArray&lt;float2> texCoords = meshData.GetVertexData&lt;float2>(3);</ins>
		<ins>texCoords[0] = 0f;</ins>
		<ins>texCoords[1] = float2€(1f, 0f);</ins>
		<ins>texCoords[2] = float2€(0f, 1f);</ins>
		<ins>texCoords[3] = 1f;</ins>
					</section>
					
					<section>
						<h3>Settings Triangles</h3>
						
						<p>We also have to reserve space for the triangle indices, which is done by invoking <code>SetIndexBufferParams</code>. Its first argument is the triangle index count. It also has a second argument, which describes the index format. Let's initially use <code>IndexFormat.UInt32</code>, which matches the <code>uint</code> type. We do this after setting the vertex data.</p>
						
						<pre translate="no">		int vertexAttributeCount = 4;
		int vertexCount = 4;
		<ins>int triangleIndexCount = 6;</ins>
		
		&hellip;
		
		<ins>meshData.SetIndexBufferParams(triangleIndexCount, IndexFormat.UInt32);</ins>
	
		var mesh = new Mesh {
			name = "Procedural Mesh"
		};</pre>
						
						<p>A native array for the triangle indices can be retrieved via the generic <code>GetIndexData</code> method. Use it to set the six indices.</p>
						
						<pre translate="no">		meshData.SetIndexBufferParams(triangleIndexCount, IndexFormat.UInt32);
		<ins>NativeArray&lt;uint> triangleIndices = meshData.GetIndexData&lt;uint>();</ins>
		<ins>triangleIndices[0] = 0;</ins>
		<ins>triangleIndices[1] = 2;</ins>
		<ins>triangleIndices[2] = 1;</ins>
		<ins>triangleIndices[3] = 1;</ins>
		<ins>triangleIndices[4] = 2;</ins>
		<ins>triangleIndices[5] = 3;</ins></pre>
						
						<p>Our mesh now has indices, but they require twice as much space as our simple mesh needs. That's because we're using the 32-bit unsigned integer type.</p>
						
						<figure>
							<img src="advanced-mesh-api/indices-uint32.png" width="320" height="40">
							<figcaption>32-bit indices.</figcaption>
						</figure>
						
						<p>This format allows access to an enormous amount of vertices, but Unity uses the smaller 16-bit type by default, which halves the size of the index buffer. This constrains the amount of accessible vertices to 65.535. As we only have six vertices we can suffice with <code>IndexFormat.UInt16</code>, which matches the <code>ushort</code> type.</p>
						
						<pre translate="no">		meshData.SetIndexBufferParams(triangleIndexCount, IndexFormat.<ins>UInt16</ins>);
		NativeArray&lt;<ins>ushort</ins>> triangleIndices = meshData.GetIndexData&lt;<ins>ushort</ins>>();</pre>
						
						<figure>
							<img src="advanced-mesh-api/indices-uint16.png" width="320" height="40">
							<figcaption>16-bit indices.</figcaption>
						</figure>
						
					</section>
					
					<section>
						<h3>Setting the Submesh</h3>
						
						<p>The final step is to define the submeshes of the mesh. We do this after setting the indices, by setting the <code>subMeshCount</code> property to 1.</p>
						
						<pre translate="no">		<ins>meshData.subMeshCount = 1;</ins>

		var mesh = new Mesh {
			name = "Procedural Mesh"
		};</pre>
						
						<figure>
							<img src="advanced-mesh-api/submesh-without-triangles.png" width="320" height="62">
							<figcaption>Submesh without triangles.</figcaption>
						</figure>
						
						<p>We also have to specify what part of the index buffer the submesh should use. This is done by invoking <code>SetSubMesh</code> with the submesh index and a <code>SubMeshDescriptor</code> value. The <code>SubMeshDescriptor</code> constructor has two arguments, for the index start and index count. In our case it should cover all indices.</p>
						
						<pre translate="no">		meshData.subMeshCount = 1;
		<ins>meshData.SetSubMesh(0, new SubMeshDescriptor(0, triangleIndexCount));</ins></pre>
						
						<p>Now we finally see our quad again.</p>
						
						<figure>
							<img src="constructing-a-quad/quad.png" width="190" height="190" alt="quad"><br>
							<img src="advanced-mesh-api/submesh-with-triangles.png" width="320" height="62" alt="inspector">
							<figcaption>Submesh with indices.</figcaption>
						</figure>
					</section>
					
					<section>
						<h3>Mesh and Submesh Bounds</h3>
						
						<p>When we create a mesh this way Unity doesn't automatically calculate its bounds. However, Unity does calculate the bounds of a submesh, which are needed in some cases. This requires checking all vertices of the submesh. We can avoid all that work be providing the correct bounds ourselves, by setting the <code>bounds</code> property of the submesh descriptor that we pass to <code>SetSubMesh</code>. We should also set its <code>vertexCount</code> property.</p>
						
						<pre translate="no">		<ins>var bounds = new Bounds(new Vector3(0.5f, 0.5f), new Vector3(1f, 1f));</ins>

		meshData.subMeshCount = 1;
		meshData.SetSubMesh(0, new SubMeshDescriptor(0, triangleIndexCount) <ins>{</ins>
			<ins>bounds = bounds,</ins>
			<ins>vertexCount = vertexCount</ins>
		<ins>}</ins>);</pre>
						
						<p>And we have to explicitly instruct Unity to not calculate these values itself, by passing <code>MeshUpdateFlags.DontRecalculateBounds</code> as a third argument to <code>SetSubMesh</code>.</p>
						
						<pre translate="no">		meshData.SetSubMesh(0, new SubMeshDescriptor(0, triangleIndexCount) {
			bounds = bounds,
			vertexCount = vertexCount
		}<ins>, MeshUpdateFlags.DontRecalculateBounds</ins>);</pre>
						
						<p>We can use the same bounds for the entire mesh, by assigning to its <code>bounds</code> property as well.</p>
						
						<pre translate="no">		var mesh = new Mesh {
			<ins>bounds = bounds,</ins>
			name = "Procedural Mesh"
		};</pre>
					</section>
					
					<section>
						<h3>Reducing Vertex Size</h3>
						
						<p>Ideally the vertex data is kept as small as possible, both to reduce memory pressure and also to improve GPU caching. The default format used for vertex attributes is <code>VertexAttributeFormat.Float32</code>, which matches the <code>float</code> type. Because our mesh is so simple we don't need this much precision. Let's reduce the tangent and texture coordinates format to half precision, by passing <code>VertexAttributeFormat.Float16</code> as a new second argument. The other two arguments then no longer need to be named.</p>
						
						<pre translate="no">		vertexAttributes[2] = new VertexAttributeDescriptor(
			VertexAttribute.Tangent, <ins>VertexAttributeFormat.Float16, 4, 2</ins>
		);
		vertexAttributes[3] = new VertexAttributeDescriptor(
			VertexAttribute.TexCoord0, <ins>VertexAttributeFormat.Float16, 2, 3</ins>
		);</pre>
						
						<p>We also have to adjust our code that sets these values so it uses the <code>half</code> type. This isn't a native C# type, which means that there isn't support for mathematical operations for this type. Instead we have to convert the final values from <code>float</code> to <code>half</code>, via the <code>half€</code> method.</p>
						
						<pre translate="no">		<ins>half h0 = half€(0f), h1 = half€(1f);</ins>

		NativeArray&lt;<ins>half4</ins>> tangents = meshData.GetVertexData&lt;half4>(2);
		tangents[0] = tangents[1] = tangents[2] = tangents[3] =
			<ins>half4€(h1, h0, h0, half€(-1f))</ins>;

		NativeArray&lt;<ins>half2</ins>> texCoords = meshData.GetVertexData&lt;half2>(3);
		texCoords[0] = <ins>h0</ins>;
		texCoords[1] = <ins>half2€(h1, h0)</ins>;
		texCoords[2] = <ins>half2€(h0, h1)</ins>;
		texCoords[3] = <ins>h1</ins>;</pre>
						
						<aside>
							<h3>How is <code>half</code> represented if it isn't a native type?</h3>
							<div>
								<p>It is a struct that contains a <code>ushort</code> value that is used to store the binary representation of the number. Conversion from <code>float</code> to <code>half</code> is done via the <code>f32tof16</code> method.</p>
							</div>
						</aside>
						
						<p>Reducing the tangents and texture coordinates to 16-bit values drops our vertex size by 12 bytes to a total of 36, a reduction of 25%.</p>
						
						<figure>
							<img src="advanced-mesh-api/tangents-uvs-half.png" width="320" height="100">
							<figcaption>16-bit tangent and uv components.</figcaption>
						</figure>
						
						<p>There are multiple data formats available, but there is a size restriction: each attribute's total size must be a multiple of four bytes. If we were to switch the position or normal to 16-bit values then their total size would be three times two bytes, so six bytes each, which isn't a multiple of four. So we cannot simply use <code>VertexAttributeFormat.Float16</code> for the position and normal attributes, unless we increased their dimensionality to 4. That would introduce a useless component but would reduce their size from 12 to 8 bytes. However, we won't do this because usually 32-bit precision is needed for these attributes. Also, the Unity editor expects 3D position vectors when it checks whether you drag-select a mesh in the scene window. If this is not the case then this action will produce a stream of errors and an incorrect selection.</p>
						
						<p>There are also other data formats, but they're not as trivial to support as converting from <code>float</code> to <code>half</code>, so I don't include them in this tutorial.</p>
					</section>
					
					<section>
						<h3>Single-Stream Approach</h3>
						
						<p>It isn't required to put each attribute in a single stream, otherwise it would only be possible to support up to four vertex attributes. The other extreme is to put all attributes in a single stream. In that case the attributes are grouped per vertex, so the data is mixed.</p>
						
						<figure>
							<img src="advanced-mesh-api/single-stream-approach.png" width="670" height="70">
							<figcaption>Single-stream approach: interleaved data grouped per vertex.</figcaption>
						</figure>
						
						<p>Although we could define the attributes in any order, Unity requires a fixed attribute order per stream: position, normal, tangent, color, texture coordinate sets from 0 up to 7, blend weights, and blend indices.</p>
						
						<aside>
							<h3>Which approach does the simple Mesh API use?</h3>
							<div>
								<p>It uses a single stream, even though the data is provided via separate arrays. So it requires an additional conversion step to interleave or isolate the data.</p>
							</div>
						</aside>
						
						<p>We'll demonstrate the single-stream approach by introducing an <code>AdvancedSingleStreamProceduralMesh</code> component type that's initially a copy of <code>AdvancedMultiStreamProceduralMesh</code> with only its name changed. Adjust the scene like we did earlier so we can see the result of this new approach.</p>
						
						<pre translate="no">public class <ins>AdvancedSingleStreamProceduralMesh</ins> : MonoBehaviour {
	
	&hellip;
}</pre>
						
						<p>To store the vertex data we have to define a struct type for it, which we'll name <code>Vertex</code>. Do this inside <code>AdvancedSingleStreamProceduralMesh</code> as we won't use it anywhere else. Give it fields for the required data matching our multi-stream approach, with the correct types and in the correct order.</p>
						
						<pre translate="no">public class AdvancedSingleStreamProceduralMesh : MonoBehaviour {
	
	<ins>struct Vertex {</ins>
		<ins>public float3 position, normal;</ins>
		<ins>public half4 tangent;</ins>
		<ins>public half2 texCoord0;</ins>
	<ins>}</ins>
	
	&hellip;
}</pre>
						
						<p>Because this data gets copied directly to the mesh and to GPU memory without modification it is essential that this data structure is used exactly as we describe it. This isn't guaranteed by default, because the C# compiler might rearrange things in an effort to optimize our code. We can enforce the exact order by attaching the <code>StructLayout</code> attribute to it with the <code>LayoutKind.Sequential</code> argument. Both are from the <code>System.Runtime.InteropServices</code> namespace.</p>
						
						<pre translate="no"><ins>using System.Runtime.InteropServices;</ins>
&hellip;

[RequireComponent(typeof(MeshFilter), typeof(MeshRenderer))]
public class AdvancedSingleStreamProceduralMesh : MonoBehaviour {

	<ins>[StructLayout(LayoutKind.Sequential)]</ins>
	struct Vertex { &hellip; }
	
	&hellip;
}</pre>
						
						<p>To put all attributes in the first stream we can simply remove the stream argument from all <code>VertexAttributeDescriptor</code> constructor invocations.</p>
						
						<pre translate="no">		vertexAttributes[0] = new VertexAttributeDescriptor(dimension: 3);
		vertexAttributes[1] = new VertexAttributeDescriptor(
			VertexAttribute.Normal, dimension: 3 <del>//, stream: 1</del>
		);
		vertexAttributes[2] = new VertexAttributeDescriptor(
			VertexAttribute.Tangent, VertexAttributeFormat.Float16, 4 <del>//, 2</del>
		);
		vertexAttributes[3] = new VertexAttributeDescriptor(
			VertexAttribute.TexCoord0, VertexAttributeFormat.Float16, 2 <del> //, 3</del>
		);</pre>
						
						<p>Next, remove the code that sets the separate streams. Retrieve the native array for the single <code>Vertex</code> stream instead.</p>
						
						<pre translate="no">		vertexAttributes.Dispose();
		
		<del>//NativeArray&lt;float3> positions = meshData.GetVertexData&lt;float3>();</del>
		<del>//&hellip;</del>
		<del>//NativeArray&lt;float3> normals = meshData.GetVertexData&lt;float3>(1);</del>
		<del>//&hellip;</del>
		<del>//NativeArray&lt;half4> tangents = meshData.GetVertexData&lt;half4>(2);</del>
		<del>//&hellip;</del>
		<del>//NativeArray&lt;half2> texCoords = meshData.GetVertexData&lt;half2>(3);</del>
		<del>//&hellip;</del>
		
		<ins>NativeArray&lt;Vertex> vertices = meshData.GetVertexData&lt;Vertex>();</ins>
		
		meshData.SetIndexBufferParams(triangleIndexCount, IndexFormat.UInt16);
		NativeArray&lt;ushort> triangleIndices = meshData.GetIndexData&lt;ushort>();</pre>
						
						<p>Finally, set the same vertices again, but now by assigning complete vertices to the single array. As they all have the same normal and tangent we can set those once, only changing the position and texture coordinates per vertex.</p>
						
						<pre translate="no">		vertexAttributes.Dispose();
		
		NativeArray&lt;Vertex> vertices = meshData.GetVertexData&lt;Vertex>();

		<ins>half h0 = half€(0f), h1 = half€(1f);</ins>

		<ins>var vertex = new Vertex {</ins>
			<ins>normal = back(),</ins>
			<ins>tangent = half4€(h1, h0, h0, half€(-1f))</ins>
		<ins>};</ins>

		<ins>vertex.position = 0f;</ins>
		<ins>vertex.texCoord0 = h0;</ins>
		<ins>vertices[0] = vertex;</ins>

		<ins>vertex.position = right();</ins>
		<ins>vertex.texCoord0 = half2€(h1, h0);</ins>
		<ins>vertices[1] = vertex;</ins>

		<ins>vertex.position = up();</ins>
		<ins>vertex.texCoord0 = half2€(h0, h1);</ins>
		<ins>vertices[2] = vertex;</ins>

		<ins>vertex.position = float3€(1f, 1f, 0f);</ins>
		<ins>vertex.texCoord0 = h1;</ins>
		<ins>vertices[3] = vertex;</ins>
		
		meshData.SetIndexBufferParams(triangleIndexCount, IndexFormat.UInt16);</pre>
						
						<p>The result appears exactly the same as the multi-stream approach&mdash;even the mesh inspector shows no difference&mdash;but the data layout is different.</p>
						
						<aside>
							<h3>Is it possible to use a mixed-stream approach?</h3>
							<div>
								<p>Yes, you could for example put all positions in the first stream and group the other data in the second stream.</p>
								
								<figure>
									<img src="advanced-mesh-api/two-stream-approach.png" width="670" height="70">
									<figcaption>Two-stream approach: first all positions, then the rest per vertex.</figcaption>
								</figure>
								
								<p>Such a layout could be beneficial for GPU caching, improving performance for depth-only and shadow-caster passes. However, if such passes required other data&mdash;for example texture coordinates for alpha testing&mdash;then this benefit would be lost.</p>
							</div>
						</aside>
						
						<p>The next tutorial is <a href="../square-grid/index.html">Square Grid</a>.</p>
					</section>
					
					<a href="../../license/index.html" class="license">license</a>
					<a href="https://bitbucket.org/catlikecodingunitytutorials/procedural-meshes-01-creating-a-mesh/" class="repository">repository</a>
					<a href="Creating-a-Mesh.pdf" download rel="nofollow">PDF</a>
				</section>
			</article>
		</main>

		<footer>
			<p>Enjoying the <a href="../../../tutorials">tutorials</a>? Are they useful? Want more?</p>
			<p><b><a href="https://www.patreon.com/catlikecoding">Please support me on Patreon!</a></b></p>
			<p><a href="https://www.patreon.com/catlikecoding"><img src="../../become-a-patron.png" alt="Become my patron!" width="217" height="51"></a></p>
			<p><b><a href="../../donating.html">Or make a direct donation</a>!</b></p>
			<p>made by <a href="../../../../about/index.html" rel="author">Jasper Flick</a></p>
		</footer>
		
		<script src="../../tutorials.js"></script>
	</body>
</html>